{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW95A63q06bI"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrV4_otH3V0T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"사용 가능 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDyvAgfO3aGj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgSksH4M6Tkr"
      },
      "source": [
        "데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIVRjgJS3gWc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# JSONL 파일 경로\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110.jsonl'\n",
        "valid_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_valid_1110.jsonl'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_test_1110.jsonl'\n",
        "\n",
        "# JSONL 로드 함수\n",
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "# 데이터 로드\n",
        "train_data = load_jsonl(train_path)\n",
        "valid_data = load_jsonl(valid_path)\n",
        "test_data = load_jsonl(test_path)\n",
        "\n",
        "print(f\"데이터 로드 완료!\")\n",
        "print(f\"- Train: {len(train_data)}개\")\n",
        "print(f\"- Valid: {len(valid_data)}개\")\n",
        "print(f\"- Test: {len(test_data)}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kx1zTn7qIgZ"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 샘플 확인\n",
        "print(\"=== Train 샘플 1개 ===\\n\")\n",
        "sample = train_data[0]\n",
        "print(json.dumps(sample, indent=2, ensure_ascii=False))\n",
        "\n",
        "print(\"\\n=== Messages 구조 ===\")\n",
        "for msg in sample['messages']:\n",
        "    print(f\"\\nRole: {msg['role']}\")\n",
        "    print(f\"Content: {msg['content'][:100]}...\" if len(msg['content']) > 100 else f\"Content: {msg['content']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6pQ7Km7qQwx"
      },
      "outputs": [],
      "source": [
        "def format_chat_prompt(sample):\n",
        "    \"\"\"\n",
        "    messages 형식을 Llama 2 Chat 프롬프트로 변환\n",
        "    \"\"\"\n",
        "    messages = sample['messages']\n",
        "\n",
        "    # system, user, assistant 메시지 추출\n",
        "    system_msg = None\n",
        "    user_msg = None\n",
        "    assistant_msg = None\n",
        "\n",
        "    for msg in messages:\n",
        "        if msg['role'] == 'system':\n",
        "            system_msg = msg['content']\n",
        "        elif msg['role'] == 'user':\n",
        "            user_msg = msg['content']\n",
        "        elif msg['role'] == 'assistant':\n",
        "            assistant_msg = msg['content']\n",
        "\n",
        "    # Llama 2 Chat 형식으로 조합\n",
        "    if system_msg:\n",
        "        prompt = f\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{user_msg} [/INST]\"\n",
        "    else:\n",
        "        prompt = f\"<s>[INST] {user_msg} [/INST]\"\n",
        "\n",
        "    # 전체 텍스트 (학습용: 프롬프트 + 답변)\n",
        "    full_text = f\"{prompt} {assistant_msg} </s>\"\n",
        "\n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'response': assistant_msg,\n",
        "        'full_text': full_text\n",
        "    }\n",
        "\n",
        "# 테스트\n",
        "formatted = format_chat_prompt(train_data[0])\n",
        "\n",
        "print(\"=== 변환된 프롬프트 ===\\n\")\n",
        "print(formatted['prompt'])\n",
        "print(\"\\n=== 기대 답변 ===\")\n",
        "print(formatted['response'])\n",
        "print(\"\\n=== 전체 텍스트 (학습용) ===\")\n",
        "print(formatted['full_text'][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh8YNLjoqf8q"
      },
      "outputs": [],
      "source": [
        "# 프롬프트 토큰화\n",
        "sample_formatted = format_chat_prompt(train_data[0])\n",
        "\n",
        "# 프롬프트만 인코딩\n",
        "prompt_tokens = tokenizer.encode(sample_formatted['prompt'], add_special_tokens=False)\n",
        "print(f\"프롬프트 토큰 길이: {len(prompt_tokens)}\")\n",
        "\n",
        "# 전체 텍스트 인코딩\n",
        "full_tokens = tokenizer.encode(sample_formatted['full_text'], add_special_tokens=False)\n",
        "print(f\"전체 텍스트 토큰 길이: {len(full_tokens)}\")\n",
        "\n",
        "# 답변만의 토큰 길이\n",
        "response_length = len(full_tokens) - len(prompt_tokens)\n",
        "print(f\"답변 토큰 길이: {response_length}\")\n",
        "\n",
        "# 디코딩 확인\n",
        "decoded = tokenizer.decode(full_tokens)\n",
        "print(f\"\\n=== 디코딩 결과 ===\")\n",
        "print(decoded[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXx8wdgo6hwK"
      },
      "source": [
        "모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZvttmf1rBKB"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes>=0.43.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUHXTv9931em"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training\n",
        "import gc\n",
        "\n",
        "# GPU 메모리 확인\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYxO-N7W6348"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "# huggingface 토큰을 입력\n",
        "login(\"토큰 입력\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhtvInn96_8s"
      },
      "outputs": [],
      "source": [
        "# 모델 이름\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 8-bit 양자화 설정 (메모리 절약)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"모델 로딩 중...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"모델 로드 완료\")\n",
        "print(f\"모델 파라미터 수: {model.num_parameters() / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIr08JCn7bf8"
      },
      "outputs": [],
      "source": [
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# padding 토큰 설정 (Llama는 기본적으로 없음)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"✓ 토크나이저 로드 완료\")\n",
        "print(f\"\\n토크나이저 정보:\")\n",
        "print(f\"- Vocab size: {len(tokenizer)}\")\n",
        "print(f\"- BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
        "print(f\"- EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "print(f\"- PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "print(f\"- UNK token: {tokenizer.unk_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFyIiOI498Xv"
      },
      "outputs": [],
      "source": [
        "# 테스트 프롬프트 (데이터셋 형식과 유사하게)\n",
        "test_prompt = \"\"\"<s>[INST] <<SYS>>\n",
        "You are an AI system that determines if the question requires clarification and classifies the ambiguity.\n",
        "\n",
        "Task:\n",
        "1. Determine if the question requires clarification: clear(no clarification needed) or ambiguous(clarification needed)\n",
        "2. Classify the ambiguity:\n",
        "   - If question is clear, set category=NONE and subclass=NONE\n",
        "   - If question is ambiguous, classify category and subclass\n",
        "\n",
        "Output format: category|subclass\n",
        "<</SYS>>\n",
        "\n",
        "Number of starbucks stores in the united states? [/INST]\"\"\"\n",
        "\n",
        "# 토크나이저로 인코딩\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(f\"입력 토큰 길이: {inputs['input_ids'].shape[1]}\")\n",
        "print(\"\\n생성 중...\")\n",
        "\n",
        "# 모델 출력 생성\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "# 디코딩\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\n=== 모델 출력 (파인튜닝 전) ===\")\n",
        "print(response)\n",
        "\n",
        "# 생성된 부분만 추출\n",
        "generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "print(\"\\n=== 생성된 답변만 ===\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2v1_l-x-7-f"
      },
      "outputs": [],
      "source": [
        "# [INST]가 어떻게 토큰화되는지 확인\n",
        "test_text = \"[INST]\"\n",
        "tokens = tokenizer.encode(test_text)\n",
        "print(f\"'[INST]' 토큰화: {tokens}\")\n",
        "print(f\"토큰 디코드: {[tokenizer.decode([t]) for t in tokens]}\")\n",
        "\n",
        "# 비교: 실제 special token\n",
        "tokens_bos = tokenizer.encode(\"<s>\", add_special_tokens=False)\n",
        "print(f\"\\n'<s>' 토큰화: {tokens_bos}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woZUAbSfqlzZ"
      },
      "source": [
        "토크나이저로 데이터 인코딩 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSxdeQTFN_vv"
      },
      "outputs": [],
      "source": [
        "# 프롬프트 토큰화\n",
        "sample_formatted = format_chat_prompt(train_data[0])\n",
        "\n",
        "# 프롬프트만 인코딩\n",
        "prompt_tokens = tokenizer.encode(sample_formatted['prompt'], add_special_tokens=False)\n",
        "print(f\"프롬프트 토큰 길이: {len(prompt_tokens)}\")\n",
        "\n",
        "# 전체 텍스트 인코딩\n",
        "full_tokens = tokenizer.encode(sample_formatted['full_text'], add_special_tokens=False)\n",
        "print(f\"전체 텍스트 토큰 길이: {len(full_tokens)}\")\n",
        "\n",
        "# 답변만의 토큰 길이\n",
        "response_length = len(full_tokens) - len(prompt_tokens)\n",
        "print(f\"답변 토큰 길이: {response_length}\")\n",
        "\n",
        "# 디코딩 확인\n",
        "decoded = tokenizer.decode(full_tokens)\n",
        "print(f\"\\n=== 디코딩 결과 ===\")\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JceX5FS5qoxn"
      },
      "outputs": [],
      "source": [
        "# 테스트용 샘플 3개\n",
        "test_samples = train_data[:3]\n",
        "\n",
        "print(\"=== 파인튜닝 전 모델 출력 테스트 ===\\n\")\n",
        "\n",
        "for idx, sample in enumerate(test_samples):\n",
        "    formatted = format_chat_prompt(sample)\n",
        "\n",
        "    # 인코딩\n",
        "    inputs = tokenizer(formatted['prompt'], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=30,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # 생성된 답변만 추출\n",
        "    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"\\n--- 샘플 {idx+1} ---\")\n",
        "    print(f\"질문: {sample['messages'][1]['content'][:80]}...\")\n",
        "    print(f\"기대 답변: {formatted['response']}\")\n",
        "    print(f\"모델 출력: {generated}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYBInG8tx9ur"
      },
      "source": [
        "포맷팅된 데이터셋 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxojiyDzyAcf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# 전처리 함수 (모든 데이터 변환)\n",
        "def format_all_data(data_list):\n",
        "    formatted_list = []\n",
        "    for sample in data_list:\n",
        "        formatted = format_chat_prompt(sample)\n",
        "        # full_text만 저장 (학습에 필요한 건 이것만)\n",
        "        formatted_list.append({\n",
        "            'text': formatted['full_text']\n",
        "        })\n",
        "    return formatted_list\n",
        "\n",
        "# Train/Valid/Test 각각 변환\n",
        "print(\"전처리 중...\")\n",
        "train_formatted = format_all_data(train_data)\n",
        "valid_formatted = format_all_data(valid_data)\n",
        "test_formatted = format_all_data(test_data)\n",
        "\n",
        "print(f\"✓ 전처리 완료\")\n",
        "print(f\"- Train: {len(train_formatted)}개\")\n",
        "print(f\"- Valid: {len(valid_formatted)}개\")\n",
        "print(f\"- Test: {len(test_formatted)}개\")\n",
        "\n",
        "# JSONL로 저장\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/woke-odds/'\n",
        "\n",
        "def save_jsonl(data, filepath):\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        for item in data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "save_jsonl(train_formatted, output_dir + 'ambiguity_train_1110_formatted.jsonl')\n",
        "save_jsonl(valid_formatted, output_dir + 'ambiguity_valid_1110_formatted.jsonl')\n",
        "save_jsonl(test_formatted, output_dir + 'ambiguity_test_1110_formatted.jsonl')\n",
        "\n",
        "print(\"\\n저장 완료!\")\n",
        "print(f\"저장 위치: {output_dir}\")\n",
        "\n",
        "# 첫 번째 샘플 확인\n",
        "print(\"\\n=== Train 첫 샘플 확인 ===\")\n",
        "print(train_formatted[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZgV8jDzvECo"
      },
      "source": [
        "훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zneaVAnss82r"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pWWk_zEpdDc4"
      },
      "outputs": [],
      "source": [
        "pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwCeWWX0wCTV"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 로그인\n",
        "login(\"토큰 입력\")\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 8-bit 설정\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16,\n",
        "    bnb_8bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 토크나이저 (이미 로드했으면 스킵)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# 모델 (이미 로드했으면 스킵)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # Gradient checkpointing 호환\n",
        ")\n",
        "\n",
        "print(\"모델 준비 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMOls9S6wJbL"
      },
      "outputs": [],
      "source": [
        "# LoRA Config 설정\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# 8-bit 학습 준비\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Llama 2 구조\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# requires_grad 확인\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"✅ {name}: requires_grad=True\")\n",
        "        break\n",
        "else:\n",
        "    print(\"학습 가능한 파라미터가 없습니다!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUIWVoARzDV8"
      },
      "outputs": [],
      "source": [
        "# 전처리된 데이터 로드\n",
        "import json\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/woke-odds/'\n",
        "\n",
        "# 로드\n",
        "train_formatted = load_jsonl(output_dir + 'ambiguity_train_1110_formatted_balanced_none90_ao15x_unf15x.jsonl')\n",
        "valid_formatted = load_jsonl(output_dir + 'ambiguity_valid_1110_formatted.jsonl')\n",
        "\n",
        "print(f\"✓ 데이터 로드 완료\")\n",
        "print(f\"- Train: {len(train_formatted)}개\")\n",
        "print(f\"- Valid: {len(valid_formatted)}개\")\n",
        "\n",
        "# Dataset 변환\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_list(train_formatted),\n",
        "    'validation': Dataset.from_list(valid_formatted)\n",
        "})\n",
        "\n",
        "print(f\"\\n Dataset 생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-LX1j-_zq5t"
      },
      "outputs": [],
      "source": [
        "# 토큰 길이 분포 확인\n",
        "\n",
        "lengths = []\n",
        "for sample in train_formatted[:100]:  # 100개 샘플 테스트\n",
        "    tokens = tokenizer(sample['text'])\n",
        "    lengths.append(len(tokens['input_ids']))\n",
        "\n",
        "import numpy as np\n",
        "print(f\"토큰 길이 통계:\")\n",
        "print(f\"- 최소: {min(lengths)}\")\n",
        "print(f\"- 최대: {max(lengths)}\")\n",
        "print(f\"- 평균: {np.mean(lengths):.1f}\")\n",
        "print(f\"- 중간값: {np.median(lengths):.1f}\")\n",
        "print(f\"\\n512 넘는 샘플: {sum(1 for l in lengths if l > 512)}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnqzYhV6zE9z"
      },
      "outputs": [],
      "source": [
        "# 토크나이즈 (이미 formatted된 데이터 사용)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # 'text' 필드에 이미 전처리된 텍스트가 있음\n",
        "    model_inputs = tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# 토크나이즈\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "print(\"토크나이즈 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZYFWgbfz16K"
      },
      "outputs": [],
      "source": [
        "# 토크나이즈 결과 확인\n",
        "\n",
        "print(\"=== 토크나이즈 확인 ===\")\n",
        "print(f\"Keys: {tokenized_dataset['train'].features}\")\n",
        "print(f\"Sample input_ids type: {type(tokenized_dataset['train'][0]['input_ids'])}\")\n",
        "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")\n",
        "\n",
        "# 디코드해서 확인\n",
        "print(\"\\n=== 첫 번째 샘플 디코드 ===\")\n",
        "decoded = tokenizer.decode(tokenized_dataset['train'][0]['input_ids'])\n",
        "print(decoded[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMo9-Mvvz-Rh"
      },
      "outputs": [],
      "source": [
        "# W&B 비활성화\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "758XdMSQ0A5Y"
      },
      "outputs": [],
      "source": [
        "# TrainingArguments 설정\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Colab Notebooks/woke-odds/llama2_classifier_none90_ao15x_unf15x',\n",
        "    num_train_epochs=8,\n",
        "    fp16=True,  # bfloat16 대신 fp16 사용 (8-bit 로딩과 호환)\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    max_grad_norm=1.0,\n",
        "    weight_decay=0.01,\n",
        "    dataloader_pin_memory=False,\n",
        "    dataloader_num_workers=2,\n",
        "    torch_empty_cache_steps=50,\n",
        "    logging_dir='/content/drive/MyDrive/Colab Notebooks/woke-odds/logs_llama2_classifier_none90_ao15x_unf15x',\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=128,\n",
        "    save_steps=128,\n",
        "    save_safetensors=True,\n",
        "    optim=\"adamw_8bit\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=4,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=[\"tensorboard\"],\n",
        ")\n",
        "\n",
        "print(\"✓ Training Arguments 설정 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7j86MbXk16Y"
      },
      "outputs": [],
      "source": [
        "# 가중치 줄거면 위 셀 말고 이거 실행\n",
        "# Custom Weighted Loss Trainer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, tokenizer=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights  # subclass별 가중치 딕셔너리\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Subclass별 가중치가 적용된 loss 계산\n",
        "        \"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Shift for causal LM\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        # 각 샘플별로 loss 계산\n",
        "        batch_size = shift_logits.size(0)\n",
        "        total_loss = 0.0\n",
        "        valid_samples = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            sample_logits = shift_logits[i]  # [seq_len, vocab_size]\n",
        "            sample_labels = shift_labels[i]  # [seq_len]\n",
        "\n",
        "            # 해당 샘플의 정답 텍스트 디코딩\n",
        "            label_tokens = sample_labels[sample_labels != -100]\n",
        "            if len(label_tokens) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # 정답 텍스트 디코딩 (Llama 2: category|subclass)\n",
        "                label_text = self.tokenizer.decode(label_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "                # category|subclass에서 subclass 추출\n",
        "                if '|' in label_text:\n",
        "                    parts = label_text.split('|')\n",
        "                    if len(parts) >= 2:\n",
        "                        subclass = parts[1].strip()\n",
        "                    else:\n",
        "                        subclass = 'NONE'\n",
        "                else:\n",
        "                    subclass = 'NONE'\n",
        "\n",
        "                # 해당 서브클래스의 가중치 가져오기\n",
        "                weight = self.class_weights.get(subclass, 1.0)\n",
        "\n",
        "            except Exception as e:\n",
        "                weight = 1.0\n",
        "\n",
        "            # Sample loss 계산\n",
        "            sample_loss = F.cross_entropy(\n",
        "                sample_logits,\n",
        "                sample_labels,\n",
        "                ignore_index=-100,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            # 가중치 적용\n",
        "            total_loss += sample_loss * weight\n",
        "            valid_samples += 1\n",
        "\n",
        "        # 평균 loss\n",
        "        loss = total_loss / valid_samples if valid_samples > 0 else total_loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# 가중치 정의\n",
        "class_weights_v1 = {\n",
        "    'NONE': 0.6,\n",
        "    'CONT': 1.0,\n",
        "    'LEX': 1.0,\n",
        "    'UNF': 1.2,\n",
        "    'SEM': 1.5,\n",
        "    'WHERE': 2.0,\n",
        "    'WHAT': 1.0,\n",
        "    'WHEN': 2.5,\n",
        "    'WHOM': 3.0,\n",
        "}\n",
        "\n",
        "print(\"✓ WeightedLossTrainer 정의 완료\")\n",
        "print(f\"클래스 가중치: {class_weights_v1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS4pVWx00HUj"
      },
      "outputs": [],
      "source": [
        "# Custom Data Collator\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # input_ids의 최대 길이 찾기\n",
        "        max_length = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "        for feature in features:\n",
        "            input_ids = feature[\"input_ids\"]\n",
        "            attention_mask = feature[\"attention_mask\"]\n",
        "            labels = feature[\"labels\"]\n",
        "\n",
        "            # 패딩 길이 계산\n",
        "            padding_length = max_length - len(input_ids)\n",
        "\n",
        "            # 오른쪽에 패딩 추가\n",
        "            padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            padded_attention_mask = attention_mask + [0] * padding_length\n",
        "            padded_labels = labels + [-100] * padding_length  # -100은 loss 계산에서 무시됨\n",
        "\n",
        "            batch[\"input_ids\"].append(padded_input_ids)\n",
        "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
        "            batch[\"labels\"].append(padded_labels)\n",
        "\n",
        "        # 리스트를 텐서로 변환\n",
        "        batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Data Collator 생성\n",
        "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)\n",
        "\n",
        "print(\"Data Collator 생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pvun7R42nxW"
      },
      "outputs": [],
      "source": [
        "# 테스트: Collator 동작 확인\n",
        "\n",
        "test_features = [\n",
        "    tokenized_dataset[\"train\"][i] for i in range(2)\n",
        "]\n",
        "\n",
        "print(\"=== Collator 테스트 ===\")\n",
        "print(f\"샘플 1 길이: {len(test_features[0]['input_ids'])}\")\n",
        "print(f\"샘플 2 길이: {len(test_features[1]['input_ids'])}\")\n",
        "\n",
        "batch = data_collator(test_features)\n",
        "\n",
        "print(f\"\\n배치 shape:\")\n",
        "print(f\"input_ids: {batch['input_ids'].shape}\")\n",
        "print(f\"attention_mask: {batch['attention_mask'].shape}\")\n",
        "print(f\"labels: {batch['labels'].shape}\")\n",
        "print(f\"\\nlabels에서 -100 개수: {(batch['labels'] == -100).sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUH7EPky2qW2"
      },
      "outputs": [],
      "source": [
        "# Trainer 생성\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,  # 3번 연속 개선 없으면 중단\n",
        "            early_stopping_threshold=0.001  # 최소 개선 임계값\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer 생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7BqiLJhlHdw"
      },
      "outputs": [],
      "source": [
        "# 가중치 줄 거면 위 셀 말고 이거 실행\n",
        "# Trainer 생성 (Weighted Loss 적용)\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "trainer = WeightedLossTrainer(  # Trainer → WeightedLossTrainer 변경\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    class_weights=class_weights_v1,  # 가중치 전달\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.01\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"WeightedLossTrainer 생성 완료\")\n",
        "print(\"가중치가 적용된 학습이 시작됩니다!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gBCLiFb23Cz"
      },
      "outputs": [],
      "source": [
        "# 학습 시작\n",
        "print(\"=== 학습 시작 ===\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmyms_Ez23ns"
      },
      "outputs": [],
      "source": [
        "# Best 모델 저장 (LoRA adapter)\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/woke-odds/llama2_classifier_none90_ao15x_unf15x_best'\n",
        "\n",
        "print(\"=== 모델 저장 중 ===\")\n",
        "print(f\"저장 위치: {save_dir}\")\n",
        "\n",
        "# LoRA adapter 저장\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(\"LoRA adapter 저장 완료\")\n",
        "print(f\"  - adapter_model.safetensors (LoRA weights)\")\n",
        "print(f\"  - adapter_config.json (LoRA 설정)\")\n",
        "print(f\"  - tokenizer files\")\n",
        "\n",
        "# 학습 정보 확인\n",
        "print(f\"\\n=== Best 모델 정보 ===\")\n",
        "print(f\"Best eval loss: {trainer.state.best_metric}\")\n",
        "print(f\"Best checkpoint: {trainer.state.best_model_checkpoint}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JwXCvyRGJtG"
      },
      "source": [
        "평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktcGJ51w3X6d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    logging as transformers_logging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-huNyN2hGS_m"
      },
      "outputs": [],
      "source": [
        "# 모델 경로 설정\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "lora_path = \"/content/drive/MyDrive/Colab Notebooks/woke-odds/llama2_classifier_none90_ao15x_unf15x_best\"\n",
        "\n",
        "print(\"=== 모델 로딩 중 ===\")\n",
        "\n",
        "# 먼저 학습이 끝났는지 확인\n",
        "import os\n",
        "if not os.path.exists(lora_path):\n",
        "    print(f\"오류: 모델 경로가 존재하지 않습니다: {lora_path}\")\n",
        "    print(\"학습이 완료되고 모델이 저장되었는지 확인하세요!\")\n",
        "else:\n",
        "    print(f\"✓ 모델 경로 확인됨: {lora_path}\")\n",
        "    files = os.listdir(lora_path)\n",
        "    print(f\"저장된 파일: {files[:5]}...\")  # 처음 5개만 출력\n",
        "\n",
        "# 토크나이저 로드 (local_files_only 추가!)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    lora_path,\n",
        "    local_files_only=True,  # 로컬 파일만 사용\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✓ 토크나이저 로드 완료\")\n",
        "\n",
        "# Base 모델 로드\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✓ Base 모델 로드 완료\")\n",
        "\n",
        "# LoRA adapter 로드 (local_files_only 추가!)\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_path,\n",
        "    local_files_only=True  # 로컬 파일만 사용\n",
        ")\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "print(\"✓ LoRA adapter 로드 완료\")\n",
        "print(f\"Device: {model.device}\")\n",
        "\n",
        "# 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"✓ 모든 로드 완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b_4kkwgGesg"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 로드 (원본 + 토크나이즈 버전 둘 다)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 원본 formatted 데이터 (토크나이즈 전)\n",
        "dataset_original = load_dataset('json', data_files={\n",
        "    'train': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl',\n",
        "    'validation': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_valid_1110_formatted.jsonl',\n",
        "    'test': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_test_1110_formatted.jsonl'\n",
        "})\n",
        "\n",
        "# 원본 messages 데이터도 로드 (질문/답변 추출용)\n",
        "dataset_messages = load_dataset('json', data_files={\n",
        "    'train': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110.jsonl',\n",
        "    'validation': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_valid_1110.jsonl',\n",
        "    'test': '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_test_1110.jsonl'\n",
        "})\n",
        "\n",
        "print(f\"데이터셋 로드 완료\")\n",
        "\n",
        "# 토크나이즈 (평가용)\n",
        "tokenized_dataset = dataset_original.map(\n",
        "    lambda examples: tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=dataset_original[\"train\"].column_names\n",
        ")\n",
        "\n",
        "def add_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)\n",
        "\n",
        "print(\"토크나이즈 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaNQPb8hGiPQ"
      },
      "outputs": [],
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset = tokenized_dataset[\"test\"]\n",
        "\n",
        "# 배치 크기 설정\n",
        "batch_size = 8\n",
        "max_new_tokens = 50\n",
        "\n",
        "# 레이블 추출 함수\n",
        "def extract_prediction(generated_text):\n",
        "    \"\"\"생성된 텍스트에서 category|subclass 추출\"\"\"\n",
        "    pattern = r\"(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "    match = re.search(pattern, generated_text)\n",
        "\n",
        "    if match:\n",
        "        return {\n",
        "            'category': match.group(1),\n",
        "            'subclass': match.group(2),\n",
        "            'full': match.group(0)\n",
        "        }\n",
        "\n",
        "    return None\n",
        "\n",
        "# DataLoader 생성\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# 평가 메트릭 초기화\n",
        "correct_cat = 0\n",
        "correct_sub = 0\n",
        "correct_all = 0\n",
        "total_samples = 0\n",
        "valid_samples = 0\n",
        "all_results = []\n",
        "\n",
        "print(\"=== 모델 평가 시작 ===\")\n",
        "\n",
        "for batch in tqdm(eval_dataloader, desc=\"평가 진행 중\"):\n",
        "    total_samples += len(batch['input_ids'])\n",
        "\n",
        "    # 원본 텍스트 디코딩\n",
        "    original_texts = [tokenizer.decode(ids, skip_special_tokens=False) for ids in batch['input_ids']]\n",
        "\n",
        "    # 배치 내 각 샘플 처리\n",
        "    for i, text in enumerate(original_texts):\n",
        "        # 정답 레이블 추출\n",
        "        label_pattern = r\"\\[/INST\\]\\s*(.*?)\\s*</s>\"\n",
        "        label_match = re.search(label_pattern, text)\n",
        "\n",
        "        if not label_match:\n",
        "            continue\n",
        "\n",
        "        label_text = label_match.group(1).strip()\n",
        "        label = extract_prediction(label_text)\n",
        "\n",
        "        if not label:\n",
        "            continue\n",
        "\n",
        "        # 프롬프트 추출\n",
        "        prompt_pattern = r\"(.*?\\[/INST\\])\"\n",
        "        prompt_match = re.search(prompt_pattern, text, re.DOTALL)\n",
        "\n",
        "        if not prompt_match:\n",
        "            continue\n",
        "\n",
        "        input_prompt = prompt_match.group(1)\n",
        "\n",
        "        # 토큰화 및 생성\n",
        "        prompt_inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **prompt_inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        # 생성된 부분 추출\n",
        "        input_length = prompt_inputs.input_ids.shape[1]\n",
        "        generated_part = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
        "\n",
        "        # 예측 레이블 추출\n",
        "        pred = extract_prediction(generated_part)\n",
        "\n",
        "        if pred:\n",
        "            valid_samples += 1\n",
        "\n",
        "            # 질문 추출\n",
        "            question_pattern = r\"<</SYS>>\\n\\n(.*?) \\[/INST\\]\"\n",
        "            question_match = re.search(question_pattern, text, re.DOTALL)\n",
        "            question = question_match.group(1).strip()[:100] if question_match else \"질문 찾을 수 없음\"\n",
        "\n",
        "            # 결과 저장\n",
        "            result = {\n",
        "                \"question\": question,\n",
        "                \"expected\": label['full'],\n",
        "                \"predicted\": pred['full'],\n",
        "                \"is_correct\": False\n",
        "            }\n",
        "\n",
        "            # 정확도 계산\n",
        "            if pred['category'] == label['category']:\n",
        "                correct_cat += 1\n",
        "\n",
        "            if pred['subclass'] == label['subclass']:\n",
        "                correct_sub += 1\n",
        "\n",
        "            if pred['category'] == label['category'] and pred['subclass'] == label['subclass']:\n",
        "                correct_all += 1\n",
        "                result[\"is_correct\"] = True\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "        # 메모리 정리\n",
        "        del prompt_inputs, outputs\n",
        "\n",
        "    # 배치 메모리 정리\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# 메트릭 계산\n",
        "metrics = {\n",
        "    'total_samples': total_samples,\n",
        "    'valid_samples': valid_samples,\n",
        "    'invalid_ratio': (total_samples - valid_samples) / total_samples if total_samples > 0 else 0,\n",
        "    'accuracy_category': correct_cat / valid_samples if valid_samples > 0 else 0,\n",
        "    'accuracy_subclass': correct_sub / valid_samples if valid_samples > 0 else 0,\n",
        "    'accuracy_all': correct_all / valid_samples if valid_samples > 0 else 0,\n",
        "}\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n=== 평가 결과 ===\")\n",
        "for key, value in metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# 오답 분석\n",
        "incorrect_predictions = [r for r in all_results if not r[\"is_correct\"]]\n",
        "print(f\"\\n=== 오답 분석 (총 {len(incorrect_predictions)}개) ===\")\n",
        "for i, result in enumerate(incorrect_predictions[:10]):\n",
        "    print(f\"{i+1}. 질문: {result['question']}\")\n",
        "    print(f\"   예상: {result['expected']}\")\n",
        "    print(f\"   예측: {result['predicted']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzHr5uwRGf-y"
      },
      "outputs": [],
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset_tokenized = tokenized_dataset[\"test\"]\n",
        "eval_dataset_messages = dataset_messages[\"test\"]  # 원본 messages\n",
        "\n",
        "num_samples = 3\n",
        "sample_indices = [int(idx) for idx in np.random.choice(len(eval_dataset_messages), min(num_samples, len(eval_dataset_messages)), replace=False)]\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for idx in tqdm(sample_indices, desc=\"테스트 샘플 처리 중\"):\n",
        "    # 원본 messages 가져오기\n",
        "    messages = eval_dataset_messages[idx]['messages']\n",
        "\n",
        "    # 질문과 답변 추출 (직관적!)\n",
        "    question = next((m['content'] for m in messages if m['role'] == 'user'), \"질문 없음\")\n",
        "    expected_label = next((m['content'] for m in messages if m['role'] == 'assistant'), \"답변 없음\")\n",
        "\n",
        "    # 프롬프트 생성 (chat template 사용)\n",
        "    # assistant 메시지 제외 (생성해야 하니까)\n",
        "    input_messages = [m for m in messages if m['role'] != 'assistant']\n",
        "\n",
        "    # Llama 2 chat template 적용\n",
        "    # HuggingFace tokenizer에 내장된 template 사용 시도\n",
        "    try:\n",
        "        input_prompt = tokenizer.apply_chat_template(\n",
        "            input_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True  # [/INST] 추가\n",
        "        )\n",
        "    except:\n",
        "        # template 없으면 수동 포맷\n",
        "        system_msg = input_messages[0]['content']\n",
        "        user_msg = input_messages[1]['content']\n",
        "        input_prompt = f\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{user_msg} [/INST]\"\n",
        "\n",
        "    # 토큰화 및 생성\n",
        "    inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    # 생성된 부분 추출\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    generated_part = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
        "\n",
        "    # 예측 레이블 추출\n",
        "    pred_pattern = r\"(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "    pred_match = re.search(pred_pattern, generated_part)\n",
        "    predicted_label = pred_match.group(0) if pred_match else \"레이블 찾을 수 없음\"\n",
        "\n",
        "    test_results.append({\n",
        "        \"sample_idx\": idx,\n",
        "        \"question\": question[:100],\n",
        "        \"expected_label\": expected_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"generated_part\": generated_part\n",
        "    })\n",
        "\n",
        "    # 메모리 정리\n",
        "    del inputs, outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 결과 출력\n",
        "for i, sample in enumerate(test_results):\n",
        "    print(f\"===== 샘플 {i+1}/{len(test_results)} =====\")\n",
        "    print(f\"질문: {sample['question']}\")\n",
        "    print(f\"예상: {sample['expected_label']}\")\n",
        "    print(f\"예측: {sample['predicted_label']}\")\n",
        "    print(f\"생성: {sample['generated_part']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SIX3kxTGkD1"
      },
      "outputs": [],
      "source": [
        "# 혼동 행렬 생성 및 시각화\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    # 레이블 추출\n",
        "    y_true = [r[\"expected\"] for r in all_results]\n",
        "    y_pred = [r[\"predicted\"] for r in all_results]\n",
        "\n",
        "    # 고유 레이블\n",
        "    unique_labels = sorted(list(set(y_true + y_pred)))\n",
        "\n",
        "    if len(unique_labels) > 20:\n",
        "        from collections import Counter\n",
        "        label_counts = Counter(y_true + y_pred)\n",
        "        unique_labels = [label for label, _ in label_counts.most_common(20)]\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
        "    cm_df = pd.DataFrame(cm, index=unique_labels, columns=unique_labels)\n",
        "\n",
        "    # 시각화\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('예측')\n",
        "    plt.ylabel('실제')\n",
        "    plt.title('Llama 2 분류 결과 혼동 행렬')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(\"시각화 라이브러리가 없어 혼동 행렬을 표시할 수 없습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dp0V42RmzYG"
      },
      "outputs": [],
      "source": [
        "# 각 split 개수 확인\n",
        "\n",
        "print(\"=== 데이터셋 개수 확인 ===\")\n",
        "print(f\"Train: {len(dataset_messages['train'])}개\")\n",
        "print(f\"Validation: {len(dataset_messages['validation'])}개\")\n",
        "print(f\"Test: {len(dataset_messages['test'])}개\")\n",
        "\n",
        "# 비율 계산\n",
        "total = len(dataset_messages['train']) + len(dataset_messages['validation']) + len(dataset_messages['test'])\n",
        "print(f\"\\n총합: {total}개\")\n",
        "print(f\"\\n비율:\")\n",
        "print(f\"  Train: {len(dataset_messages['train'])/total*100:.1f}%\")\n",
        "print(f\"  Valid: {len(dataset_messages['validation'])/total*100:.1f}%\")\n",
        "print(f\"  Test: {len(dataset_messages['test'])/total*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwXwuSa-uVWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNH6TZoDy4JeuTnU9uke6yl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}