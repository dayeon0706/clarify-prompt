{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "UtehPxm-e5l_"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv3epequNIRd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"사용 가능 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "7VyHFcigNYPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#작업 경로 지정\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/woke-odds')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "YDLZoOWONcIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 로드\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': 'ambiguity_train_1110.jsonl',\n",
        "    'validation': 'ambiguity_valid_1110.jsonl',\n",
        "    'test': 'ambiguity_test_1110.jsonl'\n",
        "})\n",
        "print(f\"훈련 데이터: {len(dataset['train'])}개\")\n",
        "print(f\"검증 데이터: {len(dataset['validation'])}개\")\n",
        "print(f\"평가 데이터: {len(dataset['test'])}개\")"
      ],
      "metadata": {
        "id": "K-zNBCDTOPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "n4WRVEmOyRbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####데이터 불균형 해소를 위한 클래스 가중치 적용 훈련"
      ],
      "metadata": {
        "id": "unCIIsHeCjTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "SRiq6fXHpo9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # Gradient checkpointing과 호환되도록 설정\n",
        ")"
      ],
      "metadata": {
        "id": "vP4l_TL4pr7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# requires_grad 확인\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"✅ {name}: requires_grad=True\")\n",
        "        break\n",
        "else:\n",
        "    print(\"❌ 학습 가능한 파라미터가 없습니다!\")"
      ],
      "metadata": {
        "id": "Ouh2Bs5Ipwix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def preprocess_function(examples):\n",
        "    # 'messages' 형식을 text로 변환 (Phi-4 chat template 적용)\n",
        "    texts = []\n",
        "    for messages in examples['messages']:\n",
        "        # Phi-4의 chat template 사용\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    # 토크나이즈\n",
        "    model_inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False  # DataCollator가 처리\n",
        "    )\n",
        "\n",
        "    # labels 설정 (CausalLM은 input_ids를 그대로 사용)\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "JQX9w2FHp3gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이즈 결과 확인\n",
        "print(\"=== 토크나이즈 확인 ===\")\n",
        "print(f\"Keys: {tokenized_dataset['train'].features}\")\n",
        "print(f\"Sample input_ids type: {type(tokenized_dataset['train'][0]['input_ids'])}\")\n",
        "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")"
      ],
      "metadata": {
        "id": "VQxLXZu8tso9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Data Collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # input_ids의 최대 길이 찾기\n",
        "        max_length = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "        for feature in features:\n",
        "            input_ids = feature[\"input_ids\"]\n",
        "            attention_mask = feature[\"attention_mask\"]\n",
        "            labels = feature[\"labels\"]\n",
        "\n",
        "            # 패딩 길이 계산\n",
        "            padding_length = max_length - len(input_ids)\n",
        "\n",
        "            # 오른쪽에 패딩 추가\n",
        "            padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            padded_attention_mask = attention_mask + [0] * padding_length\n",
        "            padded_labels = labels + [-100] * padding_length  # -100은 loss 계산에서 무시됨\n",
        "\n",
        "            batch[\"input_ids\"].append(padded_input_ids)\n",
        "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
        "            batch[\"labels\"].append(padded_labels)\n",
        "\n",
        "        # 리스트를 텐서로 변환\n",
        "        batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Data Collator 생성\n",
        "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "tmvfM2HsqCWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트: Collator 동작 확인\n",
        "test_features = [\n",
        "    tokenized_dataset[\"train\"][i] for i in range(2)\n",
        "]\n",
        "\n",
        "print(\"=== Collator 테스트 ===\")\n",
        "print(f\"샘플 1 길이: {len(test_features[0]['input_ids'])}\")\n",
        "print(f\"샘플 2 길이: {len(test_features[1]['input_ids'])}\")\n",
        "\n",
        "batch = data_collator(test_features)\n",
        "\n",
        "print(f\"\\n배치 shape:\")\n",
        "print(f\"input_ids: {batch['input_ids'].shape}\")\n",
        "print(f\"attention_mask: {batch['attention_mask'].shape}\")\n",
        "print(f\"labels: {batch['labels'].shape}\")\n",
        "print(f\"\\nlabels에서 -100 개수: {(batch['labels'] == -100).sum().item()}\")"
      ],
      "metadata": {
        "id": "jIjIXb-pt149"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v1 = {\n",
        "    'NONE': 0.6,\n",
        "    'CONT': 1.0,\n",
        "    'LEX': 1.0,\n",
        "    'UNF': 1.2,\n",
        "    'SEM': 1.5,\n",
        "    'WHERE': 2.0,\n",
        "    'WHAT': 1.0,\n",
        "    'WHEN': 2.5,\n",
        "    'WHOM': 3.0,\n",
        "}"
      ],
      "metadata": {
        "id": "UtNC5Dks16IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v2 = {\n",
        "    'AO|WHAT': 1.77,\n",
        "    'AO|WHEN': 1.78,\n",
        "    'AO|WHERE': 1.78,\n",
        "    'AO|WHOM': 1.78,\n",
        "    'EM|CONT': 1.78,\n",
        "    'EM|UNF': 1.78,\n",
        "    'LA|LEX': 1.78,\n",
        "    'LA|SEM': 1.78,\n",
        "    'NONE|NONE': 0.22\n",
        "}"
      ],
      "metadata": {
        "id": "St2is7REkoRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v3 = {\n",
        "    'AO|WHAT': 2.5,\n",
        "    'AO|WHEN': 2.5,\n",
        "    'AO|WHERE': 2.5,\n",
        "    'AO|WHOM': 2.5,\n",
        "    'EM|CONT': 1.78,\n",
        "    'EM|UNF': 1.78,\n",
        "    'LA|LEX': 1.78,\n",
        "    'LA|SEM': 1.78,\n",
        "    'NONE|NONE': 0.15\n",
        "}"
      ],
      "metadata": {
        "id": "5xMl5j0nuU5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v4 = {\n",
        "    'NONE': 1.0,\n",
        "    'WHEN': 7.57,\n",
        "    'WHAT': 7.52,\n",
        "    'WHOM': 7.57,\n",
        "    'CONT': 7.57,\n",
        "    'SEM': 7.57,\n",
        "    'LEX': 7.57,\n",
        "    'WHERE': 7.57,\n",
        "    'UNF': 7.57\n",
        "}"
      ],
      "metadata": {
        "id": "l8fAIZPW-M3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v5 = {\n",
        "    'NONE': 1.0,\n",
        "    'WHEN': 2.83,\n",
        "    'WHAT': 2.82,\n",
        "    'WHOM': 2.83,\n",
        "    'CONT': 2.83,\n",
        "    'SEM': 2.83,\n",
        "    'LEX': 2.83,\n",
        "    'WHERE': 2.83,\n",
        "    'UNF': 2.83\n",
        "}"
      ],
      "metadata": {
        "id": "g5F8wBmBA3OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_v6 = {\n",
        "    'NONE': 1.69,\n",
        "    'WHEN': 3.20,\n",
        "    'WHAT': 3.19,\n",
        "    'WHOM': 3.20,\n",
        "    'CONT': 3.20,\n",
        "    'SEM': 3.20,\n",
        "    'LEX': 3.20,\n",
        "    'WHERE': 3.20,\n",
        "    'UNF': 3.20\n",
        "}"
      ],
      "metadata": {
        "id": "YPRmGqxeA7Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, tokenizer=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights  # 서브클래스별 가중치 딕셔너리\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        서브클래스별 가중치가 적용된 loss 계산\n",
        "        \"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Shift for causal LM\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        # 각 샘플별로 loss 계산\n",
        "        batch_size = shift_logits.size(0)\n",
        "        total_loss = 0.0\n",
        "        valid_samples = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            sample_logits = shift_logits[i]  # [seq_len, vocab_size]\n",
        "            sample_labels = shift_labels[i]  # [seq_len]\n",
        "\n",
        "            # 해당 샘플의 정답 텍스트 디코딩\n",
        "            label_tokens = sample_labels[sample_labels != -100]\n",
        "            if len(label_tokens) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # 정답 텍스트에서 서브클래스 추출 (category|subclass 형태)\n",
        "                label_text = self.tokenizer.decode(label_tokens, skip_special_tokens=True)\n",
        "\n",
        "                # category|subclass에서 subclass 추출\n",
        "                if '|' in label_text:\n",
        "                    subclass = label_text.split('|')[1].strip()\n",
        "                else:\n",
        "                    subclass = 'NONE'\n",
        "\n",
        "                # 해당 서브클래스의 가중치 가져오기\n",
        "                weight = self.class_weights.get(subclass, 1.0)\n",
        "\n",
        "            except:\n",
        "                weight = 1.0\n",
        "\n",
        "            # Sample loss 계산\n",
        "            sample_loss = F.cross_entropy(\n",
        "                sample_logits,\n",
        "                sample_labels,\n",
        "                ignore_index=-100,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            # 가중치 적용\n",
        "            total_loss += sample_loss * weight\n",
        "            valid_samples += 1\n",
        "\n",
        "        # 평균 loss\n",
        "        loss = total_loss / valid_samples if valid_samples > 0 else total_loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "6B7iLqlep6FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, tokenizer=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights_v3  # category|subclass 레이블별 가중치 딕셔너리\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        category|subclass 레이블별 가중치가 적용된 loss 계산\n",
        "        \"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Shift for causal LM\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        # 각 샘플별로 loss 계산\n",
        "        batch_size = shift_logits.size(0)\n",
        "        total_loss = 0.0\n",
        "        valid_samples = 0\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            sample_logits = shift_logits[i]  # [seq_len, vocab_size]\n",
        "            sample_labels = shift_labels[i]  # [seq_len]\n",
        "\n",
        "            # 해당 샘플의 정답 텍스트 디코딩\n",
        "            label_tokens = sample_labels[sample_labels != -100]\n",
        "            if len(label_tokens) == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # 정답 텍스트에서 전체 레이블 추출 (category|subclass 형태)\n",
        "                label_text = self.tokenizer.decode(label_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "                # 해당 레이블의 가중치 가져오기\n",
        "                weight = self.class_weights.get(label_text, 1.0)\n",
        "\n",
        "            except:\n",
        "                weight = 1.0\n",
        "\n",
        "            # Sample loss 계산\n",
        "            sample_loss = F.cross_entropy(\n",
        "                sample_logits,\n",
        "                sample_labels,\n",
        "                ignore_index=-100,\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            # 가중치 적용\n",
        "            total_loss += sample_loss * weight\n",
        "            valid_samples += 1\n",
        "\n",
        "        # 평균 loss\n",
        "        loss = total_loss / valid_samples if valid_samples > 0 else total_loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "Q1evuhoGlzCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5ddf1c"
      },
      "source": [
        "# transformers 라이브러리의 로깅 레벨 설정하여 경고 숨기기\n",
        "from transformers import logging as transformers_logging\n",
        "transformers_logging.set_verbosity_error()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./ambig_improve_v6',  # 이름 변경\n",
        "    num_train_epochs=3,\n",
        "    bf16=True,\n",
        "\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    dataloader_pin_memory=False,\n",
        "    torch_empty_cache_steps=50,\n",
        "\n",
        "    logging_dir='./logs_ambig_improve_v6',  # 이름 변경\n",
        "    logging_steps=25,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=128,\n",
        "    save_steps=128,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=2,\n",
        "\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    report_to=[\"tensorboard\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Z99LEwTDp__M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#W&B 비활성화\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "PJMuHKg5zjB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "AdEGY4I3zjB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    class_weights=class_weights_v6,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,  # 3번 연속 개선 없으면 중단\n",
        "            early_stopping_threshold=0.01  # 최소 개선 임계값\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "jYqmEU796ZTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "rMcW215IzzAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 모델 저장\n",
        "trainer.save_model(\"./ambig_improve_v6\")"
      ],
      "metadata": {
        "id": "7ik97-8KqGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련"
      ],
      "metadata": {
        "id": "rYDQOUFvvx98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#메모리 정리\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "del trainer\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kOGjWyWEDYL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "import torch"
      ],
      "metadata": {
        "id": "i0aIqHYIdG-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # Gradient checkpointing과 호환되도록 설정\n",
        "    )"
      ],
      "metadata": {
        "id": "Oyu0abARfIgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LoRA Config 설정\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#requires_grad 확인\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"✅ {name}: requires_grad=True\")\n",
        "        break\n",
        "else:\n",
        "    print(\"❌ 학습 가능한 파라미터가 없습니다!\")"
      ],
      "metadata": {
        "id": "C7A64cZwdX8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def preprocess_function(examples):\n",
        "    # 'text' 컬럼을 토크나이즈\n",
        "    model_inputs = tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False  # DataCollator가 처리\n",
        "    )\n",
        "    # labels 설정\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "sIxSVagBj7B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#W&B 비활성화\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "ir4Rq89C_4Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "lPF3VVV8MhZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TrainingArguments 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_v2',\n",
        "    num_train_epochs=3,\n",
        "    bf16=True,\n",
        "\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "\n",
        "    max_grad_norm=1.0,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    dataloader_pin_memory=False,\n",
        "    torch_empty_cache_steps=50,\n",
        "\n",
        "    logging_dir='./logs_v2',\n",
        "    logging_steps= 25,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=128,\n",
        "    save_steps=128,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=2,\n",
        "\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    report_to=[\"tensorboard\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Stg5FCrjOjjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")"
      ],
      "metadata": {
        "id": "FpKP_0ojvS-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer 생성\n",
        "from transformers import EarlyStoppingCallback\n",
        "import torch\n",
        "\n",
        "# 조합 레이블 순서를 정의합니다. 이 순서는 모델의 최종 출력 레이어 순서와 일치해야 합니다.\n",
        "# 모델 출력 레이블을 0부터 N-1까지의 정수 인덱스로 매핑해야 합니다.\n",
        "# 이전 셀(ac2613c8)에서 계산된 class_weights_combined 딕셔너리를 사용합니다.\n",
        "# 주의: 모델의 실제 출력 순서를 확인하고 이 리스트를 정확하게 매핑해야 합니다.\n",
        "# 임의의 순서로 설정하며, 실제 모델 출력 순서에 맞춰 수정이 필요합니다.\n",
        "# 예를 들어, 모델의 LM head가 '0|NONE|NONE', '1|AO|WHAT', ... 순서로 출력한다면\n",
        "# label_order = ['0|NONE|NONE', '1|AO|WHAT', ...]가 되어야 합니다.\n",
        "# 현재는 combined_dist의 키 순서대로 가정합니다.\n",
        "label_order = list(combined_dist.keys()) # combined_dist는 이전 셀에 정의되어 있어야 합니다.\n",
        "\n",
        "# 클래스 가중치를 텐서로 변환하고 모델 장치로 이동\n",
        "# label_order에 정의된 순서대로 가중치를 가져옵니다.\n",
        "class_weights_list = [class_weights_combined[label] for label in label_order]\n",
        "class_weights_tensor = torch.tensor(class_weights_list, dtype=torch.float).to(model.device)\n",
        "\n",
        "# Trainer 생성 시 compute_metrics는 메모리 문제로 제외하고, class_weights를 loss_fn에 적용\n",
        "# Hugging Face Trainer는 기본적으로 내부 loss 계산 시 class_weights 인자를 지원하지 않습니다.\n",
        "# class_weights를 적용하려면 Trainer를 상속하거나, loss 계산 부분을 커스텀해야 합니다.\n",
        "# 여기서는 간단하게 EarlyStoppingCallback만 사용하여 Trainer를 초기화하고,\n",
        "# class_weights 적용은 다음 단계에서 Trainer를 커스텀하여 진행하는 것으로 계획합니다.\n",
        "# (또는 Trainer.__init__에 직접 class_weight 인자를 전달할 수 있는지 문서를 다시 확인)\n",
        "\n",
        "# 다시 확인해보니 Trainer는 __init__에 class_weight 인자를 직접 받지 않습니다.\n",
        "# Custom Trainer를 만들거나 compute_metrics를 커스텀하여 loss에 가중치를 적용해야 합니다.\n",
        "# 여기서는 Custom Trainer 생성을 준비하는 단계로 Trainer 초기화만 진행합니다.\n",
        "# 클래스 가중치는 추후 Custom Trainer에서 사용될 것입니다.\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer, # Use processing_class instead of tokenizer\n",
        "    # compute_metrics=compute_metrics # 메모리 문제로 따로 평가\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,  # 3번 연속 개선 없으면 중단\n",
        "            early_stopping_threshold=0.01  # 최소 개선 임계값\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Note: The class_weights_tensor has been created and is on the correct device,\n",
        "# but it is not yet actively used in the default Trainer's loss calculation.\n",
        "# This will require extending the Trainer class or modifying the loss calculation\n",
        "# within a custom training loop, which is a more advanced step.\n",
        "# For now, the weights are ready to be used when the custom Trainer is implemented."
      ],
      "metadata": {
        "id": "0XNGgASKvVWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9klZUfAUveQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#베스트 모델 저장\n",
        "model.save_pretrained('./v2_final_best_model')\n",
        "tokenizer.save_pretrained('./v2_final_best_model')"
      ],
      "metadata": {
        "id": "5ZOVvc3vxeXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###평가"
      ],
      "metadata": {
        "id": "Wt-fU5CxvtIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    logging as transformers_logging\n",
        ")"
      ],
      "metadata": {
        "id": "f12e5HzoxZ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 경로 설정 (저장된 모델 경로로 변경하세요)\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/woke-odds/1st_final_best_model\"\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# 특수 토큰 설정이 필요한 경우 확인\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 모델 로드\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "# GPU 메모리 사용량 최적화\n",
        "if torch.cuda.is_available():\n",
        "    model = model.eval()\n",
        "\n",
        "# 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "4hg1VZ7-xgbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미 토크나이즈된 후라면, skip\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': 'final_CLAMBER_train.jsonl',\n",
        "    'validation': 'final_CLAMBER_valid.jsonl',\n",
        "    'test': 'final_CLAMBER_test.jsonl'\n",
        "})\n",
        "\n",
        "# 데이터 전처리\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda examples: tokenizer(\n",
        "        examples['text'],\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "# 레이블 설정\n",
        "def add_labels(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
        "    return examples\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=True)"
      ],
      "metadata": {
        "id": "juqDWwvoyXz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset = tokenized_dataset[\"validation\"] if \"validation\" in tokenized_dataset else tokenized_dataset[\"test\"]\n",
        "\n",
        "# 랜덤 샘플 인덱스 선택 (3개 샘플)\n",
        "num_samples = 3\n",
        "sample_indices = [int(idx) for idx in np.random.choice(len(eval_dataset), min(num_samples, len(eval_dataset)), replace=False)]\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for idx in tqdm(sample_indices, desc=\"테스트 샘플 처리 중\"):\n",
        "    # 단일 샘플 가져오기\n",
        "    sample = eval_dataset[idx]\n",
        "\n",
        "    # 전체 텍스트 디코딩\n",
        "    full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "\n",
        "    # 질문 추출\n",
        "    question_pattern = r\"<\\|user\\|>Analyze this question: (.*?)<\\|end\\|>\"\n",
        "    question_match = re.search(question_pattern, full_text)\n",
        "    question = question_match.group(1) if question_match else \"질문을 찾을 수 없음\"\n",
        "\n",
        "    # 정답 레이블 추출\n",
        "    label_pattern = r\"<\\|assistant\\|>(.*?)<\\|end\\|>\"\n",
        "    label_match = re.search(label_pattern, full_text)\n",
        "    expected_label = label_match.group(1) if label_match else \"레이블 찾을 수 없음\"\n",
        "\n",
        "    # 시스템 메시지와 사용자 메시지만 추출\n",
        "    prompt_pattern = r\"(<\\|system\\|>.*?<\\|user\\|>Analyze this question: .*?<\\|end\\|>)\"\n",
        "    prompt_match = re.search(prompt_pattern, full_text, re.DOTALL)\n",
        "\n",
        "    if prompt_match:\n",
        "        # 원본 프롬프트 + 어시스턴트 시작 태그\n",
        "        input_prompt = prompt_match.group(1) + \"<|assistant|>\"\n",
        "\n",
        "        # 토큰화\n",
        "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 생성\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=False,  # 결정적 생성\n",
        "            )\n",
        "\n",
        "        # 생성된 전체 시퀀스\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "        # 입력 이후 생성된 부분만\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        generated_part = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=False)\n",
        "\n",
        "        # 예측 레이블 추출\n",
        "        pred_pattern = r\"([01])\\|(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "        pred_match = re.search(pred_pattern, generated_part)\n",
        "        predicted_label = pred_match.group(0) if pred_match else \"레이블 찾을 수 없음\"\n",
        "    else:\n",
        "        input_prompt = \"프롬프트를 찾을 수 없음\"\n",
        "        generated_text = \"생성 실패\"\n",
        "        generated_part = \"생성 실패\"\n",
        "        predicted_label = \"레이블 찾을 수 없음\"\n",
        "\n",
        "    test_results.append({\n",
        "        \"sample_idx\": idx,\n",
        "        \"question\": question,\n",
        "        \"expected_label\": expected_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"generated_part\": generated_part\n",
        "    })\n",
        "\n",
        "    # 메모리 정리\n",
        "    if 'inputs' in locals():\n",
        "        del inputs\n",
        "    if 'outputs' in locals():\n",
        "        del outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 테스트 결과 출력\n",
        "for i, sample in enumerate(test_results):\n",
        "    print(f\"===== 샘플 {i+1}/{len(test_results)} =====\")\n",
        "    print(f\"질문: {sample['question']}\")\n",
        "    print(f\"예상 레이블: {sample['expected_label']}\")\n",
        "    print(f\"예측 레이블: {sample['predicted_label']}\")\n",
        "    print(\"\\n생성된 부분:\")\n",
        "    print(sample['generated_part'][:200] + \"...\" if len(sample['generated_part']) > 200 else sample['generated_part'])\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "YeusM_8G17__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset = tokenized_dataset[\"validation\"] if \"validation\" in tokenized_dataset else tokenized_dataset[\"test\"]\n",
        "\n",
        "# 랜덤 샘플 인덱스 선택 (3개 샘플)\n",
        "num_samples = 3\n",
        "sample_indices = [int(idx) for idx in np.random.choice(len(eval_dataset), min(num_samples, len(eval_dataset)), replace=False)]\n",
        "\n",
        "test_results = []\n",
        "\n",
        "for idx in tqdm(sample_indices, desc=\"테스트 샘플 처리 중\"):\n",
        "    # 단일 샘플 가져오기\n",
        "    sample = eval_dataset[idx]\n",
        "\n",
        "    # 전체 텍스트 디코딩\n",
        "    full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "\n",
        "    # 질문 추출\n",
        "    question_pattern = r\"<\\|user\\|>Analyze this question: (.*?)<\\|end\\|>\"\n",
        "    question_match = re.search(question_pattern, full_text)\n",
        "    question = question_match.group(1) if question_match else \"질문을 찾을 수 없음\"\n",
        "\n",
        "    # 정답 레이블 추출\n",
        "    label_pattern = r\"<\\|assistant\\|>(.*?)<\\|end\\|>\"\n",
        "    label_match = re.search(label_pattern, full_text)\n",
        "    expected_label = label_match.group(1) if label_match else \"레이블 찾을 수 없음\"\n",
        "\n",
        "    # 시스템 메시지와 사용자 메시지만 추출\n",
        "    prompt_pattern = r\"(<\\|system\\|>.*?<\\|user\\|>Analyze this question: .*?<\\|end\\|>)\"\n",
        "    prompt_match = re.search(prompt_pattern, full_text, re.DOTALL)\n",
        "\n",
        "    if prompt_match:\n",
        "        # 원본 프롬프트 + 어시스턴트 시작 태그\n",
        "        input_prompt = prompt_match.group(1) + \"<|assistant|>\"\n",
        "\n",
        "        # 토큰화\n",
        "        inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 생성\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=False,  # 결정적 생성\n",
        "            )\n",
        "\n",
        "        # 생성된 전체 시퀀스\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "        # 입력 이후 생성된 부분만\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        generated_part = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=False)\n",
        "\n",
        "        # 예측 레이블 추출\n",
        "        pred_pattern = r\"([01])\\|(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "        pred_match = re.search(pred_pattern, generated_part)\n",
        "        predicted_label = pred_match.group(0) if pred_match else \"레이블 찾을 수 없음\"\n",
        "    else:\n",
        "        input_prompt = \"프롬프트를 찾을 수 없음\"\n",
        "        generated_text = \"생성 실패\"\n",
        "        generated_part = \"생성 실패\"\n",
        "        predicted_label = \"레이블 찾을 수 없음\"\n",
        "\n",
        "    test_results.append({\n",
        "        \"sample_idx\": idx,\n",
        "        \"question\": question,\n",
        "        \"expected_label\": expected_label,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"generated_part\": generated_part\n",
        "    })\n",
        "\n",
        "    # 메모리 정리\n",
        "    if 'inputs' in locals():\n",
        "        del inputs\n",
        "    if 'outputs' in locals():\n",
        "        del outputs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 테스트 결과 출력\n",
        "for i, sample in enumerate(test_results):\n",
        "    print(f\"===== 샘플 {i+1}/{len(test_results)} =====\")\n",
        "    print(f\"질문: {sample['question']}\")\n",
        "    print(f\"예상 레이블: {sample['expected_label']}\")\n",
        "    print(f\"예측 레이블: {sample['predicted_label']}\")\n",
        "    print(\"\\n생성된 부분:\")\n",
        "    print(sample['generated_part'][:200] + \"...\" if len(sample['generated_part']) > 200 else sample['generated_part'])\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "sJf-KPZlFQ-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset = tokenized_dataset[\"validation\"]\n",
        "\n",
        "# 배치 크기 및 기타 설정\n",
        "batch_size = 8  # GPU 메모리에 맞게 조정\n",
        "max_new_tokens = 50  # 응답이 짧으므로 50으로 충분\n",
        "\n",
        "# 결과 추출 함수\n",
        "def extract_prediction(generated_text):\n",
        "    \"\"\"생성된 텍스트에서 분류 결과를 추출합니다.\"\"\"\n",
        "    pattern = r\"([01])\\|(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "    match = re.search(pattern, generated_text)\n",
        "\n",
        "    if match:\n",
        "        return {\n",
        "            'require_clarification': int(match.group(1)),\n",
        "            'category': match.group(2),\n",
        "            'subclass': match.group(3)\n",
        "        }\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_label(text):\n",
        "    \"\"\"입력 텍스트에서 정답 레이블을 추출합니다.\"\"\"\n",
        "    pattern = r\"([01])\\|(EM|LA|AO|NONE)\\|(UNF|CONT|LEX|SEM|WHOM|WHEN|WHERE|WHAT|NONE)\"\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        return {\n",
        "            'require_clarification': int(match.group(1)),\n",
        "            'category': match.group(2),\n",
        "            'subclass': match.group(3)\n",
        "        }\n",
        "\n",
        "    return None\n",
        "\n",
        "# DataLoader 생성\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# 평가 메트릭 초기화\n",
        "correct_req = 0\n",
        "correct_cat = 0\n",
        "correct_sub = 0\n",
        "correct_all = 0\n",
        "total_samples = 0\n",
        "valid_samples = 0\n",
        "all_results = []\n",
        "\n",
        "# 평가 실행\n",
        "print(\"모델 평가 시작...\")\n",
        "for batch in tqdm(eval_dataloader, desc=\"평가 진행 중\"):\n",
        "    total_samples += len(batch['input_ids'])\n",
        "\n",
        "    # 텐서를 장치로 이동\n",
        "    input_ids = batch['input_ids'].to(model.device)\n",
        "    attention_mask = batch['attention_mask'].to(model.device)\n",
        "\n",
        "    # 원본 텍스트 디코딩 및 정답 레이블 추출\n",
        "    original_texts = [tokenizer.decode(ids, skip_special_tokens=False) for ids in batch['input_ids']]\n",
        "    labels = []\n",
        "\n",
        "    for text in original_texts:\n",
        "        label_pattern = r\"<\\|assistant\\|>(.*?)<\\|end\\|>\"\n",
        "        label_match = re.search(label_pattern, text)\n",
        "\n",
        "        if label_match:\n",
        "            label_text = label_match.group(1)\n",
        "            label = extract_label(label_text)\n",
        "            labels.append(label)\n",
        "        else:\n",
        "            labels.append(None)\n",
        "\n",
        "    # 입력 프롬프트 생성\n",
        "    input_prompts = []\n",
        "    for text in original_texts:\n",
        "        prompt_pattern = r\"(<\\|system\\|>.*?<\\|user\\|>.*?<\\|end\\|>)\"\n",
        "        prompt_match = re.search(prompt_pattern, text, re.DOTALL)\n",
        "\n",
        "        if prompt_match:\n",
        "            input_prompt = prompt_match.group(1) + \"<|assistant|>\"\n",
        "            input_prompts.append(input_prompt)\n",
        "        else:\n",
        "            input_prompts.append(None)\n",
        "\n",
        "    # 배치 내 각 샘플에 대한 예측 생성\n",
        "    for i in range(len(input_ids)):\n",
        "        if input_prompts[i] is None or labels[i] is None:\n",
        "            continue\n",
        "\n",
        "        # 토큰화\n",
        "        prompt_inputs = tokenizer(input_prompts[i], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # 생성\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **prompt_inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=False,\n",
        "            )\n",
        "\n",
        "        # 생성된 부분만 추출\n",
        "        input_length = prompt_inputs.input_ids.shape[1]\n",
        "        generated_part = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=False)\n",
        "\n",
        "        # 예측 레이블 추출\n",
        "        pred = extract_prediction(generated_part)\n",
        "\n",
        "        # 정답 레이블과 예측 레이블 비교\n",
        "        if pred:\n",
        "            valid_samples += 1\n",
        "\n",
        "            # 질문 추출\n",
        "            question_pattern = r\"<\\|user\\|>Analyze this question: (.*?)<\\|end\\|>\"\n",
        "            question_match = re.search(question_pattern, original_texts[i])\n",
        "            question = question_match.group(1) if question_match else \"질문을 찾을 수 없음\"\n",
        "\n",
        "            # 결과 저장\n",
        "            result = {\n",
        "                \"question\": question,\n",
        "                \"expected\": f\"{labels[i]['require_clarification']}|{labels[i]['category']}|{labels[i]['subclass']}\",\n",
        "                \"predicted\": f\"{pred['require_clarification']}|{pred['category']}|{pred['subclass']}\",\n",
        "                \"is_correct\": False\n",
        "            }\n",
        "\n",
        "            # 정확도 계산\n",
        "            if pred['require_clarification'] == labels[i]['require_clarification']:\n",
        "                correct_req += 1\n",
        "\n",
        "            if pred['category'] == labels[i]['category']:\n",
        "                correct_cat += 1\n",
        "\n",
        "            if pred['subclass'] == labels[i]['subclass']:\n",
        "                correct_sub += 1\n",
        "\n",
        "            if (pred['require_clarification'] == labels[i]['require_clarification'] and\n",
        "                pred['category'] == labels[i]['category'] and\n",
        "                pred['subclass'] == labels[i]['subclass']):\n",
        "                correct_all += 1\n",
        "                result[\"is_correct\"] = True\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "        # 메모리 정리\n",
        "        del prompt_inputs, outputs\n",
        "\n",
        "    # 배치 메모리 정리\n",
        "    del input_ids, attention_mask\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# 메트릭 계산\n",
        "metrics = {\n",
        "    'total_samples': total_samples,\n",
        "    'valid_samples': valid_samples,\n",
        "    'invalid_ratio': (total_samples - valid_samples) / total_samples if total_samples > 0 else 0,\n",
        "    'accuracy_require_clarification': correct_req / valid_samples if valid_samples > 0 else 0,\n",
        "    'accuracy_category': correct_cat / valid_samples if valid_samples > 0 else 0,\n",
        "    'accuracy_subclass': correct_sub / valid_samples if valid_samples > 0 else 0,\n",
        "    'accuracy_all': correct_all / valid_samples if valid_samples > 0 else 0,\n",
        "}\n",
        "\n",
        "# 메트릭 출력\n",
        "print(\"\\n=== 평가 결과 ===\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# 오류 분석\n",
        "incorrect_predictions = [r for r in all_results if not r[\"is_correct\"]]\n",
        "print(f\"\\n총 {len(incorrect_predictions)} 개의 오답 중 일부 샘플:\")\n",
        "for i, result in enumerate(incorrect_predictions[:10]):  # 처음 10개만 출력\n",
        "    print(f\"{i+1}. 질문: {result['question']}\")\n",
        "    print(f\"   예상: {result['expected']}\")\n",
        "    print(f\"   예측: {result['predicted']}\")\n",
        "    print()\n",
        "\n",
        "# 혼동 행렬 (Confusion Matrix) 생성 및 시각화 (선택 사항)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    # 클래스 레이블 추출\n",
        "    y_true = [r[\"expected\"] for r in all_results]\n",
        "    y_pred = [r[\"predicted\"] for r in all_results]\n",
        "\n",
        "    # 고유 레이블 (최대 20개까지만 표시)\n",
        "    unique_labels = sorted(list(set(y_true + y_pred)))\n",
        "    if len(unique_labels) > 20:\n",
        "        # 가장 빈번한 레이블 20개만 선택\n",
        "        from collections import Counter\n",
        "        label_counts = Counter(y_true + y_pred)\n",
        "        unique_labels = [label for label, _ in label_counts.most_common(20)]\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
        "\n",
        "    # 데이터프레임으로 변환\n",
        "    cm_df = pd.DataFrame(cm, index=unique_labels, columns=unique_labels)\n",
        "\n",
        "    # 혼동 행렬 시각화\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('예측')\n",
        "    plt.ylabel('실제')\n",
        "    plt.title('분류 결과 혼동 행렬')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except ImportError:\n",
        "    print(\"시각화 라이브러리가 설치되어 있지 않아 혼동 행렬을 표시할 수 없습니다.\")"
      ],
      "metadata": {
        "id": "TbQwd0w24nT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파라미터 설정을 위한 실험"
      ],
      "metadata": {
        "id": "UtehPxm-e5l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# 모델 이름\n",
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 각 split의 토큰 길이 계산\n",
        "def calculate_lengths(split_data, split_name):\n",
        "    lengths = [len(tokenizer.encode(item['text'])) for item in tqdm(split_data, desc=f\"{split_name}\")]\n",
        "    print(f\"\\n{split_name} 데이터:\")\n",
        "    print(f\"  최대 길이: {max(lengths)}\")\n",
        "    print(f\"  평균 길이: {np.mean(lengths):.1f}\")\n",
        "    print(f\"  중간값: {np.median(lengths):.1f}\")\n",
        "    print(f\"  95 백분위수: {np.percentile(lengths, 95):.1f}\")\n",
        "    return max(lengths)\n",
        "\n",
        "max_length_train = calculate_lengths(dataset['train'], \"Train\")\n",
        "max_length_validation = calculate_lengths(dataset['validation'], \"Validation\")\n",
        "max_length_test = calculate_lengths(dataset['test'], \"Test\")"
      ],
      "metadata": {
        "id": "z1GpkvYHcsiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "# 모델 이름\n",
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "# GPU 메모리 측정 함수\n",
        "def get_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**3  # GB 단위\n",
        "    return 0\n",
        "\n",
        "# 메모리 초기화 함수\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Gradient Checkpointing 메모리 테스트 (ON만)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 더미 데이터 준비 (max_seq_length=768, batch_size=8)\n",
        "dummy_texts = [\"This is a test sentence for memory profiling. \" * 50] * 8\n",
        "inputs = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True,\n",
        "                   truncation=True, max_length=768)\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "# Gradient Checkpointing 활성화 테스트\n",
        "print(\"\\n[테스트] Gradient Checkpointing ON + 배치 크기 8\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # Gradient checkpointing과 호환되도록 설정\n",
        ")\n",
        "\n",
        "# Gradient Checkpointing 활성화\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 입력을 모델과 같은 디바이스로 이동\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "memory_before = get_gpu_memory()\n",
        "print(f\"   모델 로드 후 메모리: {memory_before:.2f} GB\")\n",
        "\n",
        "outputs = None\n",
        "loss = None\n",
        "\n",
        "try:\n",
        "    # Forward + Backward pass\n",
        "    model.train()\n",
        "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "\n",
        "    memory_after = get_gpu_memory()\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
        "\n",
        "    print(f\"   학습 중 메모리: {memory_after:.2f} GB\")\n",
        "    print(f\"   최대 메모리 사용량: {peak_memory:.2f} GB\")\n",
        "    print(\"\\n✅ 성공: Gradient Checkpointing으로 배치 크기 8 학습 가능!\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(f\"\\n❌ OOM 발생: 배치 크기 8은 여전히 부족\")\n",
        "        print(\"   → 배치 크기를 4로 줄이고 gradient_accumulation_steps=2 사용 권장\")\n",
        "    else:\n",
        "        print(f\"에러: {e}\")\n",
        "\n",
        "# 메모리 정리\n",
        "if outputs is not None:\n",
        "    del outputs\n",
        "if loss is not None:\n",
        "    del loss\n",
        "del model\n",
        "clear_memory()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"결론\")\n",
        "print(\"=\" * 60)\n",
        "print(\"✅ Gradient Checkpointing은 반드시 활성화해야 합니다!\")"
      ],
      "metadata": {
        "id": "PM7cOl8olKs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA 파라미터 확인\n",
        "print(\"\\n=== LoRA 파라미터 확인 ===\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name.lower():\n",
        "        print(f\"{name}: requires_grad={param.requires_grad}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jdmi7e8EBvDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}