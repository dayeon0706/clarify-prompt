{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PQAQBQD90QO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    logging as transformers_logging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
        "model_path = \"/content/drive/MyDrive/woke-odds/ambig_improve_v6\"\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# íŠ¹ìˆ˜ í† í° ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš° í™•ì¸\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”\n",
        "if torch.cuda.is_available():\n",
        "    model = model.eval()"
      ],
      "metadata": {
        "id": "6kX3gkFQ947j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Data Collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # input_idsì˜ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°\n",
        "        max_length = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "        for feature in features:\n",
        "            input_ids = feature[\"input_ids\"]\n",
        "            attention_mask = feature[\"attention_mask\"]\n",
        "            labels = feature[\"labels\"]\n",
        "\n",
        "            # íŒ¨ë”© ê¸¸ì´ ê³„ì‚°\n",
        "            padding_length = max_length - len(input_ids)\n",
        "\n",
        "            # ì˜¤ë¥¸ìª½ì— íŒ¨ë”© ì¶”ê°€\n",
        "            padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            padded_attention_mask = attention_mask + [0] * padding_length\n",
        "            padded_labels = labels + [-100] * padding_length  # -100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨\n",
        "\n",
        "            batch[\"input_ids\"].append(padded_input_ids)\n",
        "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
        "            batch[\"labels\"].append(padded_labels)\n",
        "\n",
        "        # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
        "        batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Data Collator ìƒì„±\n",
        "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "0UVyFC74Bq8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •\n",
        "#data_collator = DataCollatorForLanguageModeling(\n",
        "#    tokenizer,\n",
        "#    mlm=False,\n",
        "#)"
      ],
      "metadata": {
        "id": "_foEzUxCBvf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì‘ì—… ê²½ë¡œ ì§€ì •\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/woke-odds')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "_M5spPQZOSoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': 'ambiguity_train_1110.jsonl',\n",
        "    'validation': 'ambiguity_valid_1110.jsonl',\n",
        "    'test': 'ambiguity_test_1110.jsonl'\n",
        "})\n",
        "\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "def preprocess_function(examples):\n",
        "    # 'messages' í˜•ì‹ì„ textë¡œ ë³€í™˜ (Phi-4 chat template ì ìš©)\n",
        "    texts = []\n",
        "    for messages in examples['messages']:\n",
        "        # Phi-4ì˜ chat template ì‚¬ìš©\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    # í† í¬ë‚˜ì´ì¦ˆ\n",
        "    model_inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False  # DataCollatorê°€ ì²˜ë¦¬\n",
        "    )\n",
        "\n",
        "    # labels ì„¤ì • (CausalLMì€ input_idsë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "CjIXRoxR-AXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ ì¶œë ¥ í™•ì¸ â¬‡ï¸\n"
      ],
      "metadata": {
        "id": "nfbJKI8Z-IYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ 3ê°œ ì„ íƒ\n",
        "test_samples = [tokenized_dataset[\"test\"][i] for i in range(3)]\n",
        "\n",
        "print(\"=== ëª¨ë¸ ì¶œë ¥ í…ŒìŠ¤íŠ¸ (3ê°œ ìƒ˜í”Œ) ===\\n\")\n",
        "\n",
        "for idx, sample_data in enumerate(test_samples):\n",
        "    # ì›ë³¸ messages ê°€ì ¸ì˜¤ê¸° (í† í¬ë‚˜ì´ì¦ˆ ì „ ë°ì´í„°ì—ì„œ)\n",
        "    original_sample = dataset[\"test\"][idx]\n",
        "    messages = original_sample['messages']\n",
        "\n",
        "    # system + user ë©”ì‹œì§€ë§Œ ì‚¬ìš©\n",
        "    input_messages = [msg for msg in messages if msg['role'] != 'assistant']\n",
        "\n",
        "    # Chat template ì ìš©\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        input_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # í† í¬ë‚˜ì´ì¦ˆ\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)\n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # ì •ë‹µ ì¶”ì¶œ\n",
        "    ground_truth = [msg['content'] for msg in messages if msg['role'] == 'assistant'][0]\n",
        "    user_query = [msg['content'] for msg in messages if msg['role'] == 'user'][0]\n",
        "\n",
        "    # ì¶œë ¥\n",
        "    print(f\"[ìƒ˜í”Œ {idx+1}]\")\n",
        "    print(f\"User Query: {user_query}\")\n",
        "    print(f\"\\nGround Truth: {ground_truth}\")\n",
        "    print(f\"\\nModel Output: {generated_text.strip()}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "f3z-YbsP-DmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì „ì²´ í‰ê°€ â¬‡ï¸\n",
        "\n",
        "\n",
        "*   ì •í™•ë„\n",
        "*   í˜¼ë™ í–‰ë ¬ (ëŒ€ê°ì„  ìš”ì†Œê°€ ì§„í•  ìˆ˜ë¡(ì˜ˆì¸¡=ì •ë‹µ) ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒ)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFXPiVxf-Tf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "AEjSR6GlKB6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ ë°ì´í„°ì…‹ ì„ íƒ\n",
        "eval_dataset = tokenized_dataset[\"test\"]\n",
        "\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset)}\")\n",
        "print(f\"ì²« ë²ˆì§¸ ìƒ˜í”Œ:\\n{eval_dataset[0]}\")"
      ],
      "metadata": {
        "id": "lrfOSCDvEArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í‰ê°€ ë°ì´í„°ì…‹ ì„ íƒ\n",
        "eval_dataset = tokenized_dataset[\"test\"]\n",
        "\n",
        "def clarify_generation(messages, model, tokenizer, max_new_tokens=150):\n",
        "    # system + user ë©”ì‹œì§€ë§Œ ì‚¬ìš© (assistant ì œì™¸)\n",
        "    input_messages = [msg for msg in messages if msg['role'] != 'assistant']\n",
        "\n",
        "    # Chat template ì ìš©\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        input_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # assistant ì‘ë‹µ ìƒì„± í”„ë¡¬í”„íŠ¸ ì¶”ê°€\n",
        "    )\n",
        "\n",
        "    # í† í¬ë‚˜ì´ì¦ˆ\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # ìƒì„±ëœ í† í° ID ì¶”ì¶œ\n",
        "    generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "\n",
        "    # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)\n",
        "    generated_text = tokenizer.decode(outputs.sequences[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    confidence = None\n",
        "    if outputs.scores:\n",
        "        token_probs = []\n",
        "        for step_idx, step_logits in enumerate(outputs.scores):\n",
        "            if step_idx >= len(generated_ids):\n",
        "                break\n",
        "            # í™•ë¥  ê³„ì‚°\n",
        "            step_probs = F.softmax(step_logits[0], dim=-1)\n",
        "            # ì‹¤ì œ ì„ íƒëœ í† í°ì˜ í™•ë¥ \n",
        "            token_id = generated_ids[step_idx].item()\n",
        "            token_prob = step_probs[token_id].item()\n",
        "            token_probs.append(token_prob)\n",
        "\n",
        "        confidence = sum(token_probs) / len(token_probs) if token_probs else None\n",
        "\n",
        "    return generated_text.strip(), confidence"
      ],
      "metadata": {
        "id": "HzeXvp3_-Qd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>' + message['content'] + '<|end|>'}}{% elif message['role'] == 'user' %}{{'<|user|>' + message['content'] + '<|end|>'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + message['content'] + '<|end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}\"\"\""
      ],
      "metadata": {
        "id": "_qCDgzmjLlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ì›ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "# ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "confidences = []\n",
        "\n",
        "print(\"ì˜ˆì¸¡ ì‹œì‘...\")\n",
        "for example in tqdm(eval_dataset):\n",
        "    messages = example['messages']\n",
        "\n",
        "    # ëª¨ë¸ ì˜ˆì¸¡\n",
        "    predicted, confidence = clarify_generation(messages, model, tokenizer)\n",
        "    predictions.append(predicted)\n",
        "    confidences.append(confidence)\n",
        "\n",
        "    # ì •ë‹µ (assistantì˜ ì‘ë‹µ)\n",
        "    ground_truth = [msg['content'] for msg in messages if msg['role'] == 'assistant'][0]\n",
        "    ground_truths.append(ground_truth)\n",
        "\n",
        "print(f\"ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ìƒ˜í”Œ\")"
      ],
      "metadata": {
        "id": "zEVZpPHvFAyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬\n",
        "results_df = pd.DataFrame({\n",
        "    'user_query': [msg['content'] for example in eval_dataset for msg in example['messages'] if msg['role'] == 'user'],\n",
        "    'ground_truth': ground_truths,\n",
        "    'prediction': predictions,\n",
        "    'confidence': confidences\n",
        "})\n",
        "\n",
        "# ì²˜ìŒ 5ê°œ ê²°ê³¼ í™•ì¸\n",
        "print(\"=== ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ===\")\n",
        "for idx in range(min(5, len(results_df))):\n",
        "    print(f\"\\n[ìƒ˜í”Œ {idx+1}]\")\n",
        "    print(f\"Query: {results_df.iloc[idx]['user_query']}\")\n",
        "    print(f\"Ground Truth: {results_df.iloc[idx]['ground_truth']}\")\n",
        "    print(f\"Prediction: {results_df.iloc[idx]['prediction']}\")"
      ],
      "metadata": {
        "id": "lO4M74IfFlop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì •í™•ë„ ê³„ì‚°\n",
        "results_df['is_correct'] = results_df['ground_truth'] == results_df['prediction']\n",
        "\n",
        "accuracy = results_df['is_correct'].mean()\n",
        "print(f\"ğŸ“Š ì „ì²´ ì •í™•ë„: {accuracy*100:.2f}%\")\n",
        "print(f\"   ì •ë‹µ: {results_df['is_correct'].sum()}ê°œ\")\n",
        "print(f\"   ì˜¤ë‹µ: {(~results_df['is_correct']).sum()}ê°œ\")"
      ],
      "metadata": {
        "id": "gveDf5CgJHBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_confidence = results_df['confidence'].mean()\n",
        "print(f\"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f}\")"
      ],
      "metadata": {
        "id": "jFrKKzpRJMLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# ì •ë‹µ/ì˜¤ë‹µë³„ë¡œ ì‹ ë¢°ë„ ë¶„ë¦¬\n",
        "correct_confidences = results_df[results_df['is_correct'] == True]['confidence'].dropna().tolist()\n",
        "incorrect_confidences = results_df[results_df['is_correct'] == False]['confidence'].dropna().tolist()\n",
        "\n",
        "# ê¸°ë³¸ í†µê³„ ê³„ì‚°\n",
        "def calculate_statistics(data, label):\n",
        "    if len(data) == 0:\n",
        "        print(f\"\\n{label}: ë°ì´í„° ì—†ìŒ\")\n",
        "        return None\n",
        "\n",
        "    stats_dict = {\n",
        "        'ê°œìˆ˜': len(data),\n",
        "        'í‰ê· ': np.mean(data),\n",
        "        'ì¤‘ì•™ê°’': np.median(data),\n",
        "        'í‘œì¤€í¸ì°¨': np.std(data),\n",
        "        'ìµœì†Ÿê°’': np.min(data),\n",
        "        'ìµœëŒ“ê°’': np.max(data),\n",
        "        '25% ë¶„ìœ„ìˆ˜': np.percentile(data, 25),\n",
        "        '75% ë¶„ìœ„ìˆ˜': np.percentile(data, 75),\n",
        "    }\n",
        "\n",
        "    return stats_dict\n",
        "\n",
        "# í†µê³„ ì¶œë ¥\n",
        "print(\"=\" * 60)\n",
        "print(\"ì‹ ë¢°ë„ í†µê³„ ë¶„ì„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[ì •ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„ í†µê³„]\")\n",
        "correct_stats = calculate_statistics(correct_confidences, \"ì •ë‹µ\")\n",
        "if correct_stats:\n",
        "    for key, value in correct_stats.items():\n",
        "        if key == 'ê°œìˆ˜':\n",
        "            print(f\"{key}: {value}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "\n",
        "print(\"\\n[ì˜¤ë‹µ ì˜ˆì¸¡ ì‹ ë¢°ë„ í†µê³„]\")\n",
        "incorrect_stats = calculate_statistics(incorrect_confidences, \"ì˜¤ë‹µ\")\n",
        "if incorrect_stats:\n",
        "    for key, value in incorrect_stats.items():\n",
        "        if key == 'ê°œìˆ˜':\n",
        "            print(f\"{key}: {value}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value:.6f}\")"
      ],
      "metadata": {
        "id": "Y7Pn0YBmNfeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°•ìŠ¤í”Œë¡¯ ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 6))\n",
        "box_data = [correct_confidences, incorrect_confidences]\n",
        "box_labels = ['ì •ë‹µ', 'ì˜¤ë‹µ']\n",
        "bp = plt.boxplot(box_data, labels=box_labels, patch_artist=True, showmeans=True)\n",
        "bp['boxes'][0].set_facecolor('lightgreen')\n",
        "bp['boxes'][1].set_facecolor('lightcoral')\n",
        "plt.ylabel('ì‹ ë¢°ë„', fontsize=12)\n",
        "plt.title('ì •ë‹µ/ì˜¤ë‹µ ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„ ë¶„í¬ ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QBwdcEB5RNYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„ ë¶„ì„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "confidence_bins = np.linspace(0, 1, 11)  # 0.0, 0.1, 0.2, ..., 1.0\n",
        "bin_labels = [f\"{confidence_bins[i]:.1f}-{confidence_bins[i+1]:.1f}\"\n",
        "              for i in range(len(confidence_bins)-1)]\n",
        "\n",
        "# ê° êµ¬ê°„ë³„ ì •í™•ë„ ê³„ì‚°\n",
        "bin_accuracy = []\n",
        "bin_counts = []\n",
        "\n",
        "for i in range(len(confidence_bins)-1):\n",
        "    lower = confidence_bins[i]\n",
        "    upper = confidence_bins[i+1]\n",
        "\n",
        "    # DataFrameì—ì„œ í•´ë‹¹ êµ¬ê°„ì— ì†í•˜ëŠ” ìƒ˜í”Œ í•„í„°ë§\n",
        "    in_bin = results_df[\n",
        "        (results_df['confidence'].notna()) &\n",
        "        (results_df['confidence'] >= lower) &\n",
        "        (results_df['confidence'] < upper)\n",
        "    ]\n",
        "\n",
        "    if len(in_bin) > 0:\n",
        "        accuracy = in_bin['is_correct'].mean()\n",
        "        bin_accuracy.append(accuracy)\n",
        "        bin_counts.append(len(in_bin))\n",
        "        print(f\"ì‹ ë¢°ë„ {bin_labels[i]}: ì •í™•ë„ {accuracy:.4f} ({len(in_bin)}ê°œ ìƒ˜í”Œ)\")\n",
        "    else:\n",
        "        bin_accuracy.append(0)\n",
        "        bin_counts.append(0)\n",
        "        print(f\"ì‹ ë¢°ë„ {bin_labels[i]}: ë°ì´í„° ì—†ìŒ\")\n",
        "\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "HawuPJ4lfrrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì¸¡ì • (ë°ì´í„° ë¶ˆê· í˜• í›ˆë ¨ ì´í›„ ì¶”ê°€)"
      ],
      "metadata": {
        "id": "QDs-VMtYM5oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from collections import defaultdict, Counter"
      ],
      "metadata": {
        "id": "QURd3pkTNAEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df['true_category'] = results_df['ground_truth'].apply(lambda x: x.split('|')[0] if '|' in x else x)\n",
        "results_df['true_subclass'] = results_df['ground_truth'].apply(lambda x: x.split('|')[1] if '|' in x and len(x.split('|')) > 1 else '')\n",
        "\n",
        "results_df['pred_category'] = results_df['prediction'].apply(lambda x: x.split('|')[0] if '|' in x else x)\n",
        "results_df['pred_subclass'] = results_df['prediction'].apply(lambda x: x.split('|')[1] if '|' in x and len(x.split('|')) > 1 else '')"
      ],
      "metadata": {
        "id": "YuxwiWeqPPa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸\n",
        "cat_counts = results_df['true_category'].value_counts().to_dict()\n",
        "print(\"\\nCategory í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "for cls, count in sorted(cat_counts.items()):\n",
        "    print(f\"  {cls}: {count}ê°œ ({count/len(results_df):.1%})\")\n",
        "\n",
        "# ì •í™•ë„ ê³„ì‚°\n",
        "cat_accuracy = (results_df['true_category'] == results_df['pred_category']).mean()\n",
        "print(f\"\\nCategory ì „ì²´ ì •í™•ë„: {cat_accuracy:.4f} ({cat_accuracy*100:.2f}%)\")\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ ì •í™•ë„\n",
        "print(\"\\nCategory í´ë˜ìŠ¤ë³„ ì •í™•ë„:\")\n",
        "for cls in sorted(results_df['true_category'].unique()):\n",
        "    class_samples = results_df[results_df['true_category'] == cls]\n",
        "    if len(class_samples) > 0:\n",
        "        acc = (class_samples['true_category'] == class_samples['pred_category']).mean()\n",
        "        correct_count = (class_samples['true_category'] == class_samples['pred_category']).sum()\n",
        "        print(f\"  {cls}: {acc:.4f} ({correct_count}/{len(class_samples)})\")\n",
        "\n",
        "# í˜¼ë™ í–‰ë ¬\n",
        "cat_labels = sorted(results_df['true_category'].unique())\n",
        "cat_cm = confusion_matrix(results_df['true_category'], results_df['pred_category'], labels=cat_labels)\n",
        "cat_cm_df = pd.DataFrame(cat_cm, index=[f'ì‹¤ì œ {l}' for l in cat_labels], columns=[f'ì˜ˆì¸¡ {l}' for l in cat_labels])\n",
        "\n",
        "print(\"\\nCategory í˜¼ë™ í–‰ë ¬:\")\n",
        "print(cat_cm_df)\n",
        "\n",
        "# ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜\n",
        "cat_precision, cat_recall, cat_f1, cat_support = precision_recall_fscore_support(\n",
        "    results_df['true_category'],\n",
        "    results_df['pred_category'],\n",
        "    labels=cat_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "# Category í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cat_cm_df, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'ìƒ˜í”Œ ìˆ˜'})\n",
        "plt.title('Category í˜¼ë™ í–‰ë ¬', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('ì˜ˆì¸¡', fontsize=12)\n",
        "plt.ylabel('ì‹¤ì œ', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zXbyCrcfN5rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¹ˆ subclass ì œì™¸\n",
        "subclass_df = results_df[results_df['true_subclass'] != ''].copy()\n",
        "\n",
        "if len(subclass_df) > 0:\n",
        "    # í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸\n",
        "    sub_counts = subclass_df['true_subclass'].value_counts().to_dict()\n",
        "    print(\"\\nSubclass í´ë˜ìŠ¤ ë¶„í¬:\")\n",
        "    for cls, count in sorted(sub_counts.items()):\n",
        "        print(f\"  {cls}: {count}ê°œ ({count/len(subclass_df):.1%})\")\n",
        "\n",
        "    # ì •í™•ë„ ê³„ì‚°\n",
        "    sub_accuracy = (subclass_df['true_subclass'] == subclass_df['pred_subclass']).mean()\n",
        "    print(f\"\\nSubclass ì „ì²´ ì •í™•ë„: {sub_accuracy:.4f} ({sub_accuracy*100:.2f}%)\")\n",
        "\n",
        "    # í´ë˜ìŠ¤ë³„ ì •í™•ë„\n",
        "    print(\"\\nSubclass í´ë˜ìŠ¤ë³„ ì •í™•ë„:\")\n",
        "    for cls in sorted(subclass_df['true_subclass'].unique()):\n",
        "        class_samples = subclass_df[subclass_df['true_subclass'] == cls]\n",
        "        if len(class_samples) > 0:\n",
        "            acc = (class_samples['true_subclass'] == class_samples['pred_subclass']).mean()\n",
        "            correct_count = (class_samples['true_subclass'] == class_samples['pred_subclass']).sum()\n",
        "            print(f\"  {cls}: {acc:.4f} ({correct_count}/{len(class_samples)})\")\n",
        "\n",
        "    # í˜¼ë™ í–‰ë ¬ (ìƒìœ„ 15ê°œ í´ë˜ìŠ¤ë§Œ)\n",
        "    from collections import Counter\n",
        "    sub_label_counts = Counter(subclass_df['true_subclass'].tolist() + subclass_df['pred_subclass'].tolist())\n",
        "    top_15_sub_labels = [label for label, _ in sub_label_counts.most_common(15)]\n",
        "\n",
        "    # í•„í„°ë§\n",
        "    filtered_sub_df = subclass_df[\n",
        "        subclass_df['true_subclass'].isin(top_15_sub_labels) &\n",
        "        subclass_df['pred_subclass'].isin(top_15_sub_labels)\n",
        "    ]\n",
        "\n",
        "    if len(filtered_sub_df) > 0:\n",
        "        sub_cm = confusion_matrix(\n",
        "            filtered_sub_df['true_subclass'],\n",
        "            filtered_sub_df['pred_subclass'],\n",
        "            labels=top_15_sub_labels\n",
        "        )\n",
        "        sub_cm_df = pd.DataFrame(\n",
        "            sub_cm,\n",
        "            index=[f'ì‹¤ì œ {l}' for l in top_15_sub_labels],\n",
        "            columns=[f'ì˜ˆì¸¡ {l}' for l in top_15_sub_labels]\n",
        "        )\n",
        "\n",
        "        print(\"\\nSubclass í˜¼ë™ í–‰ë ¬ (ìƒìœ„ 15ê°œ):\")\n",
        "        print(sub_cm_df)\n",
        "\n",
        "        # ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜\n",
        "        sub_precision, sub_recall, sub_f1, sub_support = precision_recall_fscore_support(\n",
        "            filtered_sub_df['true_subclass'],\n",
        "            filtered_sub_df['pred_subclass'],\n",
        "            labels=top_15_sub_labels,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(\"\\nSubclass ìƒì„¸ ì§€í‘œ (ìƒìœ„ 15ê°œ):\")\n",
        "        print(f\"{'í´ë˜ìŠ¤':<10} {'ì •ë°€ë„':<10} {'ì¬í˜„ìœ¨':<10} {'F1':<10} {'ìƒ˜í”Œìˆ˜':<10}\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, cls in enumerate(top_15_sub_labels):\n",
        "            print(f\"{cls:<10} {sub_precision[i]:<10.4f} {sub_recall[i]:<10.4f} {sub_f1[i]:<10.4f} {sub_support[i]:<10}\")\n",
        "\n",
        "        # Subclass í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
        "        plt.figure(figsize=(14, 12))\n",
        "        sns.heatmap(sub_cm_df, annot=True, fmt='d', cmap='Greens', cbar_kws={'label': 'ìƒ˜í”Œ ìˆ˜'})\n",
        "        plt.title('Subclass í˜¼ë™ í–‰ë ¬ (ìƒìœ„ 15ê°œ)', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('ì˜ˆì¸¡', fontsize=12)\n",
        "        plt.ylabel('ì‹¤ì œ', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.yticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Subclass ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "_43AXukmOEMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# ê²°í•© í´ë˜ìŠ¤ ë¶„ì„\n",
        "combined_df = pd.DataFrame({\n",
        "    'true': results_df['ground_truth'],\n",
        "    'pred': results_df['prediction']\n",
        "})\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸\n",
        "combined_counts = combined_df['true'].value_counts()\n",
        "print(f\"ì´ ê³ ìœ  ê²°í•© í´ë˜ìŠ¤ ìˆ˜: {len(combined_counts)}\")\n",
        "\n",
        "# ì „ì²´ ì •í™•ë„\n",
        "combined_accuracy = (combined_df['true'] == combined_df['pred']).mean()\n",
        "print(f\"\\nê²°í•© í´ë˜ìŠ¤ ì „ì²´ ì •í™•ë„: {combined_accuracy:.4f} ({combined_accuracy*100:.2f}%)\")\n",
        "\n",
        "# ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„\n",
        "print(\"\\nì£¼ìš” ì˜¤ë¶„ë¥˜ íŒ¨í„´:\")\n",
        "misclassifications = defaultdict(int)\n",
        "\n",
        "for _, row in combined_df.iterrows():\n",
        "    if row['true'] != row['pred']:\n",
        "        pattern = f\"{row['true']} -> {row['pred']}\"\n",
        "        misclassifications[pattern] += 1\n",
        "\n",
        "if len(misclassifications) > 0:\n",
        "    top_misclassifications = sorted(misclassifications.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"\\nì´ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ìˆ˜: {len(misclassifications)}\")\n",
        "    print(f\"ì´ ì˜¤ë¶„ë¥˜ íšŸìˆ˜: {sum(misclassifications.values())}\")\n",
        "\n",
        "    print(\"\\nìƒìœ„ 10ê°œ ì˜¤ë¶„ë¥˜ íŒ¨í„´:\")\n",
        "    print(f\"{'ì‹¤ì œ â†’ ì˜ˆì¸¡':<30} {'íšŸìˆ˜':<10} {'ë¹„ìœ¨':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "    total_errors = sum(misclassifications.values())\n",
        "    for pattern, count in top_misclassifications[:10]:\n",
        "        ratio = count / total_errors\n",
        "        print(f\"{pattern:<30} {count:<10} {ratio:<10.1%}\")\n",
        "else:\n",
        "    print(\"ì˜¤ë¶„ë¥˜ ì—†ìŒ! ì™„ë²½í•œ ì˜ˆì¸¡ì…ë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "TiTIMsi4OGHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}