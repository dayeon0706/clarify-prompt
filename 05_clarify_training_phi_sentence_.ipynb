{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "UtehPxm-e5l_"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv3epequNIRd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"사용 가능 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")"
      ],
      "metadata": {
        "id": "7VyHFcigNYPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#작업 경로 지정\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/woke-odds')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "YDLZoOWONcIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 로드\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': 'clarify_sft_train.jsonl',\n",
        "    'validation': 'clarify_sft_valid.jsonl',\n",
        "})\n",
        "print(f\"훈련 데이터: {len(dataset['train'])}개\")\n",
        "print(f\"검증 데이터: {len(dataset['validation'])}개\")"
      ],
      "metadata": {
        "id": "K-zNBCDTOPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "n4WRVEmOyRbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련"
      ],
      "metadata": {
        "id": "rYDQOUFvvx98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch"
      ],
      "metadata": {
        "id": "i0aIqHYIdG-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=False  # Gradient checkpointing과 호환되도록 설정\n",
        "    )"
      ],
      "metadata": {
        "id": "Oyu0abARfIgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LoRA Config 설정\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"qkv_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#requires_grad 확인\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"✅ {name}: requires_grad=True\")\n",
        "        break\n",
        "else:\n",
        "    print(\"❌ 학습 가능한 파라미터가 없습니다!\")"
      ],
      "metadata": {
        "id": "C7A64cZwdX8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat template이 올바르게 적용되는지 테스트\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"[EM|UNF] Test question?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Test answer.\"}\n",
        "]\n",
        "\n",
        "formatted_text = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False\n",
        ")\n",
        "\n",
        "print(\"=== Formatted Text ===\")\n",
        "print(formatted_text)\n",
        "print(\"\\n=== Tokenized ===\")\n",
        "tokens = tokenizer(formatted_text)\n",
        "print(f\"Token count: {len(tokens['input_ids'])}\")"
      ],
      "metadata": {
        "id": "Ubwiz0ffyd2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "def preprocess_function(examples):\n",
        "    # 'messages' 형식을 text로 변환 (Phi-4 chat template 적용)\n",
        "    texts = []\n",
        "    for messages in examples['messages']:\n",
        "        # Phi-4의 chat template 사용\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    # 토크나이즈\n",
        "    model_inputs = tokenizer(\n",
        "        texts,\n",
        "        max_length=768,\n",
        "        truncation=True,\n",
        "        padding=False  # DataCollator가 처리\n",
        "    )\n",
        "\n",
        "    # labels 설정 (CausalLM은 input_ids를 그대로 사용)\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "uaJMChQZygCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이즈 결과 확인\n",
        "print(\"=== 토크나이즈 확인 ===\")\n",
        "print(f\"Keys: {tokenized_dataset['train'].features}\")\n",
        "print(f\"Sample input_ids type: {type(tokenized_dataset['train'][0]['input_ids'])}\")\n",
        "print(f\"Sample input_ids length: {len(tokenized_dataset['train'][0]['input_ids'])}\")"
      ],
      "metadata": {
        "id": "HCDYGv0T6FeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#W&B 비활성화\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "ir4Rq89C_4Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "id": "lPF3VVV8MhZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TrainingArguments 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./clarifying_phi_v1',\n",
        "    num_train_epochs=3,\n",
        "    bf16=True,\n",
        "\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "\n",
        "    max_grad_norm=1.0,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    dataloader_pin_memory=False,\n",
        "    dataloader_num_workers=2, #데이터 로딩 병렬\n",
        "    torch_empty_cache_steps=50,\n",
        "\n",
        "    logging_dir='./logs_clarifying_phi_v1',\n",
        "    logging_steps= 25,\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=128,\n",
        "    save_steps=128,\n",
        "    save_safetensors=True,\n",
        "\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_total_limit=2,\n",
        "\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    report_to=[\"tensorboard\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Stg5FCrjOjjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collator\n",
        "\n",
        "#data_collator = DataCollatorForLanguageModeling(\n",
        "#    tokenizer=tokenizer,\n",
        "#    mlm=False\n",
        "#)"
      ],
      "metadata": {
        "id": "FpKP_0ojvS-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Data Collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # input_ids의 최대 길이 찾기\n",
        "        max_length = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        batch = {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": []\n",
        "        }\n",
        "\n",
        "        for feature in features:\n",
        "            input_ids = feature[\"input_ids\"]\n",
        "            attention_mask = feature[\"attention_mask\"]\n",
        "            labels = feature[\"labels\"]\n",
        "\n",
        "            # 패딩 길이 계산\n",
        "            padding_length = max_length - len(input_ids)\n",
        "\n",
        "            # 오른쪽에 패딩 추가\n",
        "            padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
        "            padded_attention_mask = attention_mask + [0] * padding_length\n",
        "            padded_labels = labels + [-100] * padding_length  # -100은 loss 계산에서 무시됨\n",
        "\n",
        "            batch[\"input_ids\"].append(padded_input_ids)\n",
        "            batch[\"attention_mask\"].append(padded_attention_mask)\n",
        "            batch[\"labels\"].append(padded_labels)\n",
        "\n",
        "        # 리스트를 텐서로 변환\n",
        "        batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Data Collator 생성\n",
        "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "uSosTgyd7cmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트: Collator 동작 확인\n",
        "test_features = [\n",
        "    tokenized_dataset[\"train\"][i] for i in range(2)\n",
        "]\n",
        "\n",
        "print(\"=== Collator 테스트 ===\")\n",
        "print(f\"샘플 1 길이: {len(test_features[0]['input_ids'])}\")\n",
        "print(f\"샘플 2 길이: {len(test_features[1]['input_ids'])}\")\n",
        "\n",
        "batch = data_collator(test_features)\n",
        "\n",
        "print(f\"\\n배치 shape:\")\n",
        "print(f\"input_ids: {batch['input_ids'].shape}\")\n",
        "print(f\"attention_mask: {batch['attention_mask'].shape}\")\n",
        "print(f\"labels: {batch['labels'].shape}\")\n",
        "print(f\"\\nlabels에서 -100 개수: {(batch['labels'] == -100).sum().item()}\")"
      ],
      "metadata": {
        "id": "j0x5PJh57sKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer 생성\n",
        "from transformers import EarlyStoppingCallback\n",
        "import torch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,  # 3번 연속 개선 없으면 중단\n",
        "            early_stopping_threshold=0.01  # 최소 개선 임계값\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0XNGgASKvVWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9klZUfAUveQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#베스트 모델 저장\n",
        "model.save_pretrained('./clarifying_phi_v1/checkpoint-best')\n",
        "tokenizer.save_pretrained('./clarifying_phi_v1/checkpoint-best')"
      ],
      "metadata": {
        "id": "5ZOVvc3vxeXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###평가"
      ],
      "metadata": {
        "id": "Wt-fU5CxvtIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re"
      ],
      "metadata": {
        "id": "f12e5HzoxZ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 경로 설정\n",
        "model_path = './clarifying_phi_v1/checkpoint-best'\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# 특수 토큰 설정이 필요한 경우 확인\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 모델 로드\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "# GPU 메모리 사용량 최적화\n",
        "if torch.cuda.is_available():\n",
        "    model = model.eval()\n",
        "\n",
        "# 데이터 콜레이터 설정\n",
        "data_collator = DataCollatorForCausalLM(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "4hg1VZ7-xgbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델로 테스트 (Trainer의 모델 사용)\n",
        "import torch\n",
        "\n",
        "# 테스트할 샘플 3개 선택\n",
        "test_samples = [tokenized_dataset[\"validation\"][i] for i in range(3)]\n",
        "\n",
        "print(\"=== 모델 출력 테스트 (3개 샘플) ===\\n\")\n",
        "\n",
        "for idx, sample_data in enumerate(test_samples):\n",
        "    # 원본 messages 가져오기 (토크나이즈 전 데이터에서)\n",
        "    original_sample = dataset[\"validation\"][idx]\n",
        "    messages = original_sample['messages']\n",
        "\n",
        "    # system + user 메시지만 사용\n",
        "    input_messages = [msg for msg in messages if msg['role'] != 'assistant']\n",
        "\n",
        "    # Chat template 적용\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        input_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # 토크나이즈\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # 디코딩 (입력 부분 제외)\n",
        "    generated_text = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # 정답 추출\n",
        "    ground_truth = [msg['content'] for msg in messages if msg['role'] == 'assistant'][0]\n",
        "    user_query = [msg['content'] for msg in messages if msg['role'] == 'user'][0]\n",
        "\n",
        "    # 출력\n",
        "    print(f\"[샘플 {idx+1}]\")\n",
        "    print(f\"User Query: {user_query}\")\n",
        "    print(f\"\\nGround Truth: {ground_truth}\")\n",
        "    print(f\"\\nModel Output: {generated_text.strip()}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "2iBEiwo_-5Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 데이터셋 선택\n",
        "eval_dataset = tokenized_dataset[\"validation\"]\n",
        "\n",
        "print(f\"테스트 샘플 수: {len(eval_dataset)}\")\n",
        "print(f\"첫 번째 샘플:\\n{eval_dataset[0]}\")"
      ],
      "metadata": {
        "id": "_BRrmnsI3P53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 예측 함수\n",
        "def generate_clarifying_question(messages, model, tokenizer, max_new_tokens=150):\n",
        "    \"\"\"\n",
        "    주어진 messages에 대해 명확화 질문 생성\n",
        "    \"\"\"\n",
        "    # system + user 메시지만 사용 (assistant 제외)\n",
        "    input_messages = [msg for msg in messages if msg['role'] != 'assistant']\n",
        "\n",
        "    # Chat template 적용\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        input_messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # assistant 응답 생성 프롬프트 추가\n",
        "    )\n",
        "\n",
        "    # 토크나이즈\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # 디코딩 (입력 부분 제외)\n",
        "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()"
      ],
      "metadata": {
        "id": "5WuQxLE7-NIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#원본 데이터셋 사용\n",
        "eval_dataset = dataset[\"validation\"]\n",
        "\n",
        "# 전체 테스트 데이터셋에 대해 예측 수행\n",
        "predictions = []\n",
        "ground_truths = []\n",
        "\n",
        "print(\"예측 시작...\")\n",
        "for example in tqdm(eval_dataset):\n",
        "    messages = example['messages']\n",
        "\n",
        "    # 모델 예측\n",
        "    predicted = generate_clarifying_question(messages, model, tokenizer)\n",
        "    predictions.append(predicted)\n",
        "\n",
        "    # 정답 (assistant의 응답)\n",
        "    ground_truth = [msg['content'] for msg in messages if msg['role'] == 'assistant'][0]\n",
        "    ground_truths.append(ground_truth)\n",
        "\n",
        "print(f\"예측 완료: {len(predictions)}개 샘플\")"
      ],
      "metadata": {
        "id": "7foi7dPC_PbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 DataFrame으로 정리\n",
        "results_df = pd.DataFrame({\n",
        "    'user_query': [msg['content'] for example in eval_dataset for msg in example['messages'] if msg['role'] == 'user'],\n",
        "    'ground_truth': ground_truths,\n",
        "    'prediction': predictions\n",
        "})\n",
        "\n",
        "# 처음 5개 결과 확인\n",
        "print(\"=== 예측 결과 샘플 ===\")\n",
        "for idx in range(min(5, len(results_df))):\n",
        "    print(f\"\\n[샘플 {idx+1}]\")\n",
        "    print(f\"Query: {results_df.iloc[idx]['user_query']}\")\n",
        "    print(f\"Ground Truth: {results_df.iloc[idx]['ground_truth']}\")\n",
        "    print(f\"Prediction: {results_df.iloc[idx]['prediction']}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "t1KwU0Pr_Rj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('clarify_phi_v1_pred_results.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "-MvhX9QFFrJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정성적 평가: <NO_CLARIFYING_QUESTION> 정확도\n",
        "def extract_no_clarification_tag(text):\n",
        "    \"\"\"텍스트에 <NO_CLARIFYING_QUESTION> 태그가 있는지 확인\"\"\"\n",
        "    return '<NO_CLARIFYING_QUESTION>' in text.upper()\n",
        "\n",
        "# 태그 존재 여부 비교\n",
        "gt_has_tag = [extract_no_clarification_tag(gt) for gt in ground_truths]\n",
        "pred_has_tag = [extract_no_clarification_tag(pred) for pred in predictions]\n",
        "\n",
        "tag_accuracy = accuracy_score(gt_has_tag, pred_has_tag)\n",
        "\n",
        "print(f\"\\n=== <NO_CLARIFYING_QUESTION> 태그 정확도 ===\")\n",
        "print(f\"정확도: {tag_accuracy:.2%}\")\n",
        "print(f\"Ground Truth에서 태그 있는 샘플: {sum(gt_has_tag)}/{len(gt_has_tag)}\")\n",
        "print(f\"Prediction에서 태그 있는 샘플: {sum(pred_has_tag)}/{len(pred_has_tag)}\")"
      ],
      "metadata": {
        "id": "M6Uh98b3_WSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Similarity & BERTScore"
      ],
      "metadata": {
        "id": "VZU_GTHmEb3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers bert-score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B_oNy6kO_Z8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from bert_score import score\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QpRAZqFqEjbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence-BERT 모델 로드\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 임베딩 생성\n",
        "pred_embeddings = semantic_model.encode(predictions, show_progress_bar=True)\n",
        "gt_embeddings = semantic_model.encode(ground_truths, show_progress_bar=True)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "semantic_scores = []\n",
        "for pred_emb, gt_emb in zip(pred_embeddings, gt_embeddings):\n",
        "    similarity = util.cos_sim(pred_emb, gt_emb).item()\n",
        "    semantic_scores.append(similarity)\n",
        "\n",
        "# 결과 통계\n",
        "avg_semantic = np.mean(semantic_scores)\n",
        "std_semantic = np.std(semantic_scores)\n",
        "min_semantic = np.min(semantic_scores)\n",
        "max_semantic = np.max(semantic_scores)\n",
        "\n",
        "print(\"=== Semantic Similarity 결과 ===\")\n",
        "print(f\"평균 유사도: {avg_semantic:.4f}\")\n",
        "print(f\"표준편차: {std_semantic:.4f}\")\n",
        "print(f\"최소값: {min_semantic:.4f}\")\n",
        "print(f\"최대값: {max_semantic:.4f}\")\n"
      ],
      "metadata": {
        "id": "NeRG6h79ElQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플별 Semantic Similarity 확인 (상위 5개, 하위 5개)\n",
        "semantic_df = pd.DataFrame({\n",
        "    'ground_truth': ground_truths,\n",
        "    'prediction': predictions,\n",
        "    'semantic_score': semantic_scores\n",
        "})\n",
        "\n",
        "semantic_df_sorted = semantic_df.sort_values('semantic_score', ascending=False)\n",
        "\n",
        "print(\"\\n=== Semantic Similarity 상위 5개 (가장 유사) ===\")\n",
        "for idx, row in semantic_df_sorted.head(5).iterrows():\n",
        "    print(f\"\\n[순위 {idx+1}] Score: {row['semantic_score']:.4f}\")\n",
        "    print(f\"GT: {row['ground_truth'][:100]}...\")\n",
        "    print(f\"Pred: {row['prediction'][:100]}...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n=== Semantic Similarity 하위 5개 (가장 불일치) ===\")\n",
        "for idx, row in semantic_df_sorted.tail(5).iterrows():\n",
        "    print(f\"\\n[순위 {idx+1}] Score: {row['semantic_score']:.4f}\")\n",
        "    print(f\"GT: {row['ground_truth'][:100]}...\")\n",
        "    print(f\"Pred: {row['prediction'][:100]}...\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "L7h_EAwWEz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore 계산\n",
        "P, R, F1 = score(\n",
        "    predictions,\n",
        "    ground_truths,\n",
        "    lang='en',\n",
        "    verbose=True,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# numpy로 변환\n",
        "P_scores = P.cpu().numpy()\n",
        "R_scores = R.cpu().numpy()\n",
        "F1_scores = F1.cpu().numpy()\n",
        "\n",
        "# 결과 통계\n",
        "avg_P = np.mean(P_scores)\n",
        "avg_R = np.mean(R_scores)\n",
        "avg_F1 = np.mean(F1_scores)\n",
        "\n",
        "print(\"=== BERTScore 결과 ===\")\n",
        "print(f\"평균 Precision: {avg_P:.4f}\")\n",
        "print(f\"평균 Recall: {avg_R:.4f}\")\n",
        "print(f\"평균 F1: {avg_F1:.4f}\")"
      ],
      "metadata": {
        "id": "qd0JV75HE8a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플별 BERTScore 확인 (상위 5개, 하위 5개)\n",
        "bertscore_df = pd.DataFrame({\n",
        "    'ground_truth': ground_truths,\n",
        "    'prediction': predictions,\n",
        "    'bert_P': P_scores,\n",
        "    'bert_R': R_scores,\n",
        "    'bert_F1': F1_scores\n",
        "})\n",
        "\n",
        "bertscore_df_sorted = bertscore_df.sort_values('bert_F1', ascending=False)\n",
        "\n",
        "print(\"\\n=== BERTScore F1 상위 5개 (가장 유사) ===\")\n",
        "for idx, row in bertscore_df_sorted.head(5).iterrows():\n",
        "    print(f\"\\n[순위 {idx+1}] P: {row['bert_P']:.4f}, R: {row['bert_R']:.4f}, F1: {row['bert_F1']:.4f}\")\n",
        "    print(f\"GT: {row['ground_truth'][:100]}...\")\n",
        "    print(f\"Pred: {row['prediction'][:100]}...\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n=== BERTScore F1 하위 5개 (가장 불일치) ===\")\n",
        "for idx, row in bertscore_df_sorted.tail(5).iterrows():\n",
        "    print(f\"\\n[순위 {idx+1}] P: {row['bert_P']:.4f}, R: {row['bert_R']:.4f}, F1: {row['bert_F1']:.4f}\")\n",
        "    print(f\"GT: {row['ground_truth'][:100]}...\")\n",
        "    print(f\"Pred: {row['prediction'][:100]}...\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "GDHMoxKJFC92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df['semantic_similarity'] = semantic_scores\n",
        "results_df['bert_F1'] = F1_scores\n",
        "\n",
        "print(results_df.head())\n",
        "print(f\"\\n컬럼 목록: {list(results_df.columns)}\")"
      ],
      "metadata": {
        "id": "WrU_FY_mFOh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('clarify_phi_v1_pred_results.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "wbORhAWBFFm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 지표 비교 분석\n",
        "combined_df = pd.DataFrame({\n",
        "    'ground_truth': ground_truths,\n",
        "    'prediction': predictions,\n",
        "    'semantic_similarity': semantic_scores,\n",
        "    'bert_F1': F1_scores\n",
        "})\n",
        "\n",
        "# 상관관계 분석\n",
        "correlation = np.corrcoef(semantic_scores, F1_scores)[0, 1]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 두 지표 비교 ===\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Semantic Similarity vs BERTScore F1 상관계수: {correlation:.4f}\")\n",
        "\n",
        "# 차이 계산\n",
        "combined_df['score_diff'] = abs(combined_df['semantic_similarity'] - combined_df['bert_F1'])\n",
        "combined_df_sorted = combined_df.sort_values('score_diff', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"=== 최종 종합 평가 ===\")\n",
        "print(\"=\"*80)\n",
        "print(f\"총 샘플 수: {len(predictions)}\")\n",
        "print(f\"\\nSemantic Similarity:\")\n",
        "print(f\"  - 평균: {avg_semantic:.4f}\")\n",
        "print(f\"  - 표준편차: {std_semantic:.4f}\")\n",
        "print(f\"\\nBERTScore:\")\n",
        "print(f\"  - 평균 Precision: {avg_P:.4f}\")\n",
        "print(f\"  - 평균 Recall: {avg_R:.4f}\")\n",
        "print(f\"  - 평균 F1: {avg_F1:.4f}\")\n",
        "print(f\"\\n상관계수: {correlation:.4f}\")"
      ],
      "metadata": {
        "id": "Pgo3U8IZHJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Similarity 하위 10개 조회\n",
        "bottom_10 = results_df.nsmallest(10, 'semantic_similarity')\n",
        "\n",
        "print(\"=== Semantic Similarity 하위 10개 ===\\n\")\n",
        "\n",
        "for idx, row in bottom_10.iterrows():\n",
        "    print(f\"User Query: {row['user_query']}\")\n",
        "    print(f\"Ground Truth: {row['ground_truth']}\")\n",
        "    print(f\"Prediction: {row['prediction']}\")\n",
        "    print(\"-\" * 100)\n",
        "    print()"
      ],
      "metadata": {
        "id": "Yn_py2JELlKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTscore 하위 10개 조회\n",
        "bottom_10 = results_df.nsmallest(10, 'bert_F1')\n",
        "\n",
        "print(\"=== bert_F1 하위 10개 ===\\n\")\n",
        "\n",
        "for idx, row in bottom_10.iterrows():\n",
        "    print(f\"User Query: {row['user_query']}\")\n",
        "    print(f\"Ground Truth: {row['ground_truth']}\")\n",
        "    print(f\"Prediction: {row['prediction']}\")\n",
        "    print(\"-\" * 100)\n",
        "    print()"
      ],
      "metadata": {
        "id": "JLSzq3SfNFm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}