{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "Ox60wyzU8v7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "AkNmUXxi8v7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 아키텍처\n",
        "print(model)"
      ],
      "metadata": {
        "id": "qcRp2yg2Aa_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 설정 확인\n",
        "print(model.config)"
      ],
      "metadata": {
        "id": "tNZEGZIDAl2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(f\"{name}: {module.__class__.__name__}\")"
      ],
      "metadata": {
        "id": "6Cw19JdlAw3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")"
      ],
      "metadata": {
        "id": "4CH-BAm7A4iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BOS token: {tokenizer.bos_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "print(f\"PAD token: {tokenizer.pad_token}\")\n",
        "print(f\"UNK token: {tokenizer.unk_token}\")"
      ],
      "metadata": {
        "id": "r-ZLVdYyBGnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat template 확인\n",
        "if hasattr(tokenizer, 'chat_template'):\n",
        "    print(f\"Chat template: {tokenizer.chat_template}\")"
      ],
      "metadata": {
        "id": "q2FjurcYBT0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-4가 지원하는 대화 형식 테스트\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
        "]\n",
        "\n",
        "# Chat template 적용\n",
        "formatted_input = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,  # 텍스트로 확인\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "print(\"Formatted input:\")\n",
        "print(formatted_input)"
      ],
      "metadata": {
        "id": "CDnC4n6MBhrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화된 형태 확인\n",
        "tokenized_input = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(f\"\\nTokenized shape: {tokenized_input.shape}\")"
      ],
      "metadata": {
        "id": "QHaPXM8zBoyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 파라미터 수\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n",
        "\n",
        "# 레이어별 파라미터 수\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape} ({param.numel():,} params)\")"
      ],
      "metadata": {
        "id": "aG-Ner2gBsnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7wCh8VtB3Od"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}