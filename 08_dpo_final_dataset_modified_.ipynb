{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOXWfDjphNCjf92N/iJUhZS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inVYnQ1IV2RF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "rpXrJ5xeYQVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import json"
      ],
      "metadata": {
        "id": "M3ueh0w_YTb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo_final_dataset.jsonl\")\n",
        "print(f\"Original dataset size: {len(dataset['train'])}\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "aDfQTamNYUz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_conversational_explicit_prompt(example):\n",
        "    \"\"\"\n",
        "    Convert from the old format to Conversational + Explicit prompt format\n",
        "\n",
        "    Old format:\n",
        "    {\n",
        "        \"prompt\": \"[LA|LEX] What is...\",\n",
        "        \"chosen\": \"Are you referring to...\",\n",
        "        \"rejected\": \"What do you mean...\"\n",
        "    }\n",
        "\n",
        "    New format:\n",
        "    {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": \"...\"},\n",
        "            {\"role\": \"user\", \"content\": \"...\"}\n",
        "        ],\n",
        "        \"chosen\": [\n",
        "            {\"role\": \"assistant\", \"content\": \"...\"}\n",
        "        ],\n",
        "        \"rejected\": [\n",
        "            {\"role\": \"assistant\", \"content\": \"...\"}\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    # System prompt (same as your SFT dataset)\n",
        "    system_content = \"\"\"You are an AI that generates a single, concise clarifying question when a user's query is ambiguous.\n",
        "\n",
        "Task:\n",
        "Generate exactly one clarifying question based on the ambiguity type.\n",
        "If the query is clear and needs no clarification, output: <NO_CLARIFYING_QUESTION>\n",
        "\n",
        "Output format: One clarifying question (or <NO_CLARIFYING_QUESTION> if not needed)\n",
        "\n",
        "Categories:\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "- NONE: Clear questions that don't require clarification\n",
        "\n",
        "Subclasses:\n",
        "For EM:\n",
        "- UNF (UNFAMILIAR): Query contains unfamiliar entities or facts\n",
        "- CONT (CONTRADICTION): Query contains self-contradictions\n",
        "\n",
        "For LA:\n",
        "- LEX (LEXICAL): Query contains terms with multiple meanings\n",
        "- SEM (SEMANTIC): Query lacks context leading to multiple interpretations\n",
        "\n",
        "For AO:\n",
        "- WHOM: Query output contains confusion due to missing personal elements\n",
        "- WHEN: Query output contains confusion due to missing temporal elements\n",
        "- WHERE: Query output contains confusion due to missing spatial elements\n",
        "- WHAT: Query output contains confusion due to missing task-specific elements\n",
        "\n",
        "For Clear Questions:\n",
        "- NONE: Use when require_clarification=0, output <NO_CLARIFYING_QUESTION>\"\"\"\n",
        "\n",
        "    # Create prompt with system and user messages\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": example['prompt']}\n",
        "    ]\n",
        "\n",
        "    # Create chosen (good response)\n",
        "    chosen = [\n",
        "        {\"role\": \"assistant\", \"content\": example['chosen']}\n",
        "    ]\n",
        "\n",
        "    # Create rejected (bad response)\n",
        "    rejected = [\n",
        "        {\"role\": \"assistant\", \"content\": example['rejected']}\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected\n",
        "    }"
      ],
      "metadata": {
        "id": "F49suIkvYWtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_dataset = dataset['train'].map(\n",
        "    convert_to_conversational_explicit_prompt,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "\n",
        "print(f\"Converted dataset size: {len(converted_dataset)}\")\n"
      ],
      "metadata": {
        "id": "di31lmMVYbCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"CONVERTED FORMAT - First Example:\")\n",
        "print(\"=\" * 80)\n",
        "print(json.dumps(converted_dataset[0], indent=2, ensure_ascii=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONVERTED FORMAT - Second Example:\")\n",
        "print(\"=\" * 80)\n",
        "print(json.dumps(converted_dataset[1], indent=2, ensure_ascii=False))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONVERTED FORMAT - Third Example:\")\n",
        "print(\"=\" * 80)\n",
        "print(json.dumps(converted_dataset[2], indent=2, ensure_ascii=False))\n"
      ],
      "metadata": {
        "id": "gJVGBO0aYdCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_structure(dataset, num_samples=5):\n",
        "    \"\"\"Verify that all samples have the correct structure\"\"\"\n",
        "    print(f\"\\nVerifying structure of {num_samples} samples...\")\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "\n",
        "        # Check required keys\n",
        "        assert 'prompt' in sample, f\"Sample {i}: Missing 'prompt' key\"\n",
        "        assert 'chosen' in sample, f\"Sample {i}: Missing 'chosen' key\"\n",
        "        assert 'rejected' in sample, f\"Sample {i}: Missing 'rejected' key\"\n",
        "\n",
        "        # Check prompt structure\n",
        "        assert isinstance(sample['prompt'], list), f\"Sample {i}: 'prompt' should be a list\"\n",
        "        assert len(sample['prompt']) >= 1, f\"Sample {i}: 'prompt' should have at least 1 message\"\n",
        "\n",
        "        # Check chosen structure\n",
        "        assert isinstance(sample['chosen'], list), f\"Sample {i}: 'chosen' should be a list\"\n",
        "        assert len(sample['chosen']) >= 1, f\"Sample {i}: 'chosen' should have at least 1 message\"\n",
        "        assert sample['chosen'][0]['role'] == 'assistant', f\"Sample {i}: 'chosen' should start with assistant\"\n",
        "\n",
        "        # Check rejected structure\n",
        "        assert isinstance(sample['rejected'], list), f\"Sample {i}: 'rejected' should be a list\"\n",
        "        assert len(sample['rejected']) >= 1, f\"Sample {i}: 'rejected' should have at least 1 message\"\n",
        "        assert sample['rejected'][0]['role'] == 'assistant', f\"Sample {i}: 'rejected' should start with assistant\"\n",
        "\n",
        "        print(f\"✓ Sample {i}: Valid structure\")\n",
        "\n",
        "    print(f\"\\n✅ All {num_samples} samples verified successfully!\")\n",
        "    return True\n",
        "\n",
        "verify_structure(converted_dataset, num_samples=10)"
      ],
      "metadata": {
        "id": "MVoiVOPvYfH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo_final_dataset_modified.jsonl\"\n",
        "\n",
        "# Save as JSONL with explicit key ordering\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for example in converted_dataset:\n",
        "        # Create ordered dict to ensure role comes before content\n",
        "        ordered_example = {\n",
        "            \"prompt\": [\n",
        "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
        "                for msg in example[\"prompt\"]\n",
        "            ],\n",
        "            \"chosen\": [\n",
        "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
        "                for msg in example[\"chosen\"]\n",
        "            ],\n",
        "            \"rejected\": [\n",
        "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
        "                for msg in example[\"rejected\"]\n",
        "            ]\n",
        "        }\n",
        "        f.write(json.dumps(ordered_example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✅ Converted dataset saved to: {output_path}\")\n",
        "print(f\"Total samples: {len(converted_dataset)}\")\n"
      ],
      "metadata": {
        "id": "I2U58fdSYrpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved file to verify\n",
        "verification_dataset = load_dataset(\"json\", data_files=output_path)\n",
        "print(f\"\\n✅ Verification: Successfully loaded {len(verification_dataset['train'])} samples from saved file\")\n",
        "\n",
        "print(\"\\nFirst sample from saved file:\")\n",
        "print(json.dumps(verification_dataset['train'][0], indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "YPIQElFcY_Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count messages in prompts\n",
        "prompt_lengths = [len(sample['prompt']) for sample in converted_dataset]\n",
        "print(f\"Average prompt length (messages): {sum(prompt_lengths) / len(prompt_lengths):.2f}\")\n",
        "print(f\"Min prompt length: {min(prompt_lengths)}\")\n",
        "print(f\"Max prompt length: {max(prompt_lengths)}\")\n",
        "\n",
        "# Count roles in prompts\n",
        "system_count = sum(1 for sample in converted_dataset if any(msg['role'] == 'system' for msg in sample['prompt']))\n",
        "print(f\"\\nSamples with system message: {system_count}/{len(converted_dataset)}\")\n",
        "\n",
        "print(\"\\n✅ Conversion completed successfully!\")\n"
      ],
      "metadata": {
        "id": "B2IxDL1VZCcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6A4ZW9l_ZFaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}