{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7PPB17z08PEij7tJzbAtD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "qBF6Kbh_J6pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import anthropic\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "print(\"라이브러리 임포트 완료!\")"
      ],
      "metadata": {
        "id": "qFif0xq9Ju9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV 불러오기\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo_with_candidates_full.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"데이터 로드 완료!\")\n",
        "print(f\"총 데이터 개수: {len(df)}개\")\n",
        "print(f\"컬럼: {df.columns.tolist()}\")\n",
        "\n",
        "# 샘플 확인\n",
        "print(\"\\n첫 번째 샘플:\")\n",
        "display(df.head(1))"
      ],
      "metadata": {
        "id": "S8eq_vppJ4fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude API 키 입력 (실행 시 입력)\n",
        "import getpass\n",
        "\n",
        "api_key = getpass.getpass('Claude API Key를 입력하세요: ')\n",
        "client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "print(\"Claude API 클라이언트 설정 완료!\")"
      ],
      "metadata": {
        "id": "ACJ28RgKKCfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_candidates_with_rlaif(user_query, ground_truth, candidates):\n",
        "    \"\"\"\n",
        "    Claude API로 후보들을 평가하고 best/rejected 선택\n",
        "\n",
        "    Args:\n",
        "        user_query: 사용자 질문 (카테고리 포함)\n",
        "        ground_truth: 정답 clarifying question\n",
        "        candidates: 생성된 후보 5개 리스트\n",
        "\n",
        "    Returns:\n",
        "        dict: {'best_idx', 'rejected_idx', 'scores', 'reasoning'}\n",
        "    \"\"\"\n",
        "\n",
        "    # 카테고리 추출\n",
        "    category = user_query.split(']')[0] + ']'\n",
        "\n",
        "    # 프롬프트 생성\n",
        "    prompt = f\"\"\"You are an expert rater of clarifying questions for ambiguous queries.\n",
        "\n",
        "AMBIGUITY CATEGORIES GUIDE:\n",
        "\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "  * UNF (UNFAMILIAR): Query contains unfamiliar entities or facts\n",
        "  * CONT (CONTRADICTION): Query contains self-contradictions\n",
        "\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "  * LEX (LEXICAL): Query contains terms with multiple meanings\n",
        "  * SEM (SEMANTIC): Query lacks context leading to multiple interpretations\n",
        "\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "  * WHOM: Missing information about WHO (person/agent)\n",
        "  * WHEN: Missing temporal information\n",
        "  * WHERE: Missing spatial/location information\n",
        "  * WHAT: Missing task-specific or object information\n",
        "\n",
        "- NONE: Clear questions that don't require clarification\n",
        "  * Expected response: <NO_CLARIFYING_QUESTION>\n",
        "\n",
        "---\n",
        "\n",
        "EVALUATION TASK:\n",
        "\n",
        "Query: {user_query}\n",
        "Category: {category}\n",
        "\n",
        "Reference Answer (Ground Truth):\n",
        "{ground_truth}\n",
        "\n",
        "Generated Candidates to Evaluate:\n",
        "1. {candidates[0]}\n",
        "2. {candidates[1]}\n",
        "3. {candidates[2]}\n",
        "4. {candidates[3]}\n",
        "5. {candidates[4]}\n",
        "\n",
        "---\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Step 1 - Analyze each candidate:\n",
        "- Does it correctly address the specific ambiguity type indicated in the category?\n",
        "- For example, if category is [AO|WHOM], does it ask about WHO is involved?\n",
        "- Is it clear, specific, and easy to understand?\n",
        "- How does it compare to the reference answer?\n",
        "\n",
        "Step 2 - Assign scores (0-100):\n",
        "- 85-100: Excellent (addresses ambiguity type correctly, comparable to reference)\n",
        "- 70-84: Good (addresses ambiguity but slightly less effective)\n",
        "- 55-69: Acceptable (addresses core issue but noticeably lacking in clarity or specificity)\n",
        "- 40-54: Weak (partially addresses ambiguity or misses key aspects)\n",
        "- 0-39: Poor (doesn't address the right ambiguity type or is ineffective)\n",
        "\n",
        "Step 3 - Select candidates:\n",
        "- BEST: Choose the highest scoring candidate\n",
        "- REJECTED: Choose a candidate in the 60-75 point range\n",
        "  * IMPORTANT: The rejected candidate MUST be different from the best candidate\n",
        "  * If the best candidate is in 60-75 range, choose the SECOND best candidate in that range\n",
        "  * If no other candidate is in 60-75 range, choose the candidate closest to 67 points (excluding the best)\n",
        "  * Avoid candidates above 80 or below 50\n",
        "\n",
        "Provide your response in JSON format ONLY:\n",
        "{{\n",
        "  \"analysis\": \"Brief analysis of all candidates\",\n",
        "  \"scores\": [score1, score2, score3, score4, score5],\n",
        "  \"best_index\": 1-5,\n",
        "  \"rejected_index\": 1-5,\n",
        "  \"rejected_score\": actual_rejected_score,\n",
        "  \"reasoning\": \"Why this rejected candidate has appropriate quality gap\"\n",
        "}}\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Claude API 호출\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-sonnet-4-20250514\",\n",
        "            max_tokens=2000,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # 응답 파싱\n",
        "        response_text = message.content[0].text\n",
        "\n",
        "        # JSON 추출 (마크다운 코드블록 제거)\n",
        "        response_text = response_text.replace('```json\\n', '').replace('\\n```', '').strip()\n",
        "        if response_text.startswith('```'):\n",
        "            response_text = response_text.split('\\n', 1)[1]\n",
        "        if response_text.endswith('```'):\n",
        "            response_text = response_text.rsplit('\\n', 1)[0]\n",
        "\n",
        "        # JSON 파싱\n",
        "        result = json.loads(response_text)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 발생: {e}\")\n",
        "        print(f\"응답 텍스트: {response_text[:200] if 'response_text' in locals() else 'N/A'}...\")\n",
        "        return None\n",
        "\n",
        "print(\"✓ RLAIF 평가 함수 정의 완료!\")"
      ],
      "metadata": {
        "id": "hldu56xdLKW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_candidates_with_rlaif(user_query, ground_truth, candidates):\n",
        "    \"\"\"\n",
        "    Claude API로 후보들을 평가하고 rejected만 선택\n",
        "\n",
        "    Args:\n",
        "        user_query: 사용자 질문 (카테고리 포함)\n",
        "        ground_truth: 정답 clarifying question (이게 chosen!)\n",
        "        candidates: 생성된 후보 5개 리스트\n",
        "\n",
        "    Returns:\n",
        "        dict: {'rejected_idx', 'rejected_score', 'scores', 'reasoning'}\n",
        "    \"\"\"\n",
        "\n",
        "    # 카테고리 추출\n",
        "    category = user_query.split(']')[0] + ']'\n",
        "\n",
        "    # 프롬프트 생성\n",
        "    prompt = f\"\"\"You are an expert rater of clarifying questions for ambiguous queries.\n",
        "\n",
        "AMBIGUITY CATEGORIES GUIDE:\n",
        "\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "  * UNF (UNFAMILIAR): Query contains unfamiliar entities or facts\n",
        "  * CONT (CONTRADICTION): Query contains self-contradictions\n",
        "\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "  * LEX (LEXICAL): Query contains terms with multiple meanings\n",
        "  * SEM (SEMANTIC): Query lacks context leading to multiple interpretations\n",
        "\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "  * WHOM: Missing information about WHO (person/agent)\n",
        "  * WHEN: Missing temporal information\n",
        "  * WHERE: Missing spatial/location information\n",
        "  * WHAT: Missing task-specific or object information\n",
        "\n",
        "- NONE: Clear questions that don't require clarification\n",
        "  * Expected response: <NO_CLARIFYING_QUESTION>\n",
        "\n",
        "---\n",
        "\n",
        "EVALUATION TASK:\n",
        "\n",
        "Query: {user_query}\n",
        "Category: {category}\n",
        "\n",
        "Reference Answer (Ground Truth - this is the CHOSEN response):\n",
        "{ground_truth}\n",
        "\n",
        "Generated Candidates to Evaluate:\n",
        "1. {candidates[0]}\n",
        "2. {candidates[1]}\n",
        "3. {candidates[2]}\n",
        "4. {candidates[3]}\n",
        "5. {candidates[4]}\n",
        "\n",
        "---\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Step 1 - Analyze and score each candidate (0-100):\n",
        "- Does it correctly address the specific ambiguity type indicated in the category?\n",
        "- Is it clear, specific, and easy to understand?\n",
        "- How does it compare to the reference answer?\n",
        "\n",
        "Scoring guide:\n",
        "- 85-100: Excellent (addresses ambiguity type correctly, comparable to reference)\n",
        "- 70-84: Good (addresses ambiguity but slightly less effective)\n",
        "- 55-69: Acceptable (addresses core issue but noticeably lacking in clarity or specificity)\n",
        "- 40-54: Weak (partially addresses ambiguity or misses key aspects)\n",
        "- 0-39: Poor (doesn't address the right ambiguity type or is ineffective)\n",
        "\n",
        "Step 2 - Select the REJECTED candidate:\n",
        "- Choose a candidate in the 60-75 point range\n",
        "- This range is CRITICAL for effective learning in Direct Preference Optimization (DPO)\n",
        "- The rejected candidate should be \"noticeably worse but not terrible\"\n",
        "- If multiple candidates are in 60-75 range, pick the one closest to 67\n",
        "- If NO candidate is in 60-75 range, pick the one closest to this range\n",
        "- Avoid candidates above 80 (too similar to reference) or below 50 (too poor to learn from)\n",
        "\n",
        "Provide your response in JSON format ONLY:\n",
        "{{\n",
        "  \"analysis\": \"Brief analysis of all candidates\",\n",
        "  \"scores\": [score1, score2, score3, score4, score5],\n",
        "  \"rejected_index\": 1-5,\n",
        "  \"rejected_score\": actual_rejected_score,\n",
        "  \"reasoning\": \"Why this rejected candidate has appropriate quality gap for DPO training\"\n",
        "}}\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Claude API 호출\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-sonnet-4-20250514\",\n",
        "            max_tokens=2000,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # 응답 파싱\n",
        "        response_text = message.content[0].text\n",
        "\n",
        "        # JSON 추출 (마크다운 코드블록 제거)\n",
        "        response_text = response_text.replace('```json\\n', '').replace('\\n```', '').strip()\n",
        "        if response_text.startswith('```'):\n",
        "            response_text = response_text.split('\\n', 1)[1]\n",
        "        if response_text.endswith('```'):\n",
        "            response_text = response_text.rsplit('\\n', 1)[0]\n",
        "\n",
        "        # JSON 파싱\n",
        "        result = json.loads(response_text)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"에러 발생: {e}\")\n",
        "        print(f\"응답 텍스트: {response_text[:200] if 'response_text' in locals() else 'N/A'}...\")\n",
        "        return None\n",
        "\n",
        "print(\"✓ RLAIF 평가 함수 정의 완료!\")"
      ],
      "metadata": {
        "id": "2Fc2mC7KPmCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3개 샘플로 출력 테스트"
      ],
      "metadata": {
        "id": "hgbmDiL_SYEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3개 샘플로 테스트\n",
        "print(\"=\"*80)\n",
        "print(\"3개 샘플로 RLAIF 테스트\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "df_test = df.head(3).copy()\n",
        "\n",
        "for idx in range(len(df_test)):\n",
        "    row = df_test.iloc[idx]\n",
        "\n",
        "    print(f\"\\n[샘플 {idx+1}/3]\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"User Query: {row['user_query']}\")\n",
        "    print(f\"Ground Truth: {row['ground_truth']}\")\n",
        "\n",
        "    # 후보 리스트\n",
        "    candidates = [\n",
        "        row['candidate_1'],\n",
        "        row['candidate_2'],\n",
        "        row['candidate_3'],\n",
        "        row['candidate_4'],\n",
        "        row['candidate_5']\n",
        "    ]\n",
        "\n",
        "    print(\"\\n후보들:\")\n",
        "    for i, cand in enumerate(candidates, 1):\n",
        "        print(f\"  {i}. {cand}\")\n",
        "\n",
        "    # RLAIF 평가\n",
        "    print(\"\\nClaude 평가 중...\")\n",
        "    result = evaluate_candidates_with_rlaif(\n",
        "        row['user_query'],\n",
        "        row['ground_truth'],\n",
        "        candidates\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\n✓ 평가 완료!\")\n",
        "        print(f\"점수: {result['scores']}\")\n",
        "        print(f\"Rejected: 후보 {result['rejected_index']}\")\n",
        "        print(f\"이유: {result['reasoning']}\")\n",
        "    else:\n",
        "        print(\"\\n✗ 평가 실패\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "    # Rate limit 방지\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n✓ 테스트 완료!\")"
      ],
      "metadata": {
        "id": "lUfXjNMaLUSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2개 샘플로 df 잘 저장되나 테스트"
      ],
      "metadata": {
        "id": "mgkvBHG7SVM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2개 샘플만 추출\n",
        "df_test_2 = df.head(2).copy()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"2개 샘플로 RLAIF 테스트 및 저장 확인\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# 결과 저장할 컬럼 추가\n",
        "df_test_2['rejected_candidate_idx'] = None\n",
        "df_test_2['rejected_text'] = None  # ← 추가!\n",
        "df_test_2['rejected_score'] = None\n",
        "df_test_2['all_scores'] = None\n",
        "df_test_2['rlaif_reasoning'] = None\n",
        "\n",
        "# 각 샘플 처리\n",
        "for idx in range(len(df_test_2)):\n",
        "    row = df_test_2.iloc[idx]\n",
        "\n",
        "    print(f\"\\n[샘플 {idx+1}/2]\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"User Query: {row['user_query'][:80]}...\")\n",
        "    print(f\"Ground Truth: {row['ground_truth'][:80]}...\")\n",
        "\n",
        "    # 후보 리스트\n",
        "    candidates = [\n",
        "        row['candidate_1'],\n",
        "        row['candidate_2'],\n",
        "        row['candidate_3'],\n",
        "        row['candidate_4'],\n",
        "        row['candidate_5']\n",
        "    ]\n",
        "\n",
        "    # RLAIF 평가\n",
        "    print(\"Claude 평가 중...\")\n",
        "    result = evaluate_candidates_with_rlaif(\n",
        "        row['user_query'],\n",
        "        row['ground_truth'],\n",
        "        candidates\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        rejected_idx = result['rejected_index']\n",
        "        rejected_text = candidates[rejected_idx - 1]  # 텍스트 추출\n",
        "\n",
        "        # DataFrame에 저장 (둘 다!)\n",
        "        df_test_2.at[idx, 'rejected_candidate_idx'] = rejected_idx\n",
        "        df_test_2.at[idx, 'rejected_text'] = rejected_text  # ← 텍스트 저장!\n",
        "        df_test_2.at[idx, 'rejected_score'] = result['rejected_score']\n",
        "        df_test_2.at[idx, 'all_scores'] = str(result['scores'])\n",
        "        df_test_2.at[idx, 'rlaif_reasoning'] = result['reasoning']\n",
        "\n",
        "        print(f\"✓ 저장 완료!\")\n",
        "        print(f\"  - Rejected: 후보 {rejected_idx} (점수: {result['rejected_score']})\")\n",
        "    else:\n",
        "        print(\"✗ 평가 실패\")\n",
        "\n",
        "    # Rate limit 방지\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✓ 테스트 완료!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "dKDuqb4aLVwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 컬럼 확인\n",
        "print(\"저장된 DataFrame 정보:\")\n",
        "print(f\"Shape: {df_test_2.shape}\")\n",
        "print(f\"컬럼: {df_test_2.columns.tolist()}\\n\")\n",
        "\n",
        "# 주요 컬럼만 표시\n",
        "print(\"=\"*80)\n",
        "print(\"저장된 데이터 확인:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "display(df_test_2)"
      ],
      "metadata": {
        "id": "xvuUeaukROkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 저장할 컬럼 추가\n",
        "df['rejected_candidate_idx'] = None\n",
        "df['rejected_text'] = None\n",
        "df['rejected_score'] = None\n",
        "df['all_scores'] = None\n",
        "df['rlaif_reasoning'] = None\n",
        "\n",
        "print(f\"전체 {len(df)}개 데이터 RLAIF 평가 시작...\")\n",
        "\n",
        "# 배치 처리\n",
        "batch_size = 50\n",
        "total_batches = (len(df) + batch_size - 1) // batch_size\n",
        "\n",
        "for batch_idx in range(total_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min((batch_idx + 1) * batch_size, len(df))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"[Batch {batch_idx+1}/{total_batches}] Processing {start_idx}-{end_idx}\")\n",
        "    print('='*80)\n",
        "\n",
        "    for idx in tqdm(range(start_idx, end_idx), desc=f\"Batch {batch_idx+1}\"):\n",
        "        row = df.iloc[idx]\n",
        "\n",
        "        # 후보 리스트\n",
        "        candidates = [\n",
        "            row['candidate_1'],\n",
        "            row['candidate_2'],\n",
        "            row['candidate_3'],\n",
        "            row['candidate_4'],\n",
        "            row['candidate_5']\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # RLAIF 평가\n",
        "            result = evaluate_candidates_with_rlaif(\n",
        "                row['user_query'],\n",
        "                row['ground_truth'],\n",
        "                candidates\n",
        "            )\n",
        "\n",
        "            if result:\n",
        "                rejected_idx = result['rejected_index']\n",
        "                rejected_text = candidates[rejected_idx - 1]\n",
        "\n",
        "                df.at[idx, 'rejected_candidate_idx'] = rejected_idx\n",
        "                df.at[idx, 'rejected_text'] = rejected_text\n",
        "                df.at[idx, 'rejected_score'] = result['rejected_score']\n",
        "                df.at[idx, 'all_scores'] = str(result['scores'])\n",
        "                df.at[idx, 'rlaif_reasoning'] = result['reasoning']\n",
        "\n",
        "            # Rate limit 방지\n",
        "            time.sleep(1.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n에러 (idx={idx}): {e}\")\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "\n",
        "    # 중간 저장\n",
        "    temp_path = f'/content/drive/MyDrive/Colab Notebooks/woke-odds/rlaif_batch_{batch_idx+1}.csv'\n",
        "    df.iloc[:end_idx].to_csv(temp_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n✓ 중간 저장: {temp_path}\")\n",
        "\n",
        "print(\"\\n전체 RLAIF 평가 완료!\")\n",
        "\n",
        "# 최종 저장\n",
        "final_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/rlaif_full_results.csv'\n",
        "df.to_csv(final_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"최종 저장: {final_path}\")"
      ],
      "metadata": {
        "id": "YIfU6If8RWvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO 포맷으로 변환\n",
        "dpo_dataset = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    dpo_dataset.append({\n",
        "        'prompt': row['user_query'],\n",
        "        'chosen': row['ground_truth'],\n",
        "        'rejected': row['rejected_text']\n",
        "    })\n",
        "\n",
        "df_dpo = pd.DataFrame(dpo_dataset)\n",
        "\n",
        "# 저장\n",
        "dpo_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo_final_dataset.jsonl'\n",
        "with open(dpo_path, 'w', encoding='utf-8') as f:\n",
        "    for idx, row in df_dpo.iterrows():\n",
        "        f.write(json.dumps(row.to_dict(), ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"✓ DPO 데이터셋 저장 완료: {dpo_path}\")"
      ],
      "metadata": {
        "id": "jBFJIKdMRKoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4R12TEBRen_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}