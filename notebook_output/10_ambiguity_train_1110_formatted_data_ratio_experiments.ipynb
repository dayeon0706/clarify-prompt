{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS75RPYHad0C",
        "outputId": "3a87a2b6-286a-4cfd-b79c-fe53da7b9b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# None만 절반으로 줄이기\n",
        "# 이미 formatted된 데이터 불러오기\n",
        "\n",
        "import json\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "# Formatted 데이터 로드\n",
        "formatted_train = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl')\n",
        "\n",
        "print(f\"원본 데이터: {len(formatted_train)}개\")\n",
        "\n",
        "# NONE과 기타 분류 (text 필드에서 확인)\n",
        "none_samples = []\n",
        "other_samples = []\n",
        "\n",
        "for sample in formatted_train:\n",
        "    # text에서 답변 부분 확인 ([/INST] 뒤)\n",
        "    if 'NONE|NONE' in sample['text']:\n",
        "        none_samples.append(sample)\n",
        "    else:\n",
        "        other_samples.append(sample)\n",
        "\n",
        "print(f\"\\n=== 원본 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개 (50.0%)\")\n",
        "print(f\"기타: {len(other_samples)}개 (50.0%)\")\n",
        "\n",
        "# NONE을 25%로\n",
        "none_target = len(other_samples) // 3\n",
        "sampled_none = random.sample(none_samples, none_target)\n",
        "\n",
        "# 합치기\n",
        "balanced_data = other_samples + sampled_none\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "print(f\"\\n=== 조정 후 분포 ===\")\n",
        "print(f\"NONE: {len(sampled_none)}개 ({len(sampled_none)/len(balanced_data)*100:.1f}%)\")\n",
        "print(f\"기타: {len(other_samples)}개 ({len(other_samples)/len(balanced_data)*100:.1f}%)\")\n",
        "print(f\"총: {len(balanced_data)}개\")\n",
        "\n",
        "# 저장\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_25.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in balanced_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✓ 저장 완료: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbbG7JrpagD3",
        "outputId": "da83935f-3ec7-458d-e925-259e8da31a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 데이터: 2561개\n",
            "\n",
            "=== 원본 분포 ===\n",
            "NONE: 1280개 (50.0%)\n",
            "기타: 1281개 (50.0%)\n",
            "\n",
            "=== 조정 후 분포 ===\n",
            "NONE: 427개 (25.0%)\n",
            "기타: 1281개 (75.0%)\n",
            "총: 1708개\n",
            "\n",
            "✓ 저장 완료: /content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_25.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling(NONE) + Oversampling(AO 2배)\n",
        "\n",
        "import json\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "# Formatted 데이터 로드\n",
        "formatted_train = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl')\n",
        "\n",
        "print(f\"원본 데이터: {len(formatted_train)}개\")\n",
        "\n",
        "# NONE, AO, 기타로 분류\n",
        "none_samples = []\n",
        "ao_samples = []\n",
        "other_samples = []\n",
        "\n",
        "for sample in formatted_train:\n",
        "    text = sample['text']\n",
        "    if 'NONE|NONE' in text:\n",
        "        none_samples.append(sample)\n",
        "    elif 'AO|' in text:\n",
        "        ao_samples.append(sample)\n",
        "    else:\n",
        "        other_samples.append(sample)\n",
        "\n",
        "print(f\"\\n=== 원본 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개 (50.0%)\")\n",
        "print(f\"AO: {len(ao_samples)}개\")\n",
        "print(f\"기타(EM/LA): {len(other_samples)}개\")\n",
        "print(f\"총: {len(formatted_train)}개\")\n",
        "\n",
        "# AO 2배 복제\n",
        "ao_duplicated = ao_samples + ao_samples\n",
        "\n",
        "# NONE 조정 (available 개수와 target 비교)\n",
        "total_non_none = len(ao_duplicated) + len(other_samples)\n",
        "none_target = int(total_non_none * 0.4 / 0.6)\n",
        "\n",
        "# NONE이 부족하면 전부 사용\n",
        "if none_target > len(none_samples):\n",
        "    print(f\"\\n⚠️ none_target({none_target})이 너무 많아서 전체 NONE 사용\")\n",
        "    sampled_none = none_samples  # 전부 사용\n",
        "else:\n",
        "    sampled_none = random.sample(none_samples, none_target)\n",
        "\n",
        "# 합치기\n",
        "balanced_data = sampled_none + ao_duplicated + other_samples\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "print(f\"\\n=== 조정 후 분포 ===\")\n",
        "print(f\"NONE: {len(sampled_none)}개 ({len(sampled_none)/len(balanced_data)*100:.1f}%)\")\n",
        "print(f\"AO: {len(ao_duplicated)}개 (×2 복제!)\")\n",
        "print(f\"기타(EM/LA): {len(other_samples)}개\")\n",
        "print(f\"총: {len(balanced_data)}개 (원본: {len(formatted_train)}개)\")\n",
        "\n",
        "# 저장\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_ao2x.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in balanced_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✓ 저장 완료: {output_path}\")\n",
        "print(f\"파일명: ambiguity_train_1110_formatted_balanced_ao2x.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQM2VNzCb3BF",
        "outputId": "4ba531ae-5194-4e0c-9fc5-e112ad518db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 데이터: 2561개\n",
            "\n",
            "=== 원본 분포 ===\n",
            "NONE: 1280개 (50.0%)\n",
            "AO: 641개\n",
            "기타(EM/LA): 640개\n",
            "총: 2561개\n",
            "\n",
            "⚠️ none_target(1281)이 너무 많아서 전체 NONE 사용\n",
            "\n",
            "=== 조정 후 분포 ===\n",
            "NONE: 1280개 (40.0%)\n",
            "AO: 1282개 (×2 복제!)\n",
            "기타(EM/LA): 640개\n",
            "총: 3202개 (원본: 2561개)\n",
            "\n",
            "✓ 저장 완료: /content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_ao2x.jsonl\n",
            "파일명: ambiguity_train_1110_formatted_balanced_ao2x.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AO 카테고리랑 UNF만 1.5배\n",
        "# 세분화된 분류\n",
        "\n",
        "import json\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "formatted_train = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl')\n",
        "\n",
        "none_samples = []\n",
        "ao_samples = []\n",
        "em_unf_samples = []\n",
        "em_cont_samples = []\n",
        "la_samples = []\n",
        "\n",
        "for sample in formatted_train:\n",
        "    text = sample['text']\n",
        "    if 'NONE|NONE' in text:\n",
        "        none_samples.append(sample)\n",
        "    elif 'AO|' in text:\n",
        "        ao_samples.append(sample)\n",
        "    elif 'EM|UNF' in text:\n",
        "        em_unf_samples.append(sample)\n",
        "    elif 'EM|CONT' in text:\n",
        "        em_cont_samples.append(sample)\n",
        "    elif 'LA|' in text:\n",
        "        la_samples.append(sample)\n",
        "\n",
        "print(f\"=== 원본 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개\")\n",
        "print(f\"AO: {len(ao_samples)}개\")\n",
        "print(f\"EM|UNF: {len(em_unf_samples)}개\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개\")\n",
        "print(f\"LA: {len(la_samples)}개\")\n",
        "\n",
        "# AO 1.5배\n",
        "ao_15x = ao_samples + [s for s in ao_samples if random.random() < 0.5]\n",
        "\n",
        "# UNF만 1.5배\n",
        "unf_15x = em_unf_samples + [s for s in em_unf_samples if random.random() < 0.5]\n",
        "\n",
        "# 나머지는 그대로\n",
        "balanced_data = none_samples + ao_15x + unf_15x + em_cont_samples + la_samples\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "print(f\"\\n=== 조정 후 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개 (유지)\")\n",
        "print(f\"AO: {len(ao_15x)}개 (×1.5)\")\n",
        "print(f\"EM|UNF: {len(unf_15x)}개 (×1.5) ⭐\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개 (유지)\")\n",
        "print(f\"LA: {len(la_samples)}개 (유지)\")\n",
        "print(f\"총: {len(balanced_data)}개\")\n",
        "\n",
        "# 저장\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_ao15x_unf15x.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in balanced_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✓ 저장 완료: ao15x_unf15x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcffRKg75eyH",
        "outputId": "0fa44b59-1e6f-4403-ca24-3a8c31ba23b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 원본 분포 ===\n",
            "NONE: 1280개\n",
            "AO: 641개\n",
            "EM|UNF: 160개\n",
            "EM|CONT: 160개\n",
            "LA: 320개\n",
            "\n",
            "=== 조정 후 분포 ===\n",
            "NONE: 1280개 (유지)\n",
            "AO: 961개 (×1.5)\n",
            "EM|UNF: 232개 (×1.5) ⭐\n",
            "EM|CONT: 160개 (유지)\n",
            "LA: 320개 (유지)\n",
            "총: 2953개\n",
            "\n",
            "✓ 저장 완료: ao15x_unf15x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AO 2배, UNF 1.5배\n",
        "\n",
        "import json\n",
        "import random\n",
        "random.seed(44)  # 새로운 시드\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "formatted_train = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl')\n",
        "\n",
        "none_samples = []\n",
        "ao_samples = []\n",
        "em_unf_samples = []\n",
        "em_cont_samples = []\n",
        "la_samples = []\n",
        "\n",
        "for sample in formatted_train:\n",
        "    text = sample['text']\n",
        "    if 'NONE|NONE' in text:\n",
        "        none_samples.append(sample)\n",
        "    elif 'AO|' in text:\n",
        "        ao_samples.append(sample)\n",
        "    elif 'EM|UNF' in text:\n",
        "        em_unf_samples.append(sample)\n",
        "    elif 'EM|CONT' in text:\n",
        "        em_cont_samples.append(sample)\n",
        "    elif 'LA|' in text:\n",
        "        la_samples.append(sample)\n",
        "\n",
        "print(f\"=== 원본 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개\")\n",
        "print(f\"AO: {len(ao_samples)}개\")\n",
        "print(f\"EM|UNF: {len(em_unf_samples)}개\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개\")\n",
        "print(f\"LA: {len(la_samples)}개\")\n",
        "\n",
        "# AO 2배 (리스트 2번 더하기!)\n",
        "ao_2x = ao_samples + ao_samples\n",
        "\n",
        "# UNF 1.5배\n",
        "unf_15x = em_unf_samples + [s for s in em_unf_samples if random.random() < 0.5]\n",
        "\n",
        "# 나머지는 그대로 (NONE 유지!)\n",
        "balanced_data = none_samples + ao_2x + unf_15x + em_cont_samples + la_samples\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "print(f\"\\n=== 조정 후 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개 (유지)\")\n",
        "print(f\"AO: {len(ao_2x)}개 (×2) ⭐\")\n",
        "print(f\"EM|UNF: {len(unf_15x)}개 (×1.5) ⭐\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개 (유지)\")\n",
        "print(f\"LA: {len(la_samples)}개 (유지)\")\n",
        "print(f\"총: {len(balanced_data)}개\")\n",
        "\n",
        "# 비율 출력\n",
        "total = len(balanced_data)\n",
        "print(f\"\\n=== 비율 ===\")\n",
        "print(f\"NONE: {len(none_samples)/total*100:.1f}%\")\n",
        "print(f\"AO: {len(ao_2x)/total*100:.1f}%\")\n",
        "print(f\"EM|UNF: {len(unf_15x)/total*100:.1f}%\")\n",
        "\n",
        "# 저장\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_ao2x_unf15x.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in balanced_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✓ 저장 완료: ao2x_unf15x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxSXwP4ckGxA",
        "outputId": "5abdae80-6cee-4692-8608-dfb899494649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 원본 분포 ===\n",
            "NONE: 1280개\n",
            "AO: 641개\n",
            "EM|UNF: 160개\n",
            "EM|CONT: 160개\n",
            "LA: 320개\n",
            "\n",
            "=== 조정 후 분포 ===\n",
            "NONE: 1280개 (유지)\n",
            "AO: 1282개 (×2) ⭐\n",
            "EM|UNF: 243개 (×1.5) ⭐\n",
            "EM|CONT: 160개 (유지)\n",
            "LA: 320개 (유지)\n",
            "총: 3285개\n",
            "\n",
            "=== 비율 ===\n",
            "NONE: 39.0%\n",
            "AO: 39.0%\n",
            "EM|UNF: 7.4%\n",
            "\n",
            "✓ 저장 완료: ao2x_unf15x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7번째 시도: AO 1.5배, UNF 1.5배, NONE 10% 감소\n",
        "\n",
        "import json\n",
        "import random\n",
        "random.seed(45)  # 새로운 시드\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "formatted_train = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted.jsonl')\n",
        "\n",
        "none_samples = []\n",
        "ao_samples = []\n",
        "em_unf_samples = []\n",
        "em_cont_samples = []\n",
        "la_samples = []\n",
        "\n",
        "for sample in formatted_train:\n",
        "    text = sample['text']\n",
        "    if 'NONE|NONE' in text:\n",
        "        none_samples.append(sample)\n",
        "    elif 'AO|' in text:\n",
        "        ao_samples.append(sample)\n",
        "    elif 'EM|UNF' in text:\n",
        "        em_unf_samples.append(sample)\n",
        "    elif 'EM|CONT' in text:\n",
        "        em_cont_samples.append(sample)\n",
        "    elif 'LA|' in text:\n",
        "        la_samples.append(sample)\n",
        "\n",
        "print(f\"=== 원본 분포 ===\")\n",
        "print(f\"NONE: {len(none_samples)}개\")\n",
        "print(f\"AO: {len(ao_samples)}개\")\n",
        "print(f\"EM|UNF: {len(em_unf_samples)}개\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개\")\n",
        "print(f\"LA: {len(la_samples)}개\")\n",
        "\n",
        "# NONE 10% 감소 (0.9배)\n",
        "none_target = int(len(none_samples) * 0.9)\n",
        "sampled_none = random.sample(none_samples, none_target)\n",
        "\n",
        "# AO 1.5배 (5번 방식 유지!)\n",
        "ao_15x = ao_samples + [s for s in ao_samples if random.random() < 0.5]\n",
        "\n",
        "# UNF 1.5배\n",
        "unf_15x = em_unf_samples + [s for s in em_unf_samples if random.random() < 0.5]\n",
        "\n",
        "# 합치기\n",
        "balanced_data = sampled_none + ao_15x + unf_15x + em_cont_samples + la_samples\n",
        "random.shuffle(balanced_data)\n",
        "\n",
        "print(f\"\\n=== 조정 후 분포 ===\")\n",
        "print(f\"NONE: {len(sampled_none)}개 (×0.9) ⭐\")\n",
        "print(f\"AO: {len(ao_15x)}개 (×1.5)\")\n",
        "print(f\"EM|UNF: {len(unf_15x)}개 (×1.5)\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)}개 (유지)\")\n",
        "print(f\"LA: {len(la_samples)}개 (유지)\")\n",
        "print(f\"총: {len(balanced_data)}개\")\n",
        "\n",
        "# 비율 출력\n",
        "total = len(balanced_data)\n",
        "print(f\"\\n=== 비율 ===\")\n",
        "print(f\"NONE: {len(sampled_none)/total*100:.1f}%\")\n",
        "print(f\"AO: {len(ao_15x)/total*100:.1f}%\")\n",
        "print(f\"EM|UNF: {len(unf_15x)/total*100:.1f}%\")\n",
        "print(f\"EM|CONT: {len(em_cont_samples)/total*100:.1f}%\")\n",
        "print(f\"LA: {len(la_samples)/total*100:.1f}%\")\n",
        "\n",
        "# 저장\n",
        "output_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_none90_ao15x_unf15x.jsonl'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for item in balanced_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\n✓ 저장 완료: none90_ao15x_unf15x\")"
      ],
      "metadata": {
        "id": "cyRO-dtzNPat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1c80bb-0bb5-4365-adfc-7519bb04257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 원본 분포 ===\n",
            "NONE: 1280개\n",
            "AO: 641개\n",
            "EM|UNF: 160개\n",
            "EM|CONT: 160개\n",
            "LA: 320개\n",
            "\n",
            "=== 조정 후 분포 ===\n",
            "NONE: 1152개 (×0.9) ⭐\n",
            "AO: 967개 (×1.5)\n",
            "EM|UNF: 245개 (×1.5)\n",
            "EM|CONT: 160개 (유지)\n",
            "LA: 320개 (유지)\n",
            "총: 2844개\n",
            "\n",
            "=== 비율 ===\n",
            "NONE: 40.5%\n",
            "AO: 34.0%\n",
            "EM|UNF: 8.6%\n",
            "EM|CONT: 5.6%\n",
            "LA: 11.3%\n",
            "\n",
            "✓ 저장 완료: none90_ao15x_unf15x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "샘플 테스트"
      ],
      "metadata": {
        "id": "GxjIrhMUcE0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 균형 조정된 데이터 샘플 확인\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "def load_jsonl(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line.strip()))\n",
        "    return data\n",
        "\n",
        "# 데이터 로드\n",
        "balanced_data = load_jsonl('/content/drive/MyDrive/Colab Notebooks/woke-odds/ambiguity_train_1110_formatted_balanced_none90_ao15x_unf15x.jsonl')\n",
        "\n",
        "print(f\"=== 데이터 로드 완료 ===\")\n",
        "print(f\"총 샘플: {len(balanced_data)}개\\n\")\n",
        "\n",
        "# 랜덤 샘플 3개 확인\n",
        "sample_indices = random.sample(range(len(balanced_data)), 3)\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    sample = balanced_data[idx]\n",
        "    text = sample['text']\n",
        "\n",
        "    print(f\"=== 샘플 {i+1} (인덱스: {idx}) ===\")\n",
        "\n",
        "    # 질문 추출\n",
        "    if '[INST]' in text and '[/INST]' in text:\n",
        "        start = text.find('<</SYS>>\\n\\n') + 10\n",
        "        end = text.find('[/INST]')\n",
        "        question = text[start:end].strip()\n",
        "    else:\n",
        "        question = \"추출 실패\"\n",
        "\n",
        "    # 답변 추출\n",
        "    if '[/INST]' in text and '</s>' in text:\n",
        "        start = text.find('[/INST]') + 8\n",
        "        end = text.find('</s>')\n",
        "        answer = text[start:end].strip()\n",
        "    else:\n",
        "        answer = \"추출 실패\"\n",
        "\n",
        "    print(f\"질문: {question[:100]}...\")\n",
        "    print(f\"답변: {answer}\")\n",
        "    print(f\"\\n전체 텍스트 (처음 200자):\")\n",
        "    print(text[:200] + \"...\\n\")\n",
        "    print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "# NONE 비율 확인\n",
        "none_count = sum(1 for s in balanced_data if 'NONE|NONE' in s['text'])\n",
        "print(f\"=== 최종 확인 ===\")\n",
        "print(f\"NONE|NONE: {none_count}개 ({none_count/len(balanced_data)*100:.1f}%)\")\n",
        "print(f\"기타: {len(balanced_data) - none_count}개 ({(len(balanced_data)-none_count)/len(balanced_data)*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC8KaxpobQvG",
        "outputId": "57259aa0-1754-43f7-f0f2-791829f9119d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 데이터 로드 완료 ===\n",
            "총 샘플: 2844개\n",
            "\n",
            "=== 샘플 1 (인덱스: 731) ===\n",
            "질문: Who did stephen curry play for in college?...\n",
            "답변: NONE|NONE\n",
            "\n",
            "전체 텍스트 (처음 200자):\n",
            "<s>[INST] <<SYS>>\n",
            "You are an AI system that determines if the question requires clarification and classifies the ambiguity.\n",
            "\n",
            "Task:\n",
            "1. Determine if the question requires clarification: clear(no clarifi...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "=== 샘플 2 (인덱스: 3) ===\n",
            "질문: Give me a list of the greatest basketball players of all time....\n",
            "답변: AO|WHOM\n",
            "\n",
            "전체 텍스트 (처음 200자):\n",
            "<s>[INST] <<SYS>>\n",
            "You are an AI system that determines if the question requires clarification and classifies the ambiguity.\n",
            "\n",
            "Task:\n",
            "1. Determine if the question requires clarification: clear(no clarifi...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "=== 샘플 3 (인덱스: 2759) ===\n",
            "질문: The sister-in-law built Amanda a garden after she earned a billion dollars.\n",
            "Who earned a billion dol...\n",
            "답변: LA|SEM\n",
            "\n",
            "전체 텍스트 (처음 200자):\n",
            "<s>[INST] <<SYS>>\n",
            "You are an AI system that determines if the question requires clarification and classifies the ambiguity.\n",
            "\n",
            "Task:\n",
            "1. Determine if the question requires clarification: clear(no clarifi...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "=== 최종 확인 ===\n",
            "NONE|NONE: 1152개 (40.5%)\n",
            "기타: 1692개 (59.5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oo7rYfmKcFpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}