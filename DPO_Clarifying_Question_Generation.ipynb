{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOCA0XQ0z5pHBGvZ0z9IZng"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4iLzzG-Zfy-"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers peft accelerate datasets trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "import gc\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "id": "Mgph1NmAZ1UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"드라이브 마운트 완료!\")"
      ],
      "metadata": {
        "id": "KMoCbp6gZ3JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO 데이터 로드\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 변환된 DPO 데이터셋 로드\n",
        "dpo_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo_final_dataset_modified.jsonl'\n",
        "\n",
        "print(\"DPO 데이터셋 로드 중...\")\n",
        "dataset = load_dataset(\"json\", data_files=dpo_path)\n",
        "train_dataset = dataset['train']\n",
        "\n",
        "print(f\"DPO 데이터셋 로드 완료: {len(train_dataset)}개\\n\")\n",
        "\n",
        "# 데이터 구조 확인\n",
        "print(\"=\" * 80)\n",
        "print(\"데이터셋 구조 확인\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nColumns: {train_dataset.column_names}\\n\")\n",
        "\n",
        "# 첫 번째 샘플 출력\n",
        "print(\"=\" * 80)\n",
        "print(\"첫 번째 샘플\")\n",
        "print(\"=\" * 80)\n",
        "example = train_dataset[0]\n",
        "\n",
        "print(\"\\n Prompt:\")\n",
        "for msg in example['prompt']:\n",
        "    print(f\"  [{msg['role']}]: {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}\")\n",
        "\n",
        "print(\"\\n Chosen:\")\n",
        "for msg in example['chosen']:\n",
        "    print(f\"  [{msg['role']}]: {msg['content']}\")\n",
        "\n",
        "print(\"\\n Rejected:\")\n",
        "for msg in example['rejected']:\n",
        "    print(f\"  [{msg['role']}]: {msg['content']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"데이터셋 샘플 추가 확인\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 두 번째와 세 번째 샘플도 간단히 확인\n",
        "for i in [1, 2]:\n",
        "    example = train_dataset[i]\n",
        "    user_msg = [m for m in example['prompt'] if m['role'] == 'user'][0]['content']\n",
        "    print(f\"\\n샘플 {i+1}:\")\n",
        "    print(f\"  User: {user_msg[:80]}{'...' if len(user_msg) > 80 else ''}\")\n",
        "    print(f\"  Chosen: {example['chosen'][0]['content'][:80]}{'...' if len(example['chosen'][0]['content']) > 80 else ''}\")\n",
        "    print(f\"  Rejected: {example['rejected'][0]['content'][:80]}{'...' if len(example['rejected'][0]['content']) > 80 else ''}\")\n",
        "\n",
        "print(\"\\n 데이터셋 확인 완료!\")"
      ],
      "metadata": {
        "id": "Rkj0ciK7bvX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "sft_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/checkpoint-best'\n",
        "base_model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "\n",
        "print(\"모델 로드 중...\\n\")\n",
        "\n",
        "# Base 모델 로드\n",
        "print(\"Base 모델 로드...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Base 모델 로드 완료\")\n",
        "\n",
        "# SFT LoRA adapter를 두 번 로드 (같은 모델 인스턴스에!)\n",
        "print(\"\\n SFT LoRA adapter 로드...\")\n",
        "from peft import PeftModel\n",
        "\n",
        "# 첫 번째: trainable adapter (학습할 것)\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    sft_path,\n",
        "    adapter_name=\"dpo_train\",\n",
        "    is_trainable=True\n",
        ")\n",
        "print(\"'dpo_train' adapter 로드 완료\")\n",
        "\n",
        "# 두 번째: reference adapter (frozen, 비교용)\n",
        "model.load_adapter(sft_path, adapter_name=\"dpo_reference\")\n",
        "print(\"'dpo_reference' adapter 로드 완료\")\n",
        "\n",
        "# Tokenizer 로드\n",
        "print(\"\\n Tokenizer 로드...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # DPO에서는 left padding 권장!\n",
        "print(\"Tokenizer 로드 완료\")\n",
        "\n",
        "# 검증\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"모델 구성 검증\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def count_trainable(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nAdapters: {list(model.peft_config.keys())}\")\n",
        "print(f\"Active adapter: {model.active_adapter}\")\n",
        "print(f\"Trainable parameters: {count_trainable(model):,}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "\n",
        "# Adapter별 파라미터 확인\n",
        "model.set_adapter(\"dpo_train\")\n",
        "print(f\"\\n'dpo_train' trainable params: {count_trainable(model):,}\")\n",
        "\n",
        "model.set_adapter(\"dpo_reference\")\n",
        "print(f\"'dpo_reference' trainable params: {count_trainable(model):,}\")\n",
        "\n",
        "model.set_adapter(\"dpo_train\")  # 다시 train으로 전환\n",
        "print(\"\\n모델 로드 완료!\") # DPOTrainer는 'dpo_train'과 'dpo_reference' adapter를 자동으로 전환"
      ],
      "metadata": {
        "id": "u3-L7MRgaVd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 테스트 (DPO 전)\n",
        "print(\"=\" * 80)\n",
        "print(\"SFT 모델 테스트 (DPO 전)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# System message 정의 (테스트용)\n",
        "SYSTEM_MESSAGE = \"\"\"You are an AI that generates a single, concise clarifying question when a user's query is ambiguous.\n",
        "\n",
        "Task:\n",
        "Generate exactly one clarifying question based on the ambiguity type.\n",
        "If the query is clear and needs no clarification, output: <NO_CLARIFYING_QUESTION>\n",
        "\n",
        "Output format: One clarifying question (or <NO_CLARIFYING_QUESTION> if not needed)\n",
        "\n",
        "Categories:\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "- NONE: Clear questions that don't require clarification\"\"\"\n",
        "\n",
        "def test_model(query):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return generated.strip()\n",
        "\n",
        "# 테스트 쿼리\n",
        "test_queries = [\n",
        "    \"[AO|WHERE] When did call of duty ww2 come out?\",\n",
        "    \"[NONE|NONE] Who won the world cup as a player and a manager?\",\n",
        "    \"[LA|LEX] What is the most common type of type used in printing?\",\n",
        "]\n",
        "\n",
        "for idx, query in enumerate(test_queries, 1):\n",
        "    print(f\"테스트 {idx}/{len(test_queries)}\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    response = test_model(query)\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(f\"{'─'*80}\\n\")\n",
        "\n",
        "print(\"테스트 완료!\")"
      ],
      "metadata": {
        "id": "oYM5Rimvbpxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아직 모호한 질문을 잘 생성하지 못하는 모습!ㅠㅠ"
      ],
      "metadata": {
        "id": "3MQNLs1Wdm8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 셀 6: DPO Config 설정 (최적화됨!)\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model'\n",
        "\n",
        "dpo_config = DPOConfig(\n",
        "    # 기본 설정\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,  # 3 epoch\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # effective batch size = 16\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate=5e-7,  # DPO 권장: 1e-7 ~ 1e-6\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,  # 전체의 10% warmup\n",
        "\n",
        "    # DPO 하이퍼파라미터\n",
        "    beta=0.1,  # DPO 논문 기본값 (0.1 ~ 0.5 추천)\n",
        "    loss_type=\"sigmoid\",  # 기본 DPO loss\n",
        "\n",
        "    # 어댑터 이름 지정\n",
        "    model_adapter_name=\"dpo_train\",\n",
        "    ref_adapter_name=\"dpo_reference\",\n",
        "\n",
        "    # 메모리 최적화\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch\",\n",
        "\n",
        "    # 로깅 & 저장\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"epoch\",  # epoch마다 저장\n",
        "    save_total_limit=2,  # 최근 2개만 유지\n",
        "    load_best_model_at_end=False,\n",
        "\n",
        "    # 평가 (없음 - 학습 데이터만 사용)\n",
        "    eval_strategy=\"no\",\n",
        "\n",
        "    # 기타\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # wandb 없이\n",
        "    seed=42,\n",
        "\n",
        "    # 데이터 길이\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        ")\n",
        "\n",
        "print(\"DPO Config 설정 완료\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"주요 설정\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  학습 데이터: {len(train_dataset)}개\")\n",
        "print(f\"  Epochs: {dpo_config.num_train_epochs}\")\n",
        "print(f\"  Batch size: {dpo_config.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {dpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {dpo_config.per_device_train_batch_size * dpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {dpo_config.learning_rate}\")\n",
        "print(f\"  Beta: {dpo_config.beta}\")\n",
        "print(f\"  Loss type: {dpo_config.loss_type}\")\n",
        "print(f\"  Model adapter: {dpo_config.model_adapter_name}\")\n",
        "print(f\"  Ref adapter: {dpo_config.ref_adapter_name}\")\n",
        "print(f\"  Save strategy: {dpo_config.save_strategy}\")\n",
        "\n",
        "# 예상 step 계산\n",
        "total_steps = (len(train_dataset) * dpo_config.num_train_epochs) // (\n",
        "    dpo_config.per_device_train_batch_size * dpo_config.gradient_accumulation_steps\n",
        ")\n",
        "print(f\"\\n  Total steps: ~{total_steps}\")\n",
        "print(f\"  Warmup steps: ~{int(total_steps * dpo_config.warmup_ratio)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Ox5ASjSadaZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 셀 7: DPO Trainer 초기화\n",
        "\n",
        "print(\"DPO Trainer 초기화 중...\\n\")\n",
        "\n",
        "# ref_model 파라미터를 전달하지 않음!\n",
        "# DPOTrainer가 model_adapter_name과 ref_adapter_name을 사용해서\n",
        "# 자동으로 adapter를 전환\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model=model,              # adapter가 로드된 모델\n",
        "    ref_model=None,           # None으로 설정\n",
        "    args=dpo_config,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"DPO Trainer 초기화 완료!\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"학습 준비 완료\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  학습 데이터: {len(train_dataset):,}개\")\n",
        "print(f\"  Effective batch size: {dpo_config.per_device_train_batch_size * dpo_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Total epochs: {dpo_config.num_train_epochs}\")\n",
        "\n",
        "# Step 계산\n",
        "steps_per_epoch = len(train_dataset) // (\n",
        "    dpo_config.per_device_train_batch_size * dpo_config.gradient_accumulation_steps\n",
        ")\n",
        "total_steps = steps_per_epoch * int(dpo_config.num_train_epochs)\n",
        "\n",
        "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Save every: {steps_per_epoch} steps (per epoch)\")\n",
        "print(f\"  Log every: {dpo_config.logging_steps} steps\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n준비 완료!\")"
      ],
      "metadata": {
        "id": "5rvdzOrYgmZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DPO 학습 실행\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DPO 학습 시작!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "# 학습 시작\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    train_result = dpo_trainer.train()\n",
        "\n",
        "    # 학습 완료\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"학습 완료!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"종료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"총 소요 시간: {elapsed_time/60:.2f}분 ({elapsed_time:.1f}초)\")\n",
        "\n",
        "    # 학습 통계\n",
        "    print(\"\\n학습 통계:\")\n",
        "    print(f\"  - Total steps: {train_result.global_step}\")\n",
        "    print(f\"  - Training loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "    if hasattr(train_result, 'metrics'):\n",
        "        print(f\"  - Metrics: {train_result.metrics}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    print(\"\\n최종 모델 저장 중...\")\n",
        "    final_output_dir = f\"{output_dir}/final\"\n",
        "    dpo_trainer.save_model(final_output_dir)\n",
        "    tokenizer.save_pretrained(final_output_dir)\n",
        "    print(f\"모델 저장 완료: {final_output_dir}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n학습이 중단되었습니다.\")\n",
        "    print(\"현재까지의 체크포인트가 저장되었습니다.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "NGR1f5Big1WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss 시각화 및 분석\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"학습 로그 분석 중...\\n\")\n",
        "\n",
        "# 수정된 경로\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model'\n",
        "\n",
        "# 가장 최근 checkpoint 찾기\n",
        "checkpoints = sorted(Path(output_dir).glob(\"checkpoint-*\"),\n",
        "                     key=lambda x: int(x.name.split('-')[1]))\n",
        "\n",
        "if checkpoints:\n",
        "    latest_checkpoint = checkpoints[-1]\n",
        "    trainer_state_path = latest_checkpoint / \"trainer_state.json\"\n",
        "\n",
        "    print(f\"최신 체크포인트: {latest_checkpoint.name}\")\n",
        "\n",
        "    if trainer_state_path.exists():\n",
        "        with open(trainer_state_path, 'r') as f:\n",
        "            trainer_state = json.load(f)\n",
        "\n",
        "        # Loss history 추출\n",
        "        log_history = trainer_state.get('log_history', [])\n",
        "\n",
        "        if log_history:\n",
        "            # Step과 loss 추출\n",
        "            steps = []\n",
        "            losses = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in log_history:\n",
        "                if 'loss' in entry:\n",
        "                    steps.append(entry['step'])\n",
        "                    losses.append(entry['loss'])\n",
        "                    if 'learning_rate' in entry:\n",
        "                        learning_rates.append(entry['learning_rate'])\n",
        "\n",
        "            # 시각화\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "            # Loss plot\n",
        "            axes[0].plot(steps, losses, 'b-', linewidth=2.5, marker='o', markersize=5, alpha=0.8)\n",
        "            axes[0].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_title('DPO Training Loss', fontsize=15, fontweight='bold', pad=15)\n",
        "            axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "            axes[0].set_xlim(left=0)\n",
        "\n",
        "            # Epoch 구분선 추가\n",
        "            steps_per_epoch = 106  # checkpoint-106, 212, 318\n",
        "            for i in range(1, 4):\n",
        "                axes[0].axvline(x=i*steps_per_epoch, color='r', linestyle='--',\n",
        "                               alpha=0.5, linewidth=1.5, label=f'Epoch {i}' if i == 1 else '')\n",
        "            axes[0].legend(fontsize=10)\n",
        "\n",
        "            # Learning rate plot\n",
        "            if learning_rates:\n",
        "                axes[1].plot(steps[:len(learning_rates)], learning_rates, 'r-',\n",
        "                           linewidth=2.5, alpha=0.8)\n",
        "                axes[1].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_title('Learning Rate Schedule (Cosine)', fontsize=15,\n",
        "                                fontweight='bold', pad=15)\n",
        "                axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "                axes[1].set_xlim(left=0)\n",
        "                axes[1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # 저장\n",
        "            plot_path = Path(output_dir) / \"training_curves.png\"\n",
        "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"Loss curve 저장: {plot_path}\\n\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # 통계 출력\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"학습 통계\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"  Total steps: {len(steps)}\")\n",
        "            print(f\"  Initial loss: {losses[0]:.6f}\")\n",
        "            print(f\"  Final loss: {losses[-1]:.6f}\")\n",
        "\n",
        "            loss_reduction = losses[0] - losses[-1]\n",
        "            loss_reduction_pct = (loss_reduction / losses[0]) * 100\n",
        "\n",
        "            print(f\"  Loss reduction: {loss_reduction:.6f} ({loss_reduction_pct:.2f}%)\")\n",
        "            print(f\"  Min loss: {min(losses):.6f} (step {steps[losses.index(min(losses))]})\")\n",
        "            print(f\"  Max loss: {max(losses):.6f} (step {steps[losses.index(max(losses))]})\")\n",
        "\n",
        "            # Epoch별 평균 loss\n",
        "            print(f\"\\n  Epoch별 평균 Loss:\")\n",
        "            for epoch in range(1, 4):\n",
        "                start_idx = (epoch - 1) * 106 // 10  # logging_steps=10 고려\n",
        "                end_idx = epoch * 106 // 10\n",
        "                epoch_losses = losses[start_idx:end_idx]\n",
        "                if epoch_losses:\n",
        "                    print(f\"    Epoch {epoch}: {sum(epoch_losses)/len(epoch_losses):.6f}\")\n",
        "\n",
        "            if learning_rates:\n",
        "                print(f\"\\n  Initial LR: {learning_rates[0]:.2e}\")\n",
        "                print(f\"  Final LR: {learning_rates[-1]:.2e}\")\n",
        "\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            # 평가\n",
        "            print(\"\\n 평가:\")\n",
        "            if loss_reduction_pct > 5:\n",
        "                print(\"  ✅ 학습이 잘 진행되었습니다!\")\n",
        "            elif loss_reduction_pct > 2:\n",
        "                print(\"  Loss 감소가 적습니다. 더 긴 학습이 필요할 수 있습니다.\")\n",
        "            else:\n",
        "                print(\"  Loss 감소가 매우 적습니다. 하이퍼파라미터 조정이 필요할 수 있습니다.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Log history가 비어있습니다.\")\n",
        "    else:\n",
        "        print(f\"Trainer state 파일을 찾을 수 없습니다: {trainer_state_path}\")\n",
        "else:\n",
        "    print(f\"Checkpoint를 찾을 수 없습니다: {output_dir}\")"
      ],
      "metadata": {
        "id": "JzlcSpTJhPqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 셀 10: SFT vs DPO 체크포인트 비교\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SFT vs DPO 모델 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 경로 설정\n",
        "sft_path = '/content/drive/MyDrive/Colab Notebooks/woke-odds/checkpoint-best'\n",
        "base_model_name = \"microsoft/Phi-4-mini-reasoning\"\n",
        "dpo_checkpoints = [\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-106/dpo_train', 'Epoch 1'),\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-212/dpo_train', 'Epoch 2'),\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-318/dpo_train', 'Epoch 3'),\n",
        "]\n",
        "\n",
        "# System message\n",
        "SYSTEM_MESSAGE = \"\"\"You are an AI that generates a single, concise clarifying question when a user's query is ambiguous.\n",
        "\n",
        "Task:\n",
        "Generate exactly one clarifying question based on the ambiguity type.\n",
        "If the query is clear and needs no clarification, output: <NO_CLARIFYING_QUESTION>\n",
        "\n",
        "Output format: One clarifying question (or <NO_CLARIFYING_QUESTION> if not needed)\n",
        "\n",
        "Categories:\n",
        "- EM (Epistemic Misalignment): Questions with unfamiliar entities or self-contradictions\n",
        "- LA (Linguistic Ambiguity): Questions with lexical or semantic ambiguity\n",
        "- AO (Aleatoric Output): Questions with missing contextual information causing confusion\n",
        "- NONE: Clear questions that don't require clarification\"\"\"\n",
        "\n",
        "# 테스트 쿼리 풀\n",
        "all_queries = [\n",
        "    (\"[AO|WHERE] When did call of duty ww2 come out?\",\n",
        "     \"Which one: release, release in the North America, release in Australia, or release in the EU?\"),\n",
        "    (\"[NONE|NONE] Who won the world cup as a player and a manager?\",\n",
        "     \"<NO_CLARIFYING_QUESTION>\"),\n",
        "    (\"[LA|LEX] What is the most common type of type used in printing?\",\n",
        "     \"Are you referring to typeface or printing method?\"),\n",
        "    (\"[EM|UNF] Does Helicotylenchus have Phaseolus galactoides as its host?\",\n",
        "     \"What is Helicotylenchus referring to?\"),\n",
        "    (\"[AO|WHEN] Give me a bulleted list of the top five most followed Instagram profiles.\",\n",
        "     \"As of which date or period would you like the information?\"),\n",
        "    (\"[LA|SEM] Melissa told Jennifer that she beguiled the father-in-law. Who beguiled the father-in-law?\",\n",
        "     \"What does she refer to? Melissa or Jennifer?\"),\n",
        "]\n",
        "\n",
        "# 랜덤하게 3개 선택\n",
        "random.seed(42)\n",
        "test_queries = random.sample(all_queries, 3)\n",
        "\n",
        "print(f\"\\n선택된 테스트 쿼리: {len(test_queries)}개\\n\")\n",
        "\n",
        "# Tokenizer 로드\n",
        "print(\"Tokenizer 로드 중...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(sft_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer 로드 완료\\n\")\n",
        "\n",
        "def generate_response(model, query):\n",
        "    \"\"\"응답 생성 함수\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return generated.strip()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"모델 로딩 및 비교 시작\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 각 쿼리에 대해 비교\n",
        "for idx, (query, expected) in enumerate(test_queries, 1):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"테스트 {idx}/{len(test_queries)}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Expected: {expected}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # SFT 응답\n",
        "    print(\"\\n[SFT - baseline]\")\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    sft_model = PeftModel.from_pretrained(base_model, sft_path, is_trainable=False)\n",
        "    sft_model.eval()\n",
        "\n",
        "    sft_response = generate_response(sft_model, query)\n",
        "    print(f\"  {sft_response}\")\n",
        "\n",
        "    # 메모리 정리\n",
        "    del sft_model, base_model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # DPO 체크포인트별 응답\n",
        "    for checkpoint_path, epoch_name in dpo_checkpoints:\n",
        "        print(f\"\\n[DPO - {epoch_name}]\")\n",
        "\n",
        "        # Base 모델 로드\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # SFT adapter 로드\n",
        "        model = PeftModel.from_pretrained(base_model, sft_path, adapter_name=\"sft\")\n",
        "\n",
        "        # DPO adapter 로드\n",
        "        model.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        model.set_adapter(\"dpo_train\")\n",
        "        model.eval()\n",
        "\n",
        "        dpo_response = generate_response(model, query)\n",
        "        print(f\"  {dpo_response}\")\n",
        "\n",
        "        # 메모리 정리 (모델만 삭제, 다음 DPO 체크포인트를 위해)\n",
        "        del model, base_model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n비교 완료!\")\n",
        "print(\"\\n분석:\")\n",
        "print(\"  - SFT: 기본 모델 (DPO 전)\")\n",
        "print(\"  - DPO Epoch 1, 2, 3: DPO 학습 진행에 따른 개선 확인\")\n",
        "print(\"  - Expected와 비교하여 DPO가 더 나은 응답을 생성하는지 확인하세요\")"
      ],
      "metadata": {
        "id": "33NpMK3foKQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DPO 학습 결과, 모델 성능이 SFT 대비 개선 (테스트 1은 둘다 ok,2는 둘다 별로,3은 에포크 3에서 개선-시간언급)\n",
        "- Epoch 3에서 최고 성능 달성, 특히 대명사 모호성(LA|SEM)과 시간 모호성(AO|WHEN) 처리 능력 향상\n",
        "- Loss 10% 감소로 더 구체적이고 정확한 명확화 질문 생성 가능"
      ],
      "metadata": {
        "id": "yIncCLf-uAnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험 2"
      ],
      "metadata": {
        "id": "HHz6vDDd8u42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 2: Epochs 증가 (3 → 5)\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 새 출력 디렉토리\n",
        "output_dir_exp2 = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model-exp2-5epochs'\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 2: 5 Epochs 학습\")\n",
        "print(\"=\" * 80)\n",
        "print(\"변경사항: num_train_epochs 3 → 5\")\n",
        "print(\"기대효과: 추가 수렴으로 성능 향상\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# DPO Config (Epochs만 변경)\n",
        "dpo_config_exp2 = DPOConfig(\n",
        "    output_dir=output_dir_exp2,\n",
        "    num_train_epochs=5,  # ← 변경!\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "\n",
        "    learning_rate=5e-7,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    beta=0.1,\n",
        "    loss_type=\"sigmoid\",\n",
        "\n",
        "    model_adapter_name=\"dpo_train\",\n",
        "    ref_adapter_name=\"dpo_reference\",\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch\",\n",
        "\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit= 3,  # 3개 유지 (더 많은 epoch)\n",
        "\n",
        "    eval_strategy=\"no\",\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        ")\n",
        "\n",
        "print(\"DPO Config 설정 완료\\n\")\n",
        "print(\"주요 설정:\")\n",
        "print(f\"  - Epochs: {dpo_config_exp2.num_train_epochs}\")\n",
        "print(f\"  - Total steps: ~{106 * 5}\")\n",
        "print(f\"  - 출력 디렉토리: {output_dir_exp2}\")"
      ],
      "metadata": {
        "id": "gaYBct4jpPOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 2: 모델 재로드 및 Trainer 초기화\n",
        "# ========================================\n",
        "print(\"\\n모델 재로드 중...\")\n",
        "\n",
        "# 메모리 정리\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'dpo_trainer' in locals():\n",
        "    del dpo_trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Base 모델 로드\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-4-mini-reasoning\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# SFT adapter 로드 (2개)\n",
        "model = PeftModel.from_pretrained(base_model, sft_path, adapter_name=\"dpo_train\", is_trainable=True)\n",
        "model.load_adapter(sft_path, adapter_name=\"dpo_reference\")\n",
        "\n",
        "print(\"모델 로드 완료\")\n",
        "\n",
        "# Trainer 초기화\n",
        "dpo_trainer_exp2 = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=dpo_config_exp2,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"DPO Trainer 초기화 완료\\n\")\n",
        "print(\"준비 완료! 학습을 시작하세요\")"
      ],
      "metadata": {
        "id": "IKd1p46MxCAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 2: 학습 실행\n",
        "# ========================================\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 2: 5 Epochs 학습 시작\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    train_result_exp2 = dpo_trainer_exp2.train()\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"학습 완료!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"종료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"총 소요 시간: {elapsed_time/60:.2f}분\")\n",
        "\n",
        "    print(\"\\n학습 통계:\")\n",
        "    print(f\"  - Total steps: {train_result_exp2.global_step}\")\n",
        "    print(f\"  - Training loss: {train_result_exp2.training_loss:.6f}\")\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    print(\"\\n최종 모델 저장 중...\")\n",
        "    final_output_dir_exp2 = f\"{output_dir_exp2}/final\"\n",
        "    dpo_trainer_exp2.save_model(final_output_dir_exp2)\n",
        "    tokenizer.save_pretrained(final_output_dir_exp2)\n",
        "    print(f\"모델 저장 완료: {final_output_dir_exp2}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "DiloOBqLxDmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 2: Loss 시각화\n",
        "# ========================================\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"실험 2: 학습 로그 분석\\n\")\n",
        "\n",
        "checkpoints_exp2 = sorted(Path(output_dir_exp2).glob(\"checkpoint-*\"),\n",
        "                          key=lambda x: int(x.name.split('-')[1]))\n",
        "\n",
        "if checkpoints_exp2:\n",
        "    latest_checkpoint_exp2 = checkpoints_exp2[-1]\n",
        "    trainer_state_path_exp2 = latest_checkpoint_exp2 / \"trainer_state.json\"\n",
        "\n",
        "    print(f\"최신 체크포인트: {latest_checkpoint_exp2.name}\")\n",
        "\n",
        "    if trainer_state_path_exp2.exists():\n",
        "        with open(trainer_state_path_exp2, 'r') as f:\n",
        "            trainer_state_exp2 = json.load(f)\n",
        "\n",
        "        log_history_exp2 = trainer_state_exp2.get('log_history', [])\n",
        "\n",
        "        if log_history_exp2:\n",
        "            steps = []\n",
        "            losses = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in log_history_exp2:\n",
        "                if 'loss' in entry:\n",
        "                    steps.append(entry['step'])\n",
        "                    losses.append(entry['loss'])\n",
        "                    if 'learning_rate' in entry:\n",
        "                        learning_rates.append(entry['learning_rate'])\n",
        "\n",
        "            # 시각화\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "            # Loss plot\n",
        "            axes[0].plot(steps, losses, 'b-', linewidth=2.5, marker='o', markersize=5, alpha=0.8)\n",
        "            axes[0].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_title('DPO Training Loss (5 Epochs)', fontsize=15, fontweight='bold', pad=15)\n",
        "            axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "            axes[0].set_xlim(left=0)\n",
        "\n",
        "            # Epoch 구분선\n",
        "            for i in range(1, 6):\n",
        "                axes[0].axvline(x=i*106, color='r', linestyle='--', alpha=0.5, linewidth=1.5,\n",
        "                               label=f'Epoch {i}' if i == 1 else '')\n",
        "            axes[0].legend(fontsize=10)\n",
        "\n",
        "            # Learning rate plot\n",
        "            if learning_rates:\n",
        "                axes[1].plot(steps[:len(learning_rates)], learning_rates, 'r-', linewidth=2.5, alpha=0.8)\n",
        "                axes[1].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_title('Learning Rate Schedule', fontsize=15, fontweight='bold', pad=15)\n",
        "                axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "                axes[1].set_xlim(left=0)\n",
        "                axes[1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plot_path_exp2 = Path(output_dir_exp2) / \"training_curves.png\"\n",
        "            plt.savefig(plot_path_exp2, dpi=150, bbox_inches='tight')\n",
        "            print(f\"\\nLoss curve 저장: {plot_path_exp2}\\n\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # 통계\n",
        "            print(\"=\" * 80)\n",
        "            print(\"학습 통계\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"  Total steps: {len(steps)}\")\n",
        "            print(f\"  Initial loss: {losses[0]:.6f}\")\n",
        "            print(f\"  Final loss: {losses[-1]:.6f}\")\n",
        "            print(f\"  Loss reduction: {losses[0] - losses[-1]:.6f} ({(losses[0] - losses[-1])/losses[0]*100:.2f}%)\")\n",
        "            print(f\"  Min loss: {min(losses):.6f} (step {steps[losses.index(min(losses))]})\")\n",
        "\n",
        "            # Epoch별 평균 loss\n",
        "            print(f\"\\n  Epoch별 평균 Loss:\")\n",
        "            for epoch in range(1, 6):\n",
        "                start_idx = max(0, (epoch - 1) * 106 // 10)\n",
        "                end_idx = min(len(losses), epoch * 106 // 10)\n",
        "                epoch_losses = losses[start_idx:end_idx]\n",
        "                if epoch_losses:\n",
        "                    print(f\"    Epoch {epoch}: {sum(epoch_losses)/len(epoch_losses):.6f}\")\n",
        "\n",
        "            print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Ih7-k3s_xELx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 1 vs 실험 2 비교 테스트\n",
        "# ========================================\n",
        "import random\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 1 (3 Epochs) vs 실험 2 (5 Epochs) 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "exp1_checkpoints = [\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-318/dpo_train', 'Exp1-E3'),\n",
        "]\n",
        "\n",
        "exp2_checkpoints = [\n",
        "    (f'{output_dir_exp2}/checkpoint-530/dpo_train', 'Exp2-E5'),  # 5 epoch 마지막\n",
        "]\n",
        "\n",
        "# 테스트 쿼리\n",
        "test_queries_compare = [\n",
        "    (\"[AO|WHERE] When did call of duty ww2 come out?\",\n",
        "     \"Which one: release, release in the North America, release in Australia, or release in the EU?\"),\n",
        "    (\"[NONE|NONE] Who won the world cup as a player and a manager?\",\n",
        "     \"<NO_CLARIFYING_QUESTION>\"),\n",
        "    (\"[LA|SEM] Melissa told Jennifer that she beguiled the father-in-law. Who beguiled the father-in-law?\",\n",
        "     \"What does she refer to? Melissa or Jennifer?\"),\n",
        "]\n",
        "\n",
        "for idx, (query, expected) in enumerate(test_queries_compare, 1):\n",
        "    print(f\"\\n테스트 {idx}/{len(test_queries_compare)}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Expected: {expected}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 실험 1 결과\n",
        "    for checkpoint_path, name in exp1_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 실험 2 결과\n",
        "    for checkpoint_path, name in exp2_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n비교 완료!\")\n",
        "print(\"\\n분석: 5 epochs 학습이 3 epochs보다 더 나은지 확인하세요\")"
      ],
      "metadata": {
        "id": "PxxbnH1qxI47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 1 vs 실험 2 비교 테스트 (확장 버전 - 유사도 평가 제거)\n",
        "# ========================================\n",
        "import random\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 1 (3 Epochs) vs 실험 2 (5 Epochs) 비교 - 확장 테스트\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "exp1_checkpoints = [\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-318/dpo_train', 'Exp1-E3'),\n",
        "]\n",
        "\n",
        "exp2_checkpoints = [\n",
        "    (f'{output_dir_exp2}/checkpoint-530/dpo_train', 'Exp2-E5'),\n",
        "]\n",
        "\n",
        "# 확장된 테스트 쿼리\n",
        "test_queries_extended = [\n",
        "    (\"[AO|WHERE] When did call of duty ww2 come out?\",\n",
        "     \"Which one: release, release in the North America, release in Australia, or release in the EU?\"),\n",
        "    (\"[NONE|NONE] Who won the world cup as a player and a manager?\",\n",
        "     \"<NO_CLARIFYING_QUESTION>\"),\n",
        "    (\"[LA|SEM] Melissa told Jennifer that she beguiled the father-in-law. Who beguiled the father-in-law?\",\n",
        "     \"What does she refer to? Melissa or Jennifer?\"),\n",
        "    (\"[EM|CONT] The all possible word categories are either \\\"indoor location\\\" or \\\"human\\\".\\nThe following two examples share a specific word category. You need to first infer the specific word category from the examples.\\nPlease output \\\"X\\\" if the given sentence mentions the specific word category. Please output \\\"Y\\\" if the given sentence does not mention the word category.\\n\\nExamples: \\nThe duck is in the cave.\\nThe fugitive is in the theatre.\\n\\nThe Given Sentence: \\nThe boar is in the hotel lobby.\",\n",
        "     \"Is the category either indoor location or human?\"),\n",
        "    (\"[LA|LEX] Who is the director of Raavanan?\",\n",
        "     \"Are you referring to the Tamil film Raavanan or the Malayalam film Raavanan?\"),\n",
        "    (\"[AO|WHOM] Who has the most career homeruns in mlb?\",\n",
        "     \"Which one: the player with the most career home runs of all time, or the active player with the most career home runs?\"),\n",
        "]\n",
        "\n",
        "print(f\"\\n총 테스트 쿼리: {len(test_queries_extended)}개\")\n",
        "print(\"  - 기존: 3개 (AO|WHERE, NONE, LA|SEM)\")\n",
        "print(\"  - 추가: 3개 (EM|CONT, LA|LEX, AO|WHOM)\\n\")\n",
        "\n",
        "for idx, (query, expected) in enumerate(test_queries_extended, 1):\n",
        "    print(f\"\\n테스트 {idx}/{len(test_queries_extended)}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 쿼리가 너무 길면 앞부분만 표시\n",
        "    query_display = query[:100] + \"...\" if len(query) > 100 else query\n",
        "    print(f\"Query: {query_display}\")\n",
        "    print(f\"Expected: {expected[:80]}{'...' if len(expected) > 80 else ''}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 실험 1 결과\n",
        "    for checkpoint_path, name in exp1_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 실험 2 결과\n",
        "    for checkpoint_path, name in exp2_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n비교 완료!\")\n",
        "print(\"\\n수동으로 각 응답을 Expected와 비교하여 평가하세요\")"
      ],
      "metadata": {
        "id": "Tmoc6xicxN2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실험 3"
      ],
      "metadata": {
        "id": "26oG91of9BTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 3: Beta 값 증가 (0.1 → 0.3)\n",
        "# ========================================\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 새 출력 디렉토리\n",
        "output_dir_exp3 = '/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model-exp3-beta0.3'\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 3: Beta 값 증가\")\n",
        "print(\"=\" * 80)\n",
        "print(\"변경사항: beta 0.1 → 0.3 (3배)\")\n",
        "print(\"기대효과:\")\n",
        "print(\"  - Reference 모델과 적절한 거리 유지\")\n",
        "print(\"  - 더 안정적이고 보수적인 학습\")\n",
        "print(\"  - 환각 및 overfitting 감소\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# DPO Config (Beta만 변경)\n",
        "dpo_config_exp3 = DPOConfig(\n",
        "    output_dir=output_dir_exp3,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "\n",
        "    learning_rate=5e-7,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    beta=0.3,  # ← 변경!\n",
        "    loss_type=\"sigmoid\",\n",
        "\n",
        "    model_adapter_name=\"dpo_train\",\n",
        "    ref_adapter_name=\"dpo_reference\",\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    bf16=True,\n",
        "    optim=\"adamw_torch\",\n",
        "\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "\n",
        "    eval_strategy=\"no\",\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        ")\n",
        "\n",
        "print(\"DPO Config 설정 완료\\n\")\n",
        "print(\"주요 설정:\")\n",
        "print(f\"  - Epochs: {dpo_config_exp3.num_train_epochs}\")\n",
        "print(f\"  - Beta: {dpo_config_exp3.beta} (이전: 0.1)\")\n",
        "print(f\"  - Total steps: ~{106 * 3}\")\n",
        "print(f\"  - 출력 디렉토리: {output_dir_exp3}\")"
      ],
      "metadata": {
        "id": "jUhCGT3q50b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 3: 모델 재로드 및 Trainer 초기화\n",
        "# ========================================\n",
        "print(\"\\n모델 재로드 중...\")\n",
        "\n",
        "# 메모리 정리\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'dpo_trainer' in locals():\n",
        "    del dpo_trainer\n",
        "if 'dpo_trainer_exp2' in locals():\n",
        "    del dpo_trainer_exp2\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Base 모델 로드\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-4-mini-reasoning\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# SFT adapter 로드 (2개)\n",
        "model = PeftModel.from_pretrained(base_model, sft_path, adapter_name=\"dpo_train\", is_trainable=True)\n",
        "model.load_adapter(sft_path, adapter_name=\"dpo_reference\")\n",
        "\n",
        "print(\"모델 로드 완료\")\n",
        "\n",
        "# Trainer 초기화\n",
        "dpo_trainer_exp3 = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    args=dpo_config_exp3,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "print(\"DPO Trainer 초기화 완료\\n\")\n",
        "print(\"준비 완료! 학습을 시작하세요\")"
      ],
      "metadata": {
        "id": "XQEry5ye9wPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 3: 학습 실행\n",
        "# ========================================\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 3: Beta=0.3 학습 시작\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"시작 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    train_result_exp3 = dpo_trainer_exp3.train()\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"학습 완료!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"종료 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"총 소요 시간: {elapsed_time/60:.2f}분\")\n",
        "\n",
        "    print(\"\\n학습 통계:\")\n",
        "    print(f\"  - Total steps: {train_result_exp3.global_step}\")\n",
        "    print(f\"  - Training loss: {train_result_exp3.training_loss:.6f}\")\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    print(\"\\n최종 모델 저장 중...\")\n",
        "    final_output_dir_exp3 = f\"{output_dir_exp3}/final\"\n",
        "    dpo_trainer_exp3.save_model(final_output_dir_exp3)\n",
        "    tokenizer.save_pretrained(final_output_dir_exp3)\n",
        "    print(f\"모델 저장 완료: {final_output_dir_exp3}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n오류 발생: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "D4HbW2-A9xsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 3: Loss 시각화\n",
        "# ========================================\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"실험 3: 학습 로그 분석\\n\")\n",
        "\n",
        "checkpoints_exp3 = sorted(Path(output_dir_exp3).glob(\"checkpoint-*\"),\n",
        "                          key=lambda x: int(x.name.split('-')[1]))\n",
        "\n",
        "if checkpoints_exp3:\n",
        "    latest_checkpoint_exp3 = checkpoints_exp3[-1]\n",
        "    trainer_state_path_exp3 = latest_checkpoint_exp3 / \"trainer_state.json\"\n",
        "\n",
        "    print(f\"최신 체크포인트: {latest_checkpoint_exp3.name}\")\n",
        "\n",
        "    if trainer_state_path_exp3.exists():\n",
        "        with open(trainer_state_path_exp3, 'r') as f:\n",
        "            trainer_state_exp3 = json.load(f)\n",
        "\n",
        "        log_history_exp3 = trainer_state_exp3.get('log_history', [])\n",
        "\n",
        "        if log_history_exp3:\n",
        "            steps = []\n",
        "            losses = []\n",
        "            learning_rates = []\n",
        "\n",
        "            for entry in log_history_exp3:\n",
        "                if 'loss' in entry:\n",
        "                    steps.append(entry['step'])\n",
        "                    losses.append(entry['loss'])\n",
        "                    if 'learning_rate' in entry:\n",
        "                        learning_rates.append(entry['learning_rate'])\n",
        "\n",
        "            # 시각화\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "            # Loss plot\n",
        "            axes[0].plot(steps, losses, 'g-', linewidth=2.5, marker='o', markersize=5, alpha=0.8, label='Beta=0.3')\n",
        "            axes[0].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\n",
        "            axes[0].set_title('DPO Training Loss (Beta=0.3)', fontsize=15, fontweight='bold', pad=15)\n",
        "            axes[0].grid(True, alpha=0.3, linestyle='--')\n",
        "            axes[0].set_xlim(left=0)\n",
        "            axes[0].legend(fontsize=11)\n",
        "\n",
        "            # Epoch 구분선\n",
        "            for i in range(1, 4):\n",
        "                axes[0].axvline(x=i*106, color='r', linestyle='--', alpha=0.5, linewidth=1.5,\n",
        "                               label=f'Epoch {i}' if i == 1 else '')\n",
        "\n",
        "            # Learning rate plot\n",
        "            if learning_rates:\n",
        "                axes[1].plot(steps[:len(learning_rates)], learning_rates, 'g-', linewidth=2.5, alpha=0.8)\n",
        "                axes[1].set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_ylabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "                axes[1].set_title('Learning Rate Schedule', fontsize=15, fontweight='bold', pad=15)\n",
        "                axes[1].grid(True, alpha=0.3, linestyle='--')\n",
        "                axes[1].set_xlim(left=0)\n",
        "                axes[1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            plot_path_exp3 = Path(output_dir_exp3) / \"training_curves.png\"\n",
        "            plt.savefig(plot_path_exp3, dpi=150, bbox_inches='tight')\n",
        "            print(f\"\\nLoss curve 저장: {plot_path_exp3}\\n\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # 통계\n",
        "            print(\"=\" * 80)\n",
        "            print(\"학습 통계\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"  Total steps: {len(steps)}\")\n",
        "            print(f\"  Initial loss: {losses[0]:.6f}\")\n",
        "            print(f\"  Final loss: {losses[-1]:.6f}\")\n",
        "            print(f\"  Loss reduction: {losses[0] - losses[-1]:.6f} ({(losses[0] - losses[-1])/losses[0]*100:.2f}%)\")\n",
        "            print(f\"  Min loss: {min(losses):.6f} (step {steps[losses.index(min(losses))]})\")\n",
        "\n",
        "            # Epoch별 평균 loss\n",
        "            print(f\"\\n  Epoch별 평균 Loss:\")\n",
        "            for epoch in range(1, 4):\n",
        "                start_idx = max(0, (epoch - 1) * 106 // 10)\n",
        "                end_idx = min(len(losses), epoch * 106 // 10)\n",
        "                epoch_losses = losses[start_idx:end_idx]\n",
        "                if epoch_losses:\n",
        "                    print(f\"    Epoch {epoch}: {sum(epoch_losses)/len(epoch_losses):.6f}\")\n",
        "\n",
        "            print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "wDNGaBYq9zOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# 실험 1 vs 실험 3 비교 테스트\n",
        "# ========================================\n",
        "print(\"=\" * 80)\n",
        "print(\"실험 1 (Beta=0.1) vs 실험 3 (Beta=0.3) 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "exp1_checkpoints = [\n",
        "    ('/content/drive/MyDrive/Colab Notebooks/woke-odds/dpo-model/checkpoint-318/dpo_train', 'Exp1-Beta0.1'),\n",
        "]\n",
        "\n",
        "exp3_checkpoints = [\n",
        "    (f'{output_dir_exp3}/checkpoint-318/dpo_train', 'Exp3-Beta0.3'),\n",
        "]\n",
        "\n",
        "# 확장된 테스트 쿼리\n",
        "test_queries_compare_exp3 = [\n",
        "    (\"[AO|WHERE] When did call of duty ww2 come out?\",\n",
        "     \"Which one: release, release in the North America, release in Australia, or release in the EU?\"),\n",
        "    (\"[NONE|NONE] Who won the world cup as a player and a manager?\",\n",
        "     \"<NO_CLARIFYING_QUESTION>\"),\n",
        "    (\"[LA|SEM] Melissa told Jennifer that she beguiled the father-in-law. Who beguiled the father-in-law?\",\n",
        "     \"What does she refer to? Melissa or Jennifer?\"),\n",
        "    (\"[EM|CONT] The all possible word categories are either \\\"indoor location\\\" or \\\"human\\\".\\nThe following two examples share a specific word category. You need to first infer the specific word category from the examples.\\nPlease output \\\"X\\\" if the given sentence mentions the specific word category. Please output \\\"Y\\\" if the given sentence does not mention the word category.\\n\\nExamples: \\nThe duck is in the cave.\\nThe fugitive is in the theatre.\\n\\nThe Given Sentence: \\nThe boar is in the hotel lobby.\",\n",
        "     \"Is the category either indoor location or human?\"),\n",
        "    (\"[LA|LEX] Who is the director of Raavanan?\",\n",
        "     \"Are you referring to the Tamil film Raavanan or the Malayalam film Raavanan?\"),\n",
        "    (\"[AO|WHOM] Who has the most career homeruns in mlb?\",\n",
        "     \"Which one: the player with the most career home runs of all time, or the active player with the most career home runs?\"),\n",
        "]\n",
        "\n",
        "print(f\"\\n총 테스트 쿼리: {len(test_queries_compare_exp3)}개\\n\")\n",
        "\n",
        "for idx, (query, expected) in enumerate(test_queries_compare_exp3, 1):\n",
        "    print(f\"\\n테스트 {idx}/{len(test_queries_compare_exp3)}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    query_display = query[:100] + \"...\" if len(query) > 100 else query\n",
        "    print(f\"Query: {query_display}\")\n",
        "    print(f\"Expected: {expected[:80]}{'...' if len(expected) > 80 else ''}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 실험 1 결과\n",
        "    for checkpoint_path, name in exp1_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 실험 3 결과\n",
        "    for checkpoint_path, name in exp3_checkpoints:\n",
        "        print(f\"\\n[{name}]\")\n",
        "        base = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-4-mini-reasoning\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        m = PeftModel.from_pretrained(base, sft_path, adapter_name=\"sft\")\n",
        "        m.load_adapter(checkpoint_path, adapter_name=\"dpo_train\")\n",
        "        m.set_adapter(\"dpo_train\")\n",
        "        m.eval()\n",
        "\n",
        "        response = generate_response(m, query)\n",
        "        print(f\"  {response}\")\n",
        "\n",
        "        del m, base\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "print(\"\\n비교 완료!\")\n",
        "print(\"\\n분석:\")\n",
        "print(\"  - Beta=0.3이 Beta=0.1보다 더 안정적인지 확인\")\n",
        "print(\"  - 환각 및 이상한 출력이 줄어들었는지 확인\")\n",
        "print(\"  - 전반적인 응답 품질 비교\")"
      ],
      "metadata": {
        "id": "EEIbFYAz96zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이후 추가 실험들 삭제 우선은 epoch3, Beta3 채택"
      ],
      "metadata": {
        "id": "AzBrdwEkTlmm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lHaJ381Twj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}